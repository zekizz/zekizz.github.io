<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>给荔枝打气</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zekizz.github.io/"/>
  <updated>2019-11-09T09:53:10.481Z</updated>
  <id>https://zekizz.github.io/</id>
  
  <author>
    <name>zekizz</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>考虑未定义类型的多分类</title>
    <link href="https://zekizz.github.io/ML/sklearn-multi-label/"/>
    <id>https://zekizz.github.io/ML/sklearn-multi-label/</id>
    <published>2019-11-07T14:41:57.000Z</published>
    <updated>2019-11-09T09:53:10.481Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>考虑这样一个场景，我们要处理一个多分类问题，由于目标的空间是开放的，我们无法穷举所有的类别。目前我们制定了K类去训练一个多分类模型，在预测未知数据时，有可能出现未识别的类型，此时我们的多分类模型会赋予它k类中的一类。然鹅，这个赋予是错误的。那怎样去避免和处理这样的现象呢？</p><a id="more"></a><h2>方案思考</h2><p>遇到这个问题，很多人的第一直观想法是，分别给没类训练一个one-class classifier（比如one-class svm）不就好了。</p><p>这确实是一个看起来很不错的想法。但是one-class classifer本质上是做异常检测的，如果某一类的样本空间分布很分散，并没有聚成团，此时可能正确的样本都会被判断成错误的类别。实际中遇到各种乱七八糟的数据，我对one-class classifier的性能是打问号的。当然如果能精心设计，以上当我扯淡。</p><p>除了one-class classifer外，我粗略考虑到的两种稍微靠谱一点的做法，这里做一下记录。（貌似找到了这方面的paper，比如<a href="https://arxiv.org/abs/1801.05609" target="_blank" rel="noopener">Unseen Class Discovery in Open-world Classification</a>，之后找时间再写一篇学习下这类做法）</p><ol><li>将multi-class改成multi-label任务</li><li>设计一个pipline，第一个分类器判断是否见过，第二就是常规的多分类</li></ol><p>第二个做法中第一个分类器的设计，见仁见智。可以设计成one-class分类器，也可以计算与已知样本/类别的距离等这样简单的做法。</p><p>目前实践了第一种做法，做一下记录</p><h2>multi-label</h2><p>Multi-label指的是，一个样本可能有多个标签，不再是单一标签。比如一副海滩的图片，我们对其进行分类，其可能同时具备标签，白云、大海、沙滩、山、树。将其只分给其中一个比如大海，是不合适的。</p><blockquote><p>扯个题外话，分类问题我觉得可以分为四类</p><ol><li>Binary classification</li><li>Multi-class classification</li><li>Multi-label classification</li><li>Multi-target classification</li></ol></blockquote><p>提供两个工具：<a href="http://scikit.ml/" target="_blank" rel="noopener">scikit-multilearn</a>、<a href="http://waikato.github.io/meka/" target="_blank" rel="noopener">meka</a></p><p>Multi-label算法的核心其实是想利用label和label之间的相关性，我们这里处理未见类别其实没有利用到这个思想。</p><p>具体算法可以分为如下几类</p><ol><li>Binary Relevance：每个标签看作一个单独的类分类问题。</li><li>Classifier Chain：单独分类每个类别，但是前面得分类伪标签会作为后面分类的特征。（利用相关性，需要组合不同的链）</li><li>Label Powerset：组合类别做多个多分类问题</li><li>Adaptive：这类就比较丰富了，MLKNN、神经网络、集成学习</li></ol><p>在本次考虑的场景下，我们并没有考虑类别之间的关系，所以采用BR做一个测试，本次没有使用scikit-multilearn，仅仅用了sklearn的multi-label工具。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit, train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer, LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_one_vs_rest</span><span class="params">(self)</span>:</span></span><br><span class="line">    all_data_file = <span class="string">'data.txt'</span></span><br><span class="line">    data_all, labels_all = self.process_data(all_data_file)</span><br><span class="line">    <span class="comment"># lb = LabelEncoder()</span></span><br><span class="line">    lb = LabelBinarizer()</span><br><span class="line">    y = lb.fit_transform(labels_all)</span><br><span class="line">    print(lb.classes_)</span><br><span class="line">    text_train, text_test, y_train, y_test = \</span><br><span class="line">        train_test_split(data_all, y, test_size=<span class="number">0.33</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    count_vect = CountVectorizer()</span><br><span class="line">    train_vec = count_vect.fit_transform(text_train)</span><br><span class="line">    clf = OneVsRestClassifier(LogisticRegression())</span><br><span class="line">    clf.fit(train_vec, y_train)</span><br><span class="line">    print(len(clf.estimators_))</span><br><span class="line">    test_vec = count_vect.transform(text_test)</span><br><span class="line">    pred = clf.predict(test_vec)</span><br><span class="line">    print(classification_report(y_test, pred))</span><br><span class="line">    self.model = clf</span><br><span class="line">    self.vectorizer = count_vect</span><br><span class="line">    pred = self.pred_multi_label(test_vec)</span><br><span class="line">    print(pred)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pred_multi_label</span><span class="params">(self, vectors)</span>:</span></span><br><span class="line">    preds = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.model.estimators_)):</span><br><span class="line">        preds.append(</span><br><span class="line">            self.model.estimators_[i].predict_proba(vectors)[:, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> np.column_stack(tuple(preds))</span><br></pre></td></tr></table></figure><p>这里需要注意的是，在预测过程中，如果采用OneVsRestClassifier默认的predict_proba，得到的没类的概率是归一化的。我想得到的就是具体属于每一类的概率，不要归一化。所以，这里采用了其属性<code>estimators_</code>来计算具体的概率。</p><p>实验发现，效果贼好。对于已知类别，对应类上的概率基本大于80%，而对于未见类别，最高也在57%左右。</p><p>此时我们完全可以采用70%去截断，如果一个类别未达到，那就是未知类别了。</p><p>当然，这种做法还有一些问题，接下来梳理下更好的做法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;考虑这样一个场景，我们要处理一个多分类问题，由于目标的空间是开放的，我们无法穷举所有的类别。目前我们制定了K类去训练一个多分类模型，在预测未知数据时，有可能出现未识别的类型，此时我们的多分类模型会赋予它k类中的一类。然鹅，这个赋予是错误的。那怎样去避免和处理这样的现象呢？&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="multi-label" scheme="https://zekizz.github.io/tags/multi-label/"/>
    
      <category term="sklearn" scheme="https://zekizz.github.io/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>vue文件上传下载</title>
    <link href="https://zekizz.github.io/visualisation/vue%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD/"/>
    <id>https://zekizz.github.io/visualisation/vue文件上传下载/</id>
    <published>2019-10-31T14:40:00.000Z</published>
    <updated>2019-10-31T15:14:22.483Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>使用booststrap-vue和FileReader做文件读取、file-saver做文件下载</p><a id="more"></a><h1>文件读取</h1><h2>1. 选取本地文件</h2><p>参考bootstrap-vue的<a href="https://bootstrap-vue.js.org/docs/components/form-file" target="_blank" rel="noopener">form-file</a>，来选取本地文件</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;b-form-file</span><br><span class="line">  v-model=<span class="string">"dataFile"</span></span><br><span class="line">  :state=<span class="string">"Boolean(dataFile)"</span></span><br><span class="line">  placeholder=<span class="string">"选择文件"</span></span><br><span class="line">  drop-placeholder=<span class="string">"拖拽到这里"</span></span><br><span class="line">  style=<span class="string">"text-align: left;"</span></span><br><span class="line">  size=<span class="string">"sm"</span></span><br><span class="line">&gt;<span class="xml"><span class="tag">&lt;/<span class="name">b-form-file</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>选取到的文件存放在compenent data的dataFile中。</p><h2>2. 读取文件内容</h2><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">loadFile () &#123;</span><br><span class="line">  <span class="keyword">const</span> reader = <span class="keyword">new</span> FileReader();</span><br><span class="line">  <span class="keyword">const</span> _this = <span class="keyword">this</span>;</span><br><span class="line">  reader.readAsText(_this.dataFile);</span><br><span class="line">  reader.onload = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="comment">// this.result为读取到的json字符串，需转成json对象</span></span><br><span class="line">    _this.importJSON = <span class="built_in">JSON</span>.parse(<span class="keyword">this</span>.result);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><h1>文件下载</h1><p>比如我们在前端编辑了一些数据，希望将这些数据下载为json文件，查了一圈发现还是<a href="https://www.npmjs.com/package/file-saver" target="_blank" rel="noopener">file-saver</a>最好用。</p><p>安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i file-saver</span><br></pre></td></tr></table></figure><p>不需要在头部import，采用<code>require</code>的方式</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">saveData()&#123;</span><br><span class="line">    <span class="keyword">var</span> FileSaver = <span class="built_in">require</span>(<span class="string">'file-saver'</span>);</span><br><span class="line">    <span class="keyword">var</span> data = <span class="built_in">JSON</span>.stringify(<span class="keyword">this</span>.graph, <span class="literal">null</span>, <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">var</span> blob = <span class="keyword">new</span> Blob([data], &#123;<span class="attr">type</span>: <span class="string">"application/json;charset=utf-8"</span>&#125;);</span><br><span class="line">    FileSaver(blob, <span class="string">"output.json"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意：这里的api与github上readme上写的已经不一样了，我目前的最新版本号<code>2.0.2</code>。这里不再采用<code>FileSaver.saveAs(file)</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用booststrap-vue和FileReader做文件读取、file-saver做文件下载&lt;/p&gt;
    
    </summary>
    
    
      <category term="visualisation" scheme="https://zekizz.github.io/categories/visualisation/"/>
    
    
      <category term="前端" scheme="https://zekizz.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
      <category term="可视化" scheme="https://zekizz.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="文件" scheme="https://zekizz.github.io/tags/%E6%96%87%E4%BB%B6/"/>
    
  </entry>
  
  <entry>
    <title>NLU调研</title>
    <link href="https://zekizz.github.io/NLU/nlu-survey/"/>
    <id>https://zekizz.github.io/NLU/nlu-survey/</id>
    <published>2019-10-27T02:57:00.000Z</published>
    <updated>2019-10-27T14:57:12.765Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>[TOC]</p><p>业务场景：小样本数据上的任务型对话理解。</p><p>对话领域三类</p><ol><li>问答类</li><li>任务类</li><li>闲聊类</li></ol><a id="more"></a><h1>1. 规则方法</h1><h2>1.1 意图识别</h2><ul><li>词典法</li><li>CFG（上下文无关语法）</li><li>JSGF（JSpeech Grammar Format）</li></ul><p>参考资料：</p><ul><li><a href="https://cs.nju.edu.cn/daixinyu/nlp-traditional.pdf" target="_blank" rel="noopener">CFG原理</a></li><li><a href="https://github.com/Samurais/text-cfg-parser" target="_blank" rel="noopener">自然语言处理之CFG句法分析</a></li><li><a href="https://github.com/Danesprite/pyjsgf" target="_blank" rel="noopener">JSpeech Grammar Format (JSGF) compiler, matcher and parser package for Python.</a></li></ul><h2>1.2 命名实体识别</h2><p>需要构造词典</p><ul><li>AC自动机算法（Aho–Corasick算法）</li><li><a href="http://www.hankcs.com/program/algorithm/aho-corasick-double-array-trie.html" target="_blank" rel="noopener">Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配</a></li><li>基于规则的模型</li></ul><p>参考：</p><ul><li><a href="https://sara-hy.github.io/2018/11/02/intent_slot/" target="_blank" rel="noopener">Review on Intent Classification and Slot Filling</a></li><li><a href="https://ifreele.com/2017/11/05/tech-2017-11-5-chatbot/" target="_blank" rel="noopener">基于Rasa_NLU的微信chatbot</a></li></ul><h1>2. 模型方法</h1><p><a href="https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey" target="_blank" rel="noopener">A dataset survey about task-oriented dialogue, including recent datasets and SoA results &amp; papers.</a></p><h2>2.1 pipeline</h2><p>pipeline方法将意图识别和槽填充分为两个独立的部分，分别进行训练。</p><h3>2.1.1 意图识别</h3><p>本质上是短文本分类任务，一般的文本分类算法都可以处理</p><p>传统算法：</p><ul><li>LR</li><li>SVM</li><li>KNN</li><li>RF</li><li>GBDT</li><li>…</li></ul><p>深度学习方法</p><ul><li>Fasttext</li><li>TextCNN</li><li>GRU</li><li>LSTM</li><li>IDCNN</li><li>TextRNN</li></ul><p>经调研，预训练fasttext词向量+单层textcnn从分类效果和速度上都相对较优，作为优先选择。</p><p>TextCNN的改进：</p><ul><li>K-max pooling</li><li>DPCNN</li><li>…</li></ul><h3>2.1.2 槽填充</h3><ul><li>CRF</li><li>RNN/LSTM/CNN+CRF</li><li>BiLSTM+CRF</li><li>BiLSTM+CNN+CRF</li></ul><h2>2.2 joint model</h2><ul><li><p><a href="https://github.com/HadoopIt/rnn-nlu" target="_blank" rel="noopener">A TensorFlow implementation of Recurrent Neural Networks for Sequence Classification and Sequence Labeling</a></p></li><li><p><a href="https://github.com/applenob/RNN-for-Joint-NLU" target="_blank" rel="noopener">Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling</a></p></li><li><p><a href="https://www.coursera.org/lecture/language-processing/intent-classifier-and-slot-tagger-nlu-RmVnE" target="_blank" rel="noopener">https://www.coursera.org/lecture/language-processing/intent-classifier-and-slot-tagger-nlu-RmVnE</a></p></li><li><p><a href="https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey" target="_blank" rel="noopener">Task-Oriented Dialogue Dataset Survey</a></p></li></ul><p>其中第三条提到的模型: <a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">Convolutional Sequence to Sequence Learning</a></p><h1>3. 企业做法</h1><h2>3.1 阿里小蜜</h2><p>Arxiv: <a href="https://arxiv.org/pdf/1801.05032.pdf" target="_blank" rel="noopener">AliMe Assist: An Intelligent Assistant for Creating an Innovative E-commerce Experience</a></p><p>note: 经内部人员考证，这套框架太老已弃用</p><ul><li>business rule parser: 大量的样式(patterns)组成的前缀树匹配结构(trie-based)</li><li>Intention classifier: 场景分类，pre-train采用fasttext，分类采用单层cnn<ul><li>requesting for assistance</li><li>asking for information or solution</li><li>chatting</li></ul></li><li>Semantic Parser: a trie-based, 匹配知识图谱中的实体</li></ul><h2>3.2 美团</h2><p>参考：<a href="https://www.infoq.cn/article/w0EfTYKY29I8All*bG6G" target="_blank" rel="noopener">美团对话理解技术及实践</a></p><p>上下文无关文法，工具，规则的写法</p><h1>4. 数据</h1><ul><li><a href="https://spaces.ac.cn/archives/4338" target="_blank" rel="noopener">【语料】百度的中文问答数据集WebQA</a></li><li><a href="https://github.com/SophonPlus/ChineseNlpCorpus" target="_blank" rel="noopener">SophonPlus/ChineseNlpCorpus</a></li><li><a href="https://github.com/candlewill/Dialog_Corpus" target="_blank" rel="noopener">candlewill/Dialog_Corpus</a>: 用于训练中英文对话系统的语料库 Datasets for Training Chatbot System</li><li><a href="https://github.com/brightmart/nlp_chinese_corpus" target="_blank" rel="noopener">brightmart/nlp_chinese_corpus</a>: 大规模中文自然语言处理语料 Large Scale Chinese Corpus for NLP</li></ul><h1>5. 开源工具</h1><h2>5.1 ChatterBot</h2><p><a href="https://github.com/gunthercox/ChatterBot" target="_blank" rel="noopener">github 9.1k</a></p><p>没有NLU模块，做法是匹配式，训练的输入是一系列完整的对话过程，数据库存储。</p><p><img src="https://chatterbot.readthedocs.io/en/stable/_images/training-graph.svg" alt></p><p>通过<code>Logic adapters</code>来获取输出结果</p><ul><li>BestMatch</li><li>TimeLogicAdapter</li><li>MathematicalEvaluation</li></ul><p>这个框架主要对问题文本 使用<strong>相似度匹配</strong>，找出库中预定好的答案。 比较适合，<strong>知识问答类</strong>的情形。</p><h2>5.2 rasa</h2><ul><li><a href="https://github.com/RasaHQ/rasa" target="_blank" rel="noopener">rasa 6.8k</a></li><li><a href="https://github.com/crownpku/Rasa_NLU_Chi" target="_blank" rel="noopener">Rasa_NLU_Chi 848</a></li></ul><p><strong>数据</strong></p><ul><li>语料标注工具：<a href="https://rasahq.github.io/rasa-nlu-trainer/" target="_blank" rel="noopener">rasa-nlu-trainer</a></li><li>数据生成工具：<a href="https://rodrigopivi.github.io/Chatito/" target="_blank" rel="noopener">chatito</a></li></ul><p><strong>意图识别</strong></p><ul><li>KeywordIntentClassifier：This classifier is mostly used as a placeholder. It is able to recognize hello and goodbye intents by searching for these keywords in the passed messages.</li><li>MitieIntentClassifier： This classifier uses MITIE to perform intent classification. The underlying classifier is using a <strong>multi-class linear SVM with a sparse linear kernel</strong> 。</li><li>SklearnIntentClassifier： The sklearn intent classifier trains a linear SVM which gets optimized using a grid search.需要前置feature extractor</li><li>EmbeddingIntentClassifier： The embedding intent classifier embeds user inputs and intent labels into the same space. Supervised embeddings are trained by maximizing similarity between them. This algorithm is based on <a href="https://arxiv.org/abs/1709.03856" target="_blank" rel="noopener">StarSpace</a>.</li></ul><p><strong>实体识别</strong></p><ul><li>MitieEntityExtractor：The underlying classifier is using a multi class linear SVM with a sparse linear kernel and custom features</li><li>SpacyEntityExtractor：Using spaCy this component predicts the entities of a message. spacy uses a statistical BILOU transition model.</li><li>EntitySynonymMapper： Maps synonymous entity values to the same value. 通过数据中的<code>value</code>来提供</li><li>CRFEntityExtractor：spaCy has to be installed. 貌似用的spaCy的实现</li><li>DucklingHTTPExtractor： Duckling lets you extract common entities like dates, amounts of money, distances, and others in a number of languages.</li></ul><p><strong>槽填充</strong></p><p><a href="https://rasa.com/docs/rasa/core/slots/" target="_blank" rel="noopener">官方文档：slot的使用</a></p><p>参考：</p><ul><li><a href="https://github.com/GaoQ1/rasa-nlp-architect" target="_blank" rel="noopener">GaoQ1/rasa-nlp-architect</a>: 采用nlp-architect实现rasa-nlu中文意图提取和槽填充</li><li>Building contextual assistants with Rasa Forms: <a href="https://blog.rasa.com/building-contextual-assistants-with-rasa-formaction/" target="_blank" rel="noopener">原文</a>, <a href="https://zhuanlan.zhihu.com/p/84678559" target="_blank" rel="noopener">译文</a></li></ul><p>均可自定义component: <a href="https://blog.rasa.com/enhancing-rasa-nlu-with-custom-components/" target="_blank" rel="noopener">Enhancing Rasa NLU models with Custom Components</a></p><h2>5.3 DeepPavlov</h2><p><a href="http://git.azurewebsites.net/deepmipt/DeepPavlov" target="_blank" rel="noopener">deepmipt/DeepPavlov</a>: 3.6k</p><blockquote><p>An open source library for deep learning end-to-end dialog systems and chatbots. <a href="https://deeppavlov.ai/" target="_blank" rel="noopener">https://deeppavlov.ai</a></p></blockquote><p>支持英文和俄语。功能全面，可作为学习参考。</p><p><img src="https://miro.medium.com/max/2326/1*DEHpboBRNsb7HY-NhL0G0A.png" alt></p><p>基本概念</p><ul><li><code>Agent</code> is a conversational agent communicating with users in natural language (text).</li><li><code>Skill</code> fulfills user’s goal in some domain. Typically, this is accomplished by presenting information or completing transaction (e.g. answer question by FAQ, booking tickets etc.). However, for some tasks a success of interaction is defined as continuous engagement (e.g. chit-chat).</li><li><code>Model</code> is any NLP model that doesn’t necessarily communicates with user in natural language.</li><li><code>Component</code> is a reusable functional part of <code>Model</code> or <code>Skill</code>.</li><li><code>Rule-based Models</code> cannot be trained.</li><li><code>Machine Learning Models</code> can be trained only stand alone.</li><li><code>Deep Learning Models</code> can be trained independently and in an end-to-end mode being joined in a chain.</li><li><code>Skill Manager</code> performs selection of the <code>Skill</code> to generate response.</li><li><code>Chainer</code> builds an agent/model pipeline from heterogeneous components (Rule-based/ML/DL). It allows to train and infer models in a pipeline as a whole.</li></ul><p>Models:</p><ul><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#ner-model-docs" target="_blank" rel="noopener">NER model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/ner.html" target="_blank" rel="noopener">docs]</a>: BERT-based and Bi-LSTM+CRF.</li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#slot-filling-models-docs" target="_blank" rel="noopener">Slot filling models </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/slot_filling.html" target="_blank" rel="noopener">docs]</a>:</li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#classification-model-docs" target="_blank" rel="noopener">Classification model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/classifiers.html" target="_blank" rel="noopener">docs]</a></li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#automatic-spelling-correction-model-docs" target="_blank" rel="noopener">Automatic spelling correction model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/spelling_correction.html" target="_blank" rel="noopener">docs]</a></li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#ranking-model-docs" target="_blank" rel="noopener">Ranking model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/neural_ranking.html" target="_blank" rel="noopener">docs]</a></li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#tf-idf-ranker-model-docs" target="_blank" rel="noopener">TF-IDF Ranker model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/tfidf_ranking.html" target="_blank" rel="noopener">docs]</a></li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#question-answering-model-docs" target="_blank" rel="noopener">Question Answering model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/squad.html" target="_blank" rel="noopener">docs]</a></li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#morphological-tagging-model-docs" target="_blank" rel="noopener">Morphological tagging model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/models/morphotagger.html" target="_blank" rel="noopener">docs]</a></li><li><a href="http://docs.deeppavlov.ai/en/master/features/overview.html#frequently-asked-questions-faq-model-docs" target="_blank" rel="noopener">Frequently Asked Questions (FAQ) model </a>[<a href="http://docs.deeppavlov.ai/en/master/features/skills/faq.html" target="_blank" rel="noopener">docs]</a></li></ul><p><strong>意图识别</strong></p><ul><li><strong>BERT classifier</strong> (see <a href="http://docs.deeppavlov.ai/en/master/apiref/models/bert.html" target="_blank" rel="noopener">here</a>) builds BERT <a href="http://docs.deeppavlov.ai/en/master/features/models/classifiers.html#id21" target="_blank" rel="noopener">8</a> architecture for classification problem on Tensorflow.</li><li><strong>Keras classifier</strong> (see <a href="http://docs.deeppavlov.ai/en/master/apiref/models/classifiers.html" target="_blank" rel="noopener">here</a>) builds neural network on Keras with tensorflow backend.</li><li><strong>Sklearn classifier</strong> (see <a href="http://docs.deeppavlov.ai/en/master/apiref/models/sklearn.html" target="_blank" rel="noopener">here</a>) builds most of sklearn classifiers.</li></ul><p>模型很丰富</p><p><strong>NER</strong></p><ul><li>standard RNN based and BERT based.</li><li>Multilingual BERT Zero-Shot Transfer</li><li>Few-shot Language-Model based</li></ul><p><strong>槽填充</strong></p><p>官方文档: <a href="http://docs.deeppavlov.ai/en/master/features/models/slot_filling.html" target="_blank" rel="noopener">Neural Named Entity Recognition and Slot Filling</a></p><blockquote><p>This model solves Slot-Filling task using Levenshtein search and different neural network architectures for NER.</p><p>Slotfiller will perform fuzzy search through the all variations of all entity values of given entity type. The entity type is determined by the NER component.</p></blockquote><p>使用博客：<a href="https://github.com/deepmipt/dp_notebooks" target="_blank" rel="noopener">DeepPavlov articles with Python code</a></p><p><strong>规则编写</strong></p><p>只见到了对话规则的编写，通过<code>PatternMatchingSkill</code>，使用正则编写pattern和response</p><p>有一个包装rasa的<code>Rasa Skill</code></p><p><strong>DeepPavlov存在的问题</strong></p><ol><li>环境依赖<ul><li>DeepPavlov是基于TensorFlow和Keras实现的，不能继承其他计算框架的模型实现（如PyTorch）。</li></ul></li><li>语言支持<ul><li>Pre-train模型和评测数据集主要基于英文和俄文，不支持中文。</li></ul></li><li>生产环境部署<ul><li>DeepPavlov在运行时需要依赖整个框架源码，开发环境对框架修改后，生产环境需要更新整个框架。</li><li>也不能直接将功能Component作为服务独立导出，不适合在生产环境的部署和发布。</li></ul></li></ol><h2>5.4 Snips-nlu</h2><p><a href="http://git.azurewebsites.net/snipsco/snips-nlu" target="_blank" rel="noopener">snipsco/snips-nlu</a>: 3k</p><blockquote><p>Snips Python library to extract meaning from text <a href="https://snips-nlu.readthedocs.io/" target="_blank" rel="noopener">https://snips-nlu.readthedocs.io</a></p></blockquote><p>不支持中文</p><p><a href="https://snips-nlu.readthedocs.io/en/latest/tutorial.html" target="_blank" rel="noopener">Tutorial</a>： 意图和槽值都放在训练数据中了</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># turnLightOn intent</span></span><br><span class="line">---</span><br><span class="line">type: intent</span><br><span class="line">name: turnLightOn</span><br><span class="line">slots:</span><br><span class="line"><span class="bullet">  - </span>name: room</span><br><span class="line"><span class="code">    entity: room</span></span><br><span class="line">utterances:</span><br><span class="line"><span class="bullet">  - </span>Turn on the lights in the [<span class="string">room</span>](<span class="link">kitchen</span>)</span><br><span class="line"><span class="bullet">  - </span>give me some light in the [<span class="string">room</span>](<span class="link">bathroom</span>) please</span><br><span class="line"><span class="bullet">  - </span>Can you light up the [<span class="string">room</span>](<span class="link">living room</span>) ?</span><br><span class="line"><span class="bullet">  - </span>switch the [<span class="string">room</span>](<span class="link">bedroom</span>)'s lights on please</span><br></pre></td></tr></table></figure><blockquote><p>This parser parses text using two steps: first it classifies the intent using an <a href="https://snips-nlu.readthedocs.io/en/latest/api.html#snips_nlu.intent_classifier.IntentClassifier" target="_blank" rel="noopener"><code>IntentClassifier</code></a> and once the intent is known, it using a <a href="https://snips-nlu.readthedocs.io/en/latest/api.html#snips_nlu.slot_filler.SlotFiller" target="_blank" rel="noopener"><code>SlotFiller</code></a> in order to extract the slots.</p></blockquote><p>IntentClassifier</p><ul><li>Logistic Regression</li><li>Feature extractor for text classification relying on ngrams tfidf and optionally word cooccurrences features</li><li>scikit-learn TfidfVectorizer</li><li>Featurizer that takes utterances and extracts ordered word cooccurrence features matrix from them</li></ul><p>SlotFiller</p><ul><li>Linear-Chain Conditional Random Fields</li></ul><h2>5.5 其他</h2><ul><li><a href="https://github.com/charlesXu86/Chatbot_CN" target="_blank" rel="noopener">基于金融-司法领域(兼有闲聊性质)的聊天机器人</a></li><li><a href="https://github.com/GaoQ1/rasa_chatbot_cn" target="_blank" rel="noopener">基于最新版本rasa搭建的对话系统demo</a></li><li><a href="https://github.com/Aguila-team/Chinese_NLU_by_using_RASA_NLU" target="_blank" rel="noopener">使用 RASA NLU 来构建中文自然语言理解系统</a></li><li><a href="https://github.com/crownpku/Awesome-Chinese-NLP" target="_blank" rel="noopener">crownpku/Awesome-Chinese-NLP</a></li></ul><hr><p>参考：</p><ol><li><a href="https://www.xuqingtang.top/2019/06/17/2019-06-17%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E8%90%BD%E5%9C%B0%E8%B0%83%E7%A0%94/" target="_blank" rel="noopener">2019-06-17问答系统项目落地调研</a></li><li><a href="http://hainanumeeting.net/YSSNLP2019/file/17.pdf" target="_blank" rel="noopener">YSSNLP2019 人机对话研究热点及前沿技术概述</a></li><li><a href="https://www.infoq.cn/article/w0EfTYKY29I8All*bG6G" target="_blank" rel="noopener">美团对话理解技术及实践</a></li><li><a href="https://www.jianshu.com/p/d713678fddfb" target="_blank" rel="noopener">对话系统 NLU/DM 任务详解</a></li><li>[NLP笔记 - NLU之意图分类](<a href="http://www.shuang0420.com/2017/04/27/NLP%E7%AC%94%E8%AE%B0" target="_blank" rel="noopener">http://www.shuang0420.com/2017/04/27/NLP笔记</a> - NLU之意图分类/)</li><li><a href="http://bbs.cnaiplus.com/thread-5258-1-1.html" target="_blank" rel="noopener">自然语言理解中的槽位填充</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;业务场景：小样本数据上的任务型对话理解。&lt;/p&gt;
&lt;p&gt;对话领域三类&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;问答类&lt;/li&gt;
&lt;li&gt;任务类&lt;/li&gt;
&lt;li&gt;闲聊类&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="NLU" scheme="https://zekizz.github.io/categories/NLU/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="NLP" scheme="https://zekizz.github.io/tags/NLP/"/>
    
      <category term="NER" scheme="https://zekizz.github.io/tags/NER/"/>
    
      <category term="NLU" scheme="https://zekizz.github.io/tags/NLU/"/>
    
  </entry>
  
  <entry>
    <title>configparser配置解析</title>
    <link href="https://zekizz.github.io/python/configparse/"/>
    <id>https://zekizz.github.io/python/configparse/</id>
    <published>2019-03-31T06:07:05.000Z</published>
    <updated>2019-10-13T09:20:48.297Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>使用configparser解析ini格式的配置文件</p><a id="more"></a><ul><li>lib: <a href="https://pypi.org/project/configparser/" target="_blank" rel="noopener">https://pypi.org/project/configparser/</a></li><li>doc: <a href="https://docs.python.org/3/library/configparser.html" target="_blank" rel="noopener">https://docs.python.org/3/library/configparser.html</a></li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> configparser</span><br></pre></td></tr></table></figure><p>get的时候，如果key在DEFAULT段中，get设置的deflault无效，总是返回DEFAULT段中的值</p><p>ini格式的config文件示例</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">ServerAliveInterval = 45</span><br><span class="line">Compression = <span class="literal">yes</span></span><br><span class="line">CompressionLevel = 9</span><br><span class="line">ForwardX11 = <span class="literal">yes</span></span><br><span class="line"></span><br><span class="line">[bitbucket.org]</span><br><span class="line">User = hg</span><br><span class="line"></span><br><span class="line">[topsecret.server.com]</span><br><span class="line">Port = 50022</span><br><span class="line">ForwardX11 = <span class="literal">no</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用configparser解析ini格式的配置文件&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://zekizz.github.io/categories/python/"/>
    
    
      <category term="python" scheme="https://zekizz.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>excel去除文本中的不可打印字符</title>
    <link href="https://zekizz.github.io/others/excel_trick/"/>
    <id>https://zekizz.github.io/others/excel_trick/</id>
    <published>2019-03-31T06:05:57.000Z</published>
    <updated>2019-10-13T09:20:48.303Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>处理csv文件的时候，有时文本中有一些不可打印的字符，影响诸如文件分行和excel的解析。<br>主要需要去除文本中的换行符和制表符。</p><a id="more"></a><h3>方法一：用CLEAN函数</h3><p>CLEAN函数可以删除文本中不能打印的字符。</p><p>假如A1单元格包含换行符，可在B1单元格中输入公式：</p> <figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"=CLEAN(A1)"</span></span><br></pre></td></tr></table></figure><p>即可删除换行符。</p><h3>方法二：查找替换法(不好用)</h3><ol><li>按快捷键<strong>Ctrl+H</strong>，打开“查找和替换”对话框；</li><li>选择“查找内容”后的文本框，按住<strong>Alt</strong>键，在数字键盘中输入“<strong>0010</strong>”。需要注意的是这样输入后，在“查找内容”后的文本框中不会显示任何内容，但实际上是有的；</li><li>单击“全部替换”按钮，换行符将被全部替换。</li></ol><p>参考文献:<br><a href="http://blog.sina.com.cn/s/blog_49f78a4b0102e3br.html" target="_blank" rel="noopener">如何快速批量删除Excel单元格中的“换行符”</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;处理csv文件的时候，有时文本中有一些不可打印的字符，影响诸如文件分行和excel的解析。&lt;br&gt;
主要需要去除文本中的换行符和制表符。&lt;/p&gt;
    
    </summary>
    
    
      <category term="others" scheme="https://zekizz.github.io/categories/others/"/>
    
    
      <category term="excel" scheme="https://zekizz.github.io/tags/excel/"/>
    
  </entry>
  
  <entry>
    <title>git笔记</title>
    <link href="https://zekizz.github.io/git/git_notes/"/>
    <id>https://zekizz.github.io/git/git_notes/</id>
    <published>2019-03-31T05:11:52.000Z</published>
    <updated>2019-10-13T09:20:48.303Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>git常用小结</p><a id="more"></a><h2>配置</h2><p>获取配置信息</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--system --list</span><br><span class="line">git<span class="built_in"> config </span>--local --list</span><br></pre></td></tr></table></figure><p>git config</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--global user.name <span class="string">"Your Name"</span></span><br><span class="line">git<span class="built_in"> config </span>--global user.email <span class="string">"email@example.com"</span></span><br></pre></td></tr></table></figure><p>github配置ssh</p><ol><li>本地生成ssh密钥对</li></ol><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ssh-keygen -t rsa</span></span><br></pre></td></tr></table></figure><ol start="2"><li>密钥对生成完成后存放于当前用户 ~/.ssh 目录中，查看 id_rsa.pub</li></ol><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~<span class="regexp">/.ssh/i</span>d_rsa.pub</span><br></pre></td></tr></table></figure><ol start="3"><li>添加入github的setting中</li></ol><h2>常用命令</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">git diff <span class="comment">--cached  # 查看已经暂存起来的文件和上次提交时的快照之间的差异</span></span><br><span class="line">git diff <span class="comment">--staged  # Git 1.6.1及更高版本，效果同上</span></span><br><span class="line"></span><br><span class="line">git <span class="keyword">commit</span> -a -m <span class="string">'comment'</span>  <span class="comment"># 自动把所有已经跟踪过的文件暂存起来一并提交</span></span><br><span class="line">git <span class="keyword">commit</span> <span class="comment">--amend  # 撤销操作重新提交</span></span><br><span class="line">    <span class="comment"># 只生成一个commit</span></span><br><span class="line">    git <span class="keyword">commit</span> -m <span class="string">'initial commit'</span></span><br><span class="line">    git <span class="keyword">add</span> forgotten_file</span><br><span class="line">    git <span class="keyword">commit</span> <span class="comment">--amend</span></span><br><span class="line"></span><br><span class="line">git rm <span class="comment">--cached filename  # 移除跟踪但不删除文件</span></span><br><span class="line"></span><br><span class="line">git <span class="keyword">log</span> </span><br><span class="line">    <span class="comment">--pretty=oneline  # 每个提交放在一行显示, 其他：short，full 和 fuller</span></span><br><span class="line">    <span class="comment">--pretty=format:"%h %s"  # 更加简洁的信息 </span></span><br><span class="line">    <span class="comment">--graph  # oneline 或 format 时结合 --graph 选项</span></span><br><span class="line">    - p  <span class="comment"># 显示每次提交的内容差异</span></span><br><span class="line">    - <span class="number">2</span>  <span class="comment"># 显示最近的两次更新</span></span><br><span class="line">git <span class="keyword">log</span> <span class="comment">--graph --pretty=oneline  # 常用查看log</span></span><br><span class="line"></span><br><span class="line">git <span class="keyword">reset</span> <span class="keyword">HEAD</span> &lt;<span class="keyword">file</span>&gt;  <span class="comment"># 已经add, 把暂存区的修改撤销掉</span></span><br><span class="line">git checkout <span class="comment">-- &lt;file&gt;  # 还未add, 撤销工作区的修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># git log 查看版本号，再版本回退，若想再次恢复到新版本，git reflog 查看版本号</span></span><br><span class="line">git <span class="keyword">reset</span> <span class="comment">--hard [commit id]  </span></span><br><span class="line"></span><br><span class="line">git checkout branchname  <span class="comment"># 切换分支</span></span><br><span class="line">git checkout -b brachname  <span class="comment"># 创建并切换分支</span></span><br><span class="line">git branch -d branchname  <span class="comment"># 删除分支</span></span><br><span class="line"><span class="comment"># 强制禁用Fast forward模式，Git就会在merge时生成一个新的commit</span></span><br><span class="line">git <span class="keyword">merge</span> <span class="comment">--no-ff -m 'comment' branch  </span></span><br><span class="line"></span><br><span class="line">git stash  <span class="comment"># bug 分支</span></span><br><span class="line">    <span class="number">1.</span> 在当前分支git stash，工作区恢复到最近一次<span class="keyword">commit</span></span><br><span class="line">    <span class="number">2.</span> 处理完其他分支问题</span><br><span class="line">    <span class="number">3.</span> 在当前分支git stash <span class="keyword">list</span>查看stash内容</span><br><span class="line">    <span class="number">4.</span> git stash pop，恢复并删除stash</span><br><span class="line"></span><br><span class="line">git remote -v  <span class="comment"># 查看远程库分支</span></span><br><span class="line">git push origin <span class="keyword">master</span>/dev  <span class="comment"># 推送分支</span></span><br><span class="line">git checkout -b dev origin/dev  <span class="comment"># 创建远程origin的dev分支到本地，需先创建本地dev分支</span></span><br><span class="line">git pull  <span class="comment"># 拉取远程到本地，遇到推送有冲突的时候，先 git pull，本地解决冲突，再push</span></span><br><span class="line"></span><br><span class="line">git remote <span class="keyword">add</span> origin git@github.com:xxx/xxx.git  <span class="comment"># 关联远程库</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;git常用小结&lt;/p&gt;
    
    </summary>
    
    
      <category term="git" scheme="https://zekizz.github.io/categories/git/"/>
    
    
      <category term="git" scheme="https://zekizz.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>基于句法依存树的信息抽取</title>
    <link href="https://zekizz.github.io/NLP/information-extraction/"/>
    <id>https://zekizz.github.io/NLP/information-extraction/</id>
    <published>2018-12-16T11:34:59.000Z</published>
    <updated>2019-10-13T09:20:48.300Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>信息抽取是一个常见的nlp任务，为经常一起提到的知识图谱的基础。</p><p>这里有一份比较好的知识图谱入门资料:<br><a href="https://kgtutorial.github.io/" target="_blank" rel="noopener">Mining Knowledge Graphs from Text</a></p><p>信息抽取分为有监督和无监督方法。实际中监督信息往往是缺失的，所以本文主要提无监督方法。</p><p>在无监督方法中，一个广泛采用的工具是句法依存树，或者叫句法解析树（Dependency Tree）。</p><a id="more"></a><p>可视化工具：</p><ul><li><a href="http://spyysalo.github.io/conllu.js/" target="_blank" rel="noopener">conllu.js</a></li><li><a href="http://nlp.nju.edu.cn/tanggc/tools/DependencyViewer.html" target="_blank" rel="noopener">Dependency Viewer</a></li></ul><p>工作中总结出来的一种基于句法解析树的信息提取的一般方法（尚未完善）：</p><ol><li>从CONLL格式的句法依存解析结果，生成具备孩子节点和父节点索引的树，并获取根结点root_id；</li><li>自上而下递归的进行解析，孩子节点的解析结果上传到父节点进行汇总；<ol><li>如果当前节点为叶子节点（无孩子节点），判断当前节点类型，返回dict，上传给父节点；</li><li>如果当前词为动词（一般句子的核心是动词，一般方法也都是从动词开始扩展）或者用户指定的目标词，根据制定的规则提取指定关系类型的孩子，如定中关系、状中关系、动宾关系、主谓关系等，这里需要注意的是并列关系，有可能是补充，有可能是同级并列；</li><li>如果当前词非动词，先判断其所属信息类型，若能判断，关系向孩子传递，如不能判断，等待孩子节点上传的结果；</li><li>合并所有孩子的信息，按原始句子顺序排序</li></ol></li><li>修正后处理</li></ol><p>以提取文本中，关于道路信息的（时间、原因、地点、时间）四元组为例，输入句子：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">黄石高速:因交通管制,晋州站、辛集站、藁城东站双向关闭。因沧州服务区附近K81处黄骅方向发生交通事故,沧州服务区附近K81处黄骅方向车辆缓慢通行约4公里</span></span><br></pre></td></tr></table></figure><p>句法解析树：</p><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>黄石黄石nhnr<span class="keyword">_</span><span class="number">3</span>SBV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">2</span>高速高速dd<span class="keyword">_</span><span class="number">3</span>ADV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">3</span>::vv<span class="keyword">_</span><span class="number">0</span>HED<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">4</span>因因pp<span class="keyword">_</span><span class="number">14</span>ADV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">5</span>交通管制交通管制vv<span class="keyword">_</span><span class="number">4</span>POB<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">6</span>,,wpw<span class="keyword">_</span><span class="number">4</span>WP<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">7</span>晋州站晋州站nsns<span class="keyword">_</span><span class="number">14</span>SBV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">8</span>、、wpw<span class="keyword">_</span><span class="number">9</span>WP<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">9</span>辛集站辛集站nn<span class="keyword">_</span><span class="number">7</span>COO<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">10</span>、、wpw<span class="keyword">_</span><span class="number">12</span>WP<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">11</span>藁城藁城nsns<span class="keyword">_</span><span class="number">12</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">12</span>东站东站nn<span class="keyword">_</span><span class="number">7</span>COO<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">13</span>双向双向dd<span class="keyword">_</span><span class="number">14</span>ADV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">14</span>关闭关闭vv<span class="keyword">_</span><span class="number">3</span>COO<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">15</span>。。wpw<span class="keyword">_</span><span class="number">3</span>WP<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">16</span>因因pp<span class="keyword">_</span><span class="number">24</span>ADV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">17</span>沧州沧州nsns<span class="keyword">_</span><span class="number">18</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">18</span>服务区服务区nn<span class="keyword">_</span><span class="number">19</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">19</span>附近附近ndf<span class="keyword">_</span><span class="number">21</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">20</span>K81K81wsnx<span class="keyword">_</span><span class="number">21</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">21</span>处处nn<span class="keyword">_</span><span class="number">22</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">22</span>黄骅黄骅nsns<span class="keyword">_</span><span class="number">23</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">23</span>方向方向nn<span class="keyword">_</span><span class="number">16</span>POB<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">24</span>发生发生vv<span class="keyword">_</span><span class="number">3</span>COO<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">25</span>交通事故交通事故nn<span class="keyword">_</span><span class="number">24</span>VOB<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">26</span>,,wpw<span class="keyword">_</span><span class="number">24</span>WP<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">27</span>沧州沧州nsns<span class="keyword">_</span><span class="number">28</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">28</span>服务区服务区nn<span class="keyword">_</span><span class="number">29</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">29</span>附近附近ndf<span class="keyword">_</span><span class="number">31</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">30</span>K81K81wsnx<span class="keyword">_</span><span class="number">31</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">31</span>处处nn<span class="keyword">_</span><span class="number">34</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">32</span>黄骅黄骅nsns<span class="keyword">_</span><span class="number">33</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">33</span>方向方向nn<span class="keyword">_</span><span class="number">34</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">34</span>车辆车辆nn<span class="keyword">_</span><span class="number">36</span>SBV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">35</span>缓慢缓慢aad<span class="keyword">_</span><span class="number">36</span>ADV<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">36</span>通行通行vv<span class="keyword">_</span><span class="number">24</span>COO<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">37</span>约约dd<span class="keyword">_</span><span class="number">38</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">38</span><span class="number">4</span><span class="number">4</span>mm<span class="keyword">_</span><span class="number">39</span>ATT<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">39</span>公里公里qq<span class="keyword">_</span><span class="number">36</span>CMP<span class="keyword">_</span><span class="keyword">_</span></span><br><span class="line"><span class="number">40</span>。。wpw<span class="keyword">_</span><span class="number">3</span>WP<span class="keyword">_</span><span class="keyword">_</span></span><br></pre></td></tr></table></figure><p><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/NLP/test_dependency.png" alt><br>核心代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordBean</span><span class="params">(object)</span>:</span></span><br><span class="line">   <span class="string">''' 扩展conllword，存储父节点与孩子节点索引 '''</span></span><br><span class="line"></span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">           self.lemma = <span class="literal">None</span></span><br><span class="line">           self.postag = <span class="literal">None</span></span><br><span class="line">           self.relation = <span class="literal">None</span></span><br><span class="line">           self.head_id = <span class="literal">None</span></span><br><span class="line">           self.flag = <span class="literal">True</span>  <span class="comment"># 是否还处于树中标志位，已合并的ATT将置为False</span></span><br><span class="line">           self.child = []</span><br><span class="line">   </span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">set_word</span><span class="params">(self, conll_word)</span>:</span></span><br><span class="line">           self.lemma = conll_word.LEMMA</span><br><span class="line">           self.postag = conll_word.POSTAG</span><br><span class="line">           self.relation = conll_word.DEPREL</span><br><span class="line">           self.head_id = conll_word.HEAD.ID - <span class="number">1</span></span><br><span class="line">   </span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">add_child</span><span class="params">(self, child_id, child_relation)</span>:</span></span><br><span class="line">           self.child.append((child_id, child_relation))</span><br><span class="line">       </span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">extract_entity_tuple</span><span class="params">(self, dependency_tree, seed_id, head_type=<span class="string">''</span>)</span>:</span></span><br><span class="line">       <span class="string">'''自上而下解析树'''</span></span><br><span class="line">       res_entity_tuple_list = []</span><br><span class="line">       res_dict = dict()</span><br><span class="line">       res_dict[<span class="string">'time'</span>] = []</span><br><span class="line">       res_dict[<span class="string">'reason'</span>] = []</span><br><span class="line">       res_dict[<span class="string">'place'</span>] = []</span><br><span class="line">       res_dict[<span class="string">'status'</span>] = []</span><br><span class="line">       <span class="comment"># 判断是否是叶子节点</span></span><br><span class="line">       <span class="keyword">if</span> len(dependency_tree[seed_id].child) == <span class="number">0</span>:</span><br><span class="line">           <span class="keyword">if</span> dependency_tree[seed_id].lemma <span class="keyword">in</span> self.status_set:</span><br><span class="line">               res_dict[<span class="string">'status'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           <span class="keyword">elif</span> self.check_is_time(dependency_tree, seed_id):</span><br><span class="line">               res_dict[<span class="string">'time'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           <span class="keyword">elif</span> head_type == <span class="string">'reason'</span>:</span><br><span class="line">               res_dict[<span class="string">'reason'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           <span class="keyword">elif</span> head_type == <span class="string">'time'</span>:</span><br><span class="line">               res_dict[<span class="string">'time'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               <span class="keyword">if</span> <span class="keyword">not</span> dependency_tree[seed_id].lemma <span class="keyword">in</span> self.discard_word_set:</span><br><span class="line">                   res_dict[<span class="string">'place'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           res_entity_tuple_list.append(res_dict)</span><br><span class="line">           <span class="keyword">return</span> res_entity_tuple_list</span><br><span class="line"></span><br><span class="line">       <span class="comment"># 非叶子节点需要向下递归解析</span></span><br><span class="line">       <span class="keyword">if</span> dependency_tree[seed_id].lemma <span class="keyword">in</span> self.status_set:</span><br><span class="line">           <span class="comment"># 当前节点为状态节点</span></span><br><span class="line">           status_merge_list = []</span><br><span class="line">           <span class="keyword">for</span> c_id, c_relation <span class="keyword">in</span> dependency_tree[seed_id].child:</span><br><span class="line">               child_bean = dependency_tree[c_id]</span><br><span class="line">               <span class="keyword">if</span> c_relation <span class="keyword">in</span> (<span class="string">'COO'</span>, <span class="string">'并列关系'</span>):</span><br><span class="line">                   <span class="comment"># 假设状态下不存在嵌套状态，有视为补充</span></span><br><span class="line">                   <span class="keyword">if</span> dependency_tree[c_id].postag == <span class="string">'v'</span> <span class="keyword">and</span> len(dependency_tree[c_id].child) == <span class="number">0</span>:</span><br><span class="line">                       res_dict[<span class="string">'status'</span>].append((c_id, dependency_tree[c_id].lemma))</span><br><span class="line">                   <span class="keyword">else</span>:</span><br><span class="line">                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)</span><br><span class="line">                       <span class="keyword">for</span> child_dict <span class="keyword">in</span> child_dict_list:</span><br><span class="line">                           self.merge_two_tuple_dict(res_dict, child_dict)</span><br><span class="line">               <span class="keyword">elif</span> c_relation <span class="keyword">in</span> (<span class="string">'ADV'</span>, <span class="string">'状中结构'</span>):</span><br><span class="line">                   <span class="comment"># 处理状中结构</span></span><br><span class="line">                   <span class="keyword">if</span> child_bean.lemma <span class="keyword">in</span> (<span class="string">'因'</span>, <span class="string">'受'</span>, <span class="string">'由于'</span>):</span><br><span class="line">                       <span class="comment"># 处理原因</span></span><br><span class="line">                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)</span><br><span class="line">                       <span class="keyword">for</span> child_dict <span class="keyword">in</span> child_dict_list:</span><br><span class="line">                           self.merge_two_tuple_dict(res_dict, child_dict)</span><br><span class="line">                   <span class="keyword">elif</span> child_bean.lemma == <span class="string">'处'</span> <span class="keyword">or</span> child_bean.postag == <span class="string">'p'</span>:</span><br><span class="line">                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)</span><br><span class="line">                       <span class="keyword">for</span> child_dict <span class="keyword">in</span> child_dict_list:</span><br><span class="line">                           self.merge_two_tuple_dict(res_dict, child_dict)</span><br><span class="line">                   <span class="keyword">elif</span> child_bean.postag <span class="keyword">in</span> (<span class="string">'a'</span>, <span class="string">'ad'</span>, <span class="string">'d'</span>):</span><br><span class="line">                       self.merge_att(dependency_tree, c_id)</span><br><span class="line">                       status_merge_list.append(c_id)</span><br><span class="line">                   <span class="keyword">elif</span> self.check_is_time(dependency_tree, c_id):</span><br><span class="line">                       self.merge_att(dependency_tree, c_id)</span><br><span class="line">                       res_dict[<span class="string">'time'</span>].append((c_id, dependency_tree[c_id].lemma))</span><br><span class="line">               <span class="keyword">elif</span> c_relation <span class="keyword">in</span> (<span class="string">'POB'</span>, <span class="string">'介宾关系'</span>) <span class="keyword">and</span> </span><br><span class="line">                       child_bean.lemma <span class="keyword">in</span> (<span class="string">'因'</span>, <span class="string">'受'</span>, <span class="string">'由于'</span>):</span><br><span class="line">                   self.merge_att(dependency_tree, c_id)</span><br><span class="line">                   res_dict[<span class="string">'reason'</span>].append((c_id, dependency_tree[c_id].lemma))</span><br><span class="line">               <span class="keyword">elif</span> c_relation <span class="keyword">in</span> (<span class="string">'CMP'</span>, <span class="string">'动补结构'</span>):</span><br><span class="line">                   self.merge_att(dependency_tree, c_id)</span><br><span class="line">                   status_merge_list.append(c_id)</span><br><span class="line">               <span class="keyword">elif</span> c_relation <span class="keyword">in</span> (<span class="string">'SBV'</span>, <span class="string">'主谓关系'</span>):</span><br><span class="line">                   <span class="comment"># 处理主谓关系，解析具体地点</span></span><br><span class="line">                   child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)</span><br><span class="line">                   <span class="keyword">for</span> child_dict <span class="keyword">in</span> child_dict_list:</span><br><span class="line">                       self.merge_two_tuple_dict(res_dict, child_dict)</span><br><span class="line">               <span class="keyword">elif</span> c_relation <span class="keyword">in</span> (<span class="string">'VOB'</span>, <span class="string">'动宾关系'</span>):</span><br><span class="line">                   <span class="keyword">if</span> len(dependency_tree[c_id].child) == <span class="number">0</span>:</span><br><span class="line">                       res_dict[<span class="string">'status'</span>].append((c_id, dependency_tree[c_id].lemma))</span><br><span class="line">                   <span class="keyword">else</span>:</span><br><span class="line">                       child_dict_list = self.extract_entity_tuple(dependency_tree, c_id)</span><br><span class="line">                       <span class="keyword">for</span> child_dict <span class="keyword">in</span> child_dict_list:</span><br><span class="line">                           self.merge_two_tuple_dict(res_dict, child_dict)</span><br><span class="line"></span><br><span class="line">           status_buffer = []</span><br><span class="line">           status_merge_list.append(seed_id)</span><br><span class="line">           status_merge_list.sort()</span><br><span class="line">           <span class="keyword">for</span> id <span class="keyword">in</span> status_merge_list:</span><br><span class="line">               status_buffer.append(dependency_tree[id].lemma)</span><br><span class="line">           res_dict[<span class="string">'status'</span>].append((seed_id, <span class="string">''</span>.join(status_buffer)))</span><br><span class="line"></span><br><span class="line">           res_entity_tuple_list.append(res_dict)</span><br><span class="line">           <span class="keyword">return</span> res_entity_tuple_list</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           <span class="comment"># 当前节点为非状态节点</span></span><br><span class="line">           pre_head_type = head_type</span><br><span class="line">           <span class="keyword">if</span> self.check_is_time(dependency_tree, seed_id):</span><br><span class="line">               <span class="comment"># 为时间节点</span></span><br><span class="line">               head_type = <span class="string">'time'</span></span><br><span class="line">               res_dict[<span class="string">'time'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           <span class="keyword">elif</span> dependency_tree[seed_id].lemma <span class="keyword">in</span> (<span class="string">'因'</span>, <span class="string">'受'</span>, <span class="string">'由于'</span>) <span class="keyword">or</span> head_type == <span class="string">'reason'</span>:</span><br><span class="line">               <span class="comment"># 为原因节点</span></span><br><span class="line">               head_type = <span class="string">'reason'</span></span><br><span class="line">               res_dict[<span class="string">'reason'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               <span class="keyword">if</span> <span class="keyword">not</span> dependency_tree[seed_id].lemma <span class="keyword">in</span> self.discard_word_set:</span><br><span class="line">                   res_dict[<span class="string">'place'</span>].append((seed_id, dependency_tree[seed_id].lemma))</span><br><span class="line">           child_dict_list = []</span><br><span class="line">           coo_list = []</span><br><span class="line">           <span class="keyword">for</span> c_id, c_relation <span class="keyword">in</span> dependency_tree[seed_id].child:</span><br><span class="line">               <span class="comment"># if c_relation in ('COO', '并列关系'):</span></span><br><span class="line">               <span class="comment">#     coo_list.append(c_id)</span></span><br><span class="line">               <span class="keyword">if</span> c_relation <span class="keyword">in</span> (<span class="string">'WP'</span>, <span class="string">'标点符号'</span>):</span><br><span class="line">                   <span class="keyword">continue</span></span><br><span class="line">               <span class="keyword">else</span>:</span><br><span class="line">                   <span class="keyword">if</span> head_type == <span class="string">'reason'</span> <span class="keyword">and</span> <span class="keyword">not</span> pre_head_type == <span class="string">'reason'</span>:</span><br><span class="line">                       <span class="keyword">if</span> c_relation <span class="keyword">in</span> (<span class="string">'POB'</span>, <span class="string">'介宾关系'</span>):</span><br><span class="line">                           child_dict_list.extend(self.extract_entity_tuple(dependency_tree, c_id, head_type))</span><br><span class="line">                           <span class="keyword">if</span> dependency_tree[c_id].lemma <span class="keyword">in</span> self.status_set:</span><br><span class="line">                               res_dict[<span class="string">'reason'</span>].append((c_id, dependency_tree[c_id].lemma))</span><br><span class="line">                   <span class="keyword">else</span>:</span><br><span class="line">                       child_dict_list.extend(self.extract_entity_tuple(dependency_tree, c_id, head_type))</span><br><span class="line"></span><br><span class="line">           <span class="comment"># 先合并非状态</span></span><br><span class="line">           status_dict_list = []</span><br><span class="line">           <span class="keyword">for</span> child_dict <span class="keyword">in</span> child_dict_list:</span><br><span class="line">               <span class="keyword">if</span> len(child_dict[<span class="string">'status'</span>]) &gt; <span class="number">0</span>:</span><br><span class="line">                   status_dict_list.append(child_dict)</span><br><span class="line">               <span class="keyword">else</span>:</span><br><span class="line">                   self.merge_two_tuple_dict(res_dict, child_dict)</span><br><span class="line"></span><br><span class="line">           <span class="comment"># 再合并存在状态的</span></span><br><span class="line">           <span class="keyword">if</span> len(status_dict_list) == <span class="number">0</span>:</span><br><span class="line">               res_entity_tuple_list.append(res_dict)</span><br><span class="line">           <span class="keyword">else</span>:</span><br><span class="line">               <span class="keyword">for</span> child_dict <span class="keyword">in</span> status_dict_list:</span><br><span class="line">                   <span class="comment"># tmp_dict = res_dict.copy()</span></span><br><span class="line">                   tmp_dict = copy.deepcopy(res_dict)</span><br><span class="line">                   self.merge_two_tuple_dict(tmp_dict, child_dict)</span><br><span class="line">                   res_entity_tuple_list.append(tmp_dict)</span><br><span class="line"></span><br><span class="line">           <span class="keyword">return</span> res_entity_tuple_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">extract_information</span><span class="params">(self, line)</span>:</span></span><br><span class="line">       segs = self.nlp_tokenizer.seg(line)</span><br><span class="line">       <span class="comment"># fix segs</span></span><br><span class="line">       self.fix_seged_postag(segs)</span><br><span class="line">       conll_words = self.parser.parse(segs).getWordArray()</span><br><span class="line">       dependency_tree, root_id = self.construct_dependency_tree(conll_words)</span><br><span class="line"></span><br><span class="line">       res_entity.append(self.extract_entity_tuple(dependency_tree, i))</span><br><span class="line">       res_entity = self.extract_entity_tuple(dependency_tree, root_id)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># print entity tuples</span></span><br><span class="line">       <span class="keyword">for</span> entity <span class="keyword">in</span> res_entity:</span><br><span class="line">           entity[<span class="string">'time'</span>].sort()</span><br><span class="line">           entity[<span class="string">'place'</span>].sort()</span><br><span class="line">           entity[<span class="string">'reason'</span>].sort()</span><br><span class="line">           entity[<span class="string">'status'</span>].sort()</span><br><span class="line">           self.fix_entity_tuple_dict(entity)</span><br><span class="line">           print(entity)</span><br><span class="line">       <span class="keyword">return</span> res_entity</span><br></pre></td></tr></table></figure><p>实验结果：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 输入句子</span><br><span class="line"><span class="string">'黄石高速:因交通管制,晋州站、辛集站、藁城东站双向关闭。因沧州服务区附近K81处黄骅方向发生交通事故,沧州服务区附近K81处黄骅方向车辆缓慢通行约4公里。'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 信息提取结果</span><br><span class="line">&#123;<span class="string">'time'</span>: [], <span class="string">'reason'</span>: [<span class="string">'因交通管制'</span>], <span class="string">'place'</span>: [<span class="string">'黄石高速:, 晋州站, 辛集站, 藁城东站'</span>], <span class="string">'status'</span>: [<span class="string">'交通管制, 双向关闭'</span>]&#125;</span><br><span class="line">&#123;<span class="string">'time'</span>: [], <span class="string">'reason'</span>: [<span class="string">'因沧州服务区附近K81处黄骅方向'</span>], <span class="string">'place'</span>: [<span class="string">'黄石高速:, 发生交通事故, 沧州服务区附近K81处黄骅方向'</span>], <span class="string">'status'</span>: [<span class="string">'缓慢通行约4公里'</span>]&#125;</span><br></pre></td></tr></table></figure><p>可以看到，第一句的解析没问题，但是第二句原因的解析边界出错。当前的解析方法仍然比较依赖于句法依存树的准确性，实体的边界的准确性不够，也是需要改进的地方。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;信息抽取是一个常见的nlp任务，为经常一起提到的知识图谱的基础。&lt;/p&gt;
&lt;p&gt;这里有一份比较好的知识图谱入门资料:&lt;br&gt;
&lt;a href=&quot;https://kgtutorial.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mining Knowledge Graphs from Text&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;信息抽取分为有监督和无监督方法。实际中监督信息往往是缺失的，所以本文主要提无监督方法。&lt;/p&gt;
&lt;p&gt;在无监督方法中，一个广泛采用的工具是句法依存树，或者叫句法解析树（Dependency Tree）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://zekizz.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://zekizz.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>样本类别不均衡处理</title>
    <link href="https://zekizz.github.io/ML/imblanced-samples/"/>
    <id>https://zekizz.github.io/ML/imblanced-samples/</id>
    <published>2018-12-16T11:29:59.000Z</published>
    <updated>2019-10-13T09:20:48.304Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>分类任务中样本类别不均衡是常有的事，当样本之间的不均衡程度较小的时候，可以不作处理，当正负样本比例较大（比如10:1）且训练数据较少的时候，就需要做不均衡的处理。常见的处理方式可以分为如下几类：</p><ul><li>采样方法<ul><li>下采样（或欠采样，under-sampling）</li><li>上采样（或过采样，over-sampling）</li><li>混合采样</li></ul></li><li>数据增强<ul><li>收集更多的数据</li><li>造数据</li></ul></li><li>更改评价指标</li><li>cost sensitive（代价敏感，class weight）</li><li>模型集成</li><li>one-class classifier</li></ul><a id="more"></a><h2>采样方法</h2><p>对于下采样，最简单方便的是随机采样。以两分类为例，这里涉及两个问题：</p><ol><li>应该采样哪些样本？</li><li>采样比例应该采样为1:1么？</li></ol><p>理想情况下，我们希望采样的样本能反应实际的数据空间分布。所以可以通过对多数类进行聚类，然后挑选中心。<a href="http://imbalanced-learn.org/en/stable/index.html" target="_blank" rel="noopener">imblearn</a>中的<a href="http://imbalanced-learn.org/en/stable/generated/imblearn.under_sampling.ClusterCentroids.html#imblearn.under_sampling.ClusterCentroids" target="_blank" rel="noopener">ClusterCentroids</a>就是这种思想。<a href="http://imbalanced-learn.org/en/stable/index.html" target="_blank" rel="noopener">imblearn</a>还提供了其他几种样本挑选方式：</p><ol><li>NearMiss-1：到少数类样本的n近邻的平均距离的最小的多数样本</li><li>NearMiss-2：到n个最远的少数类样本的平均距离的最小的多数样本</li><li>NearMiss-3：对于每个少数类样本，先保留其多数类M近邻，再从中挑选N近邻平均距离最大的</li><li>EditedNearestNeighbours：通过近邻，移除与邻居差异较大的样本</li></ol><p>还有几种EditedNearestNeighbours的扩张不再赘述。以上几种方法的本质是<strong>挑选分类边界附近的样本</strong>。这一部分其实可以参考半监督学习，可参考周志华老师的一篇经典paper:<a href="https://papers.nips.cc/paper/4176-active-learning-by-querying-informative-and-representative-examples.pdf" target="_blank" rel="noopener">Active Learning by Querying Informative and Representative Examples</a>，挑选最具信息量和最具代表性的样本。</p><p>对于上采样，这一部分与数据增强有一部分重叠，因为其本质是生成新样本。</p><p>简单copy少数样本，新生成的样本也就是数据集中样本的一个复制，这样对有些算法是无效的。</p><p>其次，可以通过简单差值的方式生成新样本。<br>最后，常用的两个方法：</p><ol><li>the Synthetic Minority Oversampling Technique (SMOTE)</li><li>the Adaptive Synthetic (ADASYN)</li></ol><h2>更改评价指标</h2><p>一般分类问题的评价指标为，准确率和p、r、f值。当类别失衡时，准确率就不太可信。<br>这是还可以使用AUC和ROC，但是AUC在类别不均衡时也不太可信，一般还要综合看一下PR曲线。</p><h2>代价敏感</h2><p>我使用代价敏感这个词，最开始是在贝叶斯中学习得来的，有个最小风险贝叶斯估计。其中会指定一个风险矩阵，调整loss function。</p><p>一般机器学习方法的损失函数为交叉熵、log损失、最小二乘、指数损失、hinge损失等。<br>下面以深度学习中的交叉熵为例。</p><p>常用计算方式如下</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">losses = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax_cross_entropy_with_logits</span>(logits=self<span class="selector-class">.scores</span>, labels=self.input_y)</span><br></pre></td></tr></table></figure><p>那么能不能像sklearn那样简单添加class weight的方式来调整损失呢？</p><p>答案是可以的。tf中有一个API可以帮忙解决来，<strong>tf.losses.softmax_cross_entropy</strong>。<br>其中有一个参数为：weights。反映的是batch中每个样本的权重，我们可以通过生成这个weights来变相实现class_weight。具体方式如下：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.<span class="attr">class_weight</span> = tf.placeholder(tf.float32, <span class="attr">shape=[1,</span> num_classes], <span class="attr">name='class_weight')</span></span><br><span class="line"></span><br><span class="line"><span class="attr">sample_weights</span> = tf.reduce_sum(tf.multiply(self.input_y, self.class_weight), <span class="number">1</span>) <span class="comment"># size of class_weights: [1, num_classes]</span></span><br><span class="line"><span class="attr">losses</span> = tf.losses.softmax_cross_entropy(<span class="attr">onehot_labels=self.input_y,</span> <span class="attr">logits=self.scores,</span></span><br><span class="line">                                                     <span class="attr">weights=sample_weights)</span></span><br></pre></td></tr></table></figure><p>这里的class_weight可以手工指定，也可以通过训练数据计算得出。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categoris</span> = np.argmax(y_train, axis=<span class="number">1</span>)</span><br><span class="line"><span class="attr">train_class_weight</span> = n_train_samples / (n_classes * np.bincount(categoris))</span><br><span class="line"><span class="attr">train_class_weight</span> = train_class_weight.reshape(<span class="number">1</span>, n_classes)</span><br></pre></td></tr></table></figure><h2>模型集成</h2><p>模型集成也分为两种</p><ol><li>单纯的集成方法，比如adboost、gbdt、random forest等；</li><li>通过数据采样来造成训练样本差异</li></ol><p>第一种方法就不用说了，树模型天生对数据不均衡不敏感。<br>第二种方法，通过不同的采样率生成不同正负样本比例的数据集进行训练，然后再集成这些模型。</p><p>比如：</p><ul><li>分别设置采样率为1:1, 1:2, 1:3等的采样数据集</li><li>保留n个少数类样本，并随机抽取10*n 个多数类样本。然后，只需将 10*n个样本分成10份，并训练10个不同的模型。</li></ul><h2>one-class classifier</h2><p>转为一分类问题，one-class classifier本身为一种异常检查算法，尽量学得目标类别的边界。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分类任务中样本类别不均衡是常有的事，当样本之间的不均衡程度较小的时候，可以不作处理，当正负样本比例较大（比如10:1）且训练数据较少的时候，就需要做不均衡的处理。常见的处理方式可以分为如下几类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采样方法
&lt;ul&gt;
&lt;li&gt;下采样（或欠采样，under-sampling）&lt;/li&gt;
&lt;li&gt;上采样（或过采样，over-sampling）&lt;/li&gt;
&lt;li&gt;混合采样&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据增强
&lt;ul&gt;
&lt;li&gt;收集更多的数据&lt;/li&gt;
&lt;li&gt;造数据&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;更改评价指标&lt;/li&gt;
&lt;li&gt;cost sensitive（代价敏感，class weight）&lt;/li&gt;
&lt;li&gt;模型集成&lt;/li&gt;
&lt;li&gt;one-class classifier&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>html自定义标签</title>
    <link href="https://zekizz.github.io/html/html-tag/"/>
    <id>https://zekizz.github.io/html/html-tag/</id>
    <published>2018-10-12T14:30:17.000Z</published>
    <updated>2019-10-13T09:20:48.302Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>我们在分析文本时，比如命名实体识别，可能想将不同的实体词文本用不同的颜色高亮标识出来，那么采用html自定义标签就是一个比较轻量的方法。</p><a id="more"></a><p>给出一个例子</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span>自定义标签Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">style</span> <span class="attr">type</span>=<span class="string">"text/css"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">time&#123;</span><br><span class="line"><span class="css"><span class="selector-tag">color</span>: <span class="selector-id">#2980B9</span> ;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location&#123;</span><br><span class="line"><span class="css"><span class="selector-tag">color</span>: <span class="selector-id">#8E44AD</span>;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">status&#123;</span><br><span class="line"><span class="css"><span class="selector-tag">color</span>: <span class="selector-id">#FF0000</span>;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"margin: 20px"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h3</span>&gt;</span>自定义标签：<span class="tag">&lt;/<span class="name">h3</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">time</span>&gt;</span>时间<span class="tag">&lt;/<span class="name">time</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">location</span>&gt;</span>地点<span class="tag">&lt;/<span class="name">location</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">status</span>&gt;</span>状态<span class="tag">&lt;/<span class="name">status</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"margin: 20px"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">time</span>&gt;</span>2018年10月12日<span class="tag">&lt;/<span class="name">time</span>&gt;</span>，我在<span class="tag">&lt;<span class="name">location</span>&gt;</span>中关村<span class="tag">&lt;/<span class="name">location</span>&gt;</span><span class="tag">&lt;<span class="name">status</span>&gt;</span>打代码<span class="tag">&lt;/<span class="name">status</span>&gt;</span>。</span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>效果<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/html/customer_tag.png" alt></p><p>这里需要注意的是标签之间的覆盖规则，选择最近的标签，参见<a href="https://blog.csdn.net/wl110231/article/details/7642652" target="_blank" rel="noopener">CSS样式覆盖规则</a></p><p>给出两个网页颜色选择器</p><ul><li><a href="https://tools.guardui.net/nose/page.html" target="_blank" rel="noopener">网页颜色选择器</a></li><li><a href="https://htmlcolorcodes.com/zh/" target="_blank" rel="noopener">html颜色代码</a></li></ul><p>参考：</p><p><a href="http://www.ruanyifeng.com/blog/2017/06/custom-elements.html" target="_blank" rel="noopener">HTML 自定义元素教程</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在分析文本时，比如命名实体识别，可能想将不同的实体词文本用不同的颜色高亮标识出来，那么采用html自定义标签就是一个比较轻量的方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="html" scheme="https://zekizz.github.io/categories/html/"/>
    
    
      <category term="html" scheme="https://zekizz.github.io/tags/html/"/>
    
      <category term="前端" scheme="https://zekizz.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
  </entry>
  
  <entry>
    <title>网页文本中空格</title>
    <link href="https://zekizz.github.io/html/web-space/"/>
    <id>https://zekizz.github.io/html/web-space/</id>
    <published>2018-10-12T14:28:11.000Z</published>
    <updated>2019-10-13T09:20:48.304Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>网页中文本空格存在如下几类</p><ul><li>u200b：零宽度空格</li><li>u0020：普通空格</li><li>u00a0：不换行空格</li><li>  不换行空格的转义字符</li></ul><a id="more"></a><p>处理时，先替换成常规的空格字符</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sentence.replaceAll(<span class="string">"\\u200B|\\u0020|\\u00a0"</span>, <span class="string">" "</span>).trim()</span><br></pre></td></tr></table></figure><p>参考：</p><p><a href="https://objcer.com/2017/05/22/Unicode-spaces/" target="_blank" rel="noopener">Unicode 之神奇的空格</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;网页中文本空格存在如下几类&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;u200b：零宽度空格&lt;/li&gt;
&lt;li&gt;u0020：普通空格&lt;/li&gt;
&lt;li&gt;u00a0：不换行空格&lt;/li&gt;
&lt;li&gt;  不换行空格的转义字符&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="html" scheme="https://zekizz.github.io/categories/html/"/>
    
    
      <category term="html" scheme="https://zekizz.github.io/tags/html/"/>
    
      <category term="前端" scheme="https://zekizz.github.io/tags/%E5%89%8D%E7%AB%AF/"/>
    
      <category term="文本处理" scheme="https://zekizz.github.io/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>双数组Trie树(DoubleArrayTrie)</title>
    <link href="https://zekizz.github.io/NLP/DoubleArrayTrie/"/>
    <id>https://zekizz.github.io/NLP/DoubleArrayTrie/</id>
    <published>2018-10-08T16:13:19.000Z</published>
    <updated>2019-10-13T09:20:48.299Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>双数组Trie树(DoubleArrayTrie)</p><p>NLP领域又很多需要词典匹配的需求，也就是，字典树+词典的基本方案。对于中文这种字较多的语言，双数组Trie树是比Trie树更好的结构。</p><a id="more"></a><p>原理讲解：</p><ul><li><a href="https://segmentfault.com/a/1190000008877595" target="_blank" rel="noopener">小白详解 Trie 树</a></li><li><a href="https://zhuanlan.zhihu.com/p/35193582" target="_blank" rel="noopener">双数组前缀树（Double-Array Trie）</a></li><li><a href="https://blog.csdn.net/heiyeshuwu/article/details/42526461" target="_blank" rel="noopener">Trie树优化算法：Double Array Trie 双数组Trie</a></li></ul><p>代码实现：</p><ul><li><a href="https://github.com/komiya-atsushi/darts-java" target="_blank" rel="noopener">darts-java: Double-ARray Trie System Java implementation</a></li><li><a href="http://www.hankcs.com/program/java/%E5%8F%8C%E6%95%B0%E7%BB%84trie%E6%A0%91doublearraytriejava%E5%AE%9E%E7%8E%B0.html" target="_blank" rel="noopener">双数组Trie树(DoubleArrayTrie)Java实现</a></li><li><a href="https://cloud.tencent.com/developer/article/1057813" target="_blank" rel="noopener">从Trie树到双数组Trie树</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;双数组Trie树(DoubleArrayTrie)&lt;/p&gt;
&lt;p&gt;NLP领域又很多需要词典匹配的需求，也就是，字典树+词典的基本方案。对于中文这种字较多的语言，双数组Trie树是比Trie树更好的结构。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://zekizz.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://zekizz.github.io/tags/NLP/"/>
    
      <category term="数据结构" scheme="https://zekizz.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统学习笔记</title>
    <link href="https://zekizz.github.io/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/recommendation-system/"/>
    <id>https://zekizz.github.io/推荐系统/recommendation-system/</id>
    <published>2018-10-06T12:36:45.000Z</published>
    <updated>2019-10-13T09:20:48.301Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>最近对以前学习推荐系统的知识点笔记的一个汇总。</p><a id="more"></a><h1>概念</h1><p>你需要推荐系统吗？</p><ol><li>看看产品的目的：建立越多连接越好。</li><li>看产品现有的连接：产品的数量</li></ol><p>一个简单指标</p><p>$$<br>\frac{\Delta connection}{ \Delta user \times \Delta item}<br>$$</p><p>分子是增加的连接数，分母是增加的活跃用户数和增加的有效物品数。<br>如果增加的连接数主要靠增加的活跃用户数和增加的物品数贡献，则该值较小，不适合加入推荐系统。反之，如果增加的连接数和新增活跃用户和物品关系不大，说明连接数已经有自发生长的趋势，适合加入推荐系统加速这个过程。</p><p>推荐系统的问题/任务</p><ol><li>评分预测</li><li>行为预测</li></ol><h2>评分预测</h2><p>显示打分，目标减小预测分数与实际分数之间的误差，回归问题。</p><p>评价标准：RMSE、MAE</p><p>$$ RMSE = \sqrt{ \frac{\sum_{t=1}^n (\widehat{y}_t - y_t)^2}{n} }  $$</p><p>$$ MAE = \frac{\sum_{t=1}^n | \widehat{y}_t - y_t| }{n} $$</p><p>评分预测存在的问题：</p><ol><li>数据不易收集</li><li>数据质量不能保证</li><li>评分分布不稳定</li></ol><p>显示反馈很少，更多的是隐式反馈，通常为各类用户行为。行为预测更多地利用这部分数据。</p><h2>行为预测</h2><p>隐式反馈：登陆刷新、购买、收藏、浏览、点击等</p><p>行为预测有两种方式：</p><ol><li>直接预测用户行为：CTR预估</li><li>预测物品的相对排序：learning2rank</li></ol><p>隐式数据的好处：</p><ol><li>比显式更加稠密</li><li>隐式更加代表用户的真实想法</li><li>隐式反馈常常和模型的目标函数关联更密切，也因此通常更容易在 AB 测试中和测试指标挂钩。</li></ol><p>推荐系统中几个普遍的问题</p><ol><li>冷启动问题</li><li>探索与利用问题：Exploit 和 Explore （EE问题）</li><li>安全问题<ol><li>给出不靠谱的推荐结果，影响用户体验并最终影响品牌形象；</li><li>收集了不靠谱的脏数据，这个影响会一直持续留存在产品中，很难完全消除；</li><li>损失了产品的商业利益，这个是直接的经济损失。</li></ol></li></ol><h1>推荐方法</h1><h2>基于内容推荐</h2><h3>用户画像</h3><blockquote><p>用户画像应该给机器看，而不是给人看。</p></blockquote><p>用户画像是将用户向量化后的结果，其关键因素：维度和量化。</p><p>维度：</p><ol><li>每个维度的名称是可理解的</li><li>维度的数目是拍脑袋决定的</li><li>维度的筛选也是拍脑袋决定的</li></ol><p>维度越多越精细，但是计算代价会变大，同时也会引入噪声</p><p>量化：<br>不要为了用户画像而用户画像，它只是推荐系统的一个副产品，所以要根据推荐效果（排序好坏、召回覆盖等指标）来指导用户画像量化。</p><p>用户画像构建方法</p><ol><li>直接使用原始数据。比如人口统计学信息、购买历史、浏览历史等。</li><li>堆数据。堆积历史数据，做统计工作，常用的比如兴趣标签。</li><li>机器学习方法，比如隐语义模型、矩阵分解等embedding，构建无法直观理解的稠密向量。</li></ol><p>从文本数据中挖掘用户画像</p><ul><li>用户：昵称、姓名、性别、动态、评论等</li><li>物品：标题、描述、内容等</li></ul><p>构建用户画像步骤</p><ol><li>分析用户的文本和物品的文本，使其结构化；</li><li>标签选择，为用户挑选有信息量的结构化数据，作为其画像内容。</li></ol><p><strong>结构化文本算法</strong></p><ol><li>关键词提取：TF-IDF、TextRank。</li><li>实体识别NER：常用基于词典的方法结合 CRF 模型。</li><li>内容分类：将文本按照频道体系分类，用分类来表达较粗粒度的结构化信息。短文本常用Facebook 开源的 <strong>FastText</strong>。</li><li>聚类：无监督聚类，分簇，使用编号</li><li>主题模型：LDA</li><li>编码embedding</li></ol><p>$$ TF = count(w) $$</p><p>$$ IDF = log \frac{N}{n+1} $$</p><p>实体识别还有比较实用化的非模型做法：词典法。提前准备好各种实体的词典，使用trie-tree结构存储，拿着分好的词去词典寻找。<br>工业级工具：spaCy</p><p>LDA工具：Gensim和PLDA等</p><p><strong>标签选择</strong></p><p>通过户端的文本，物品端的文本如何结构化，得到了诸如标签（关键词、分类等）、主题、词嵌入向量。接下来就是第二步：如何把物品的结构化信息给用户呢？</p><p>我们把用户对物品的行为，消费或者没有消费看成是一个分类问题。用户用实际行动帮我们标注了若干数据，那么挑选出他实际感兴趣的特性就变成了特征选择问题。</p><p>最常用的是两个方法：卡方检验（CHI）和信息增益（IG）。基本思想是：</p><ol><li>把物品的结构化内容看成文档；</li><li>把用户对物品的行为看成是类别；</li><li>每个用户看见过的物品就是一个文本集合；</li><li>在这个文本集合上使用特征选择算法选出每个用户关心的东西。</li></ol><p>卡方检验</p><table><thead><tr><th>卡方检验</th><th>属于类别C_j</th><th>不属于类别C_j</th><th>总计</th></tr></thead><tbody><tr><td>包含词W_i</td><td>A</td><td>B</td><td>A+B</td></tr><tr><td>不包含词W_i</td><td>C</td><td>D</td><td>C+D</td></tr><tr><td>总计</td><td>A+C</td><td>B+D</td><td>N = A+B+C+D</td></tr></tbody></table><p>计算每一个词和每一个类别的卡方值：</p><p>$$ \chi^2 (W_i, C_j) = \frac{N(AD-BC)^2}{ (A+C)(A+B)(B+D)(C+D)} $$</p><ol><li>每个词和每个类别都要计算，只要对其中一个类别有帮助的词都应该留下；</li><li>由于是比较卡方值的大小，所以公式中的 N 可以不参与计算，因为它对每个词都一样，就是总的文本数；</li><li>卡方值越大，意味着偏离“词和类别相互独立”的假设越远，靠“词和类别互相不独立”这个备择假设越近。</li></ol><p><strong>误区：</strong> 基于内容的推荐系统，标签只是很小一部分。而且就算是标签，衡量质量的方式也不是数目够不够。</p><p>所谓的基于内容推荐，通俗一点来讲，就是一个包装成推荐系统的信息检索系统。这听上去有点残酷，但通常一个复杂的推荐系统很可能是从基于内容推荐成长起来的。</p><p>为什么基于内容的推荐系统这么重要呢？因为内容数据非常易得，哪怕是在一个产品刚刚上线，用心找的话总能找到一些可以使用的内容，不需要有用户行为数据就能够做出推荐系统的第一版。</p><p>要把基于内容的推荐做好，需要做好“抓、洗、挖、算”四门功课。它们分别是：</p><ol><li>抓：一直持续抓数据丰富自己的内容，所以做好一个基于内容的推荐，抓取数据补充内容源，增加分析的维度，两者必不可少。</li><li>洗：冗余的内容、垃圾内容、政治色情等敏感内容等等都需要被洗出去。</li><li>挖：很多推荐系统提升效果并不是用了更复杂的推荐算法，而是对内容的挖掘做得更加深入。</li><li>算：匹配用户的兴趣和物品的属性，计算出更合理的相关性，这是推荐系统本身的使命，不仅仅是基于内容的推荐才要做的。</li></ol><p>结合基于内容推荐的框架看上诉几个步骤</p><p><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/RS/ContentConfig.png" alt></p><p>内容这一端：内容源经过内容分析，得到结构化的内容库和内容模型，也就是物品画像。用户这一端：用户看过推荐列表后，会产生用户行为数据，结合物品画像，经过用户分析得到用户画像。</p><p>内容分析的产出</p><ol><li>结构化内容库</li><li>内容分析模型<br>结构化的内容库，最重要的用途是结合用户反馈行为去学习用户画像。容易被忽略的是第二个用途，在内容分析过程中得到的模型，比如说：</li></ol><ol><li>分类器模型；</li><li>主题模型；</li><li>实体识别模型；</li><li>嵌入模型。</li></ol><p>这些模型主要用在：当新的物品刚刚进入时，需要实时地被推荐出去，这时候对内容的实时分析，提取结构化内容，再于用户画像匹配。</p><p>内容推荐算法</p><ol><li>直接计算相似度，BM25F算法</li><li>转为预估问题、分类问题</li></ol><p>一种最典型的场景：提高某种行为的转化率，如点击、收藏、转发等。那么标准的做法是：收集这类行为的日志数据，转换成训练样本，训练预估模型。</p>每一条样本由两部分构成：一部分是特征，包含用户端的画像内容，物品端的结构化内容，可选的还有日志记录时一些上下文场景信息，如时间、地理位置、设备等等，另一部分就是用户行为，作为标注信息，包含“有反馈”和“无反馈”两类。<p>二分类：LR+GBDT</p><h2>协同过滤</h2><h3>User-based CF</h3><p>$$<br>p(u,i) = \sum_{v \in S(u,K) \cap N(i)} w_{uv} r_{vi}<br>$$</p><p>$S(u, K)$ 和用户 $ u $ 兴趣最接近的K个用户,$N(i)$ 是对物品i有过行为的用户集合，$w_{uv} $ 用户u和v之间的相似度，$ r_{ui} $ 用户v对物品i的兴趣，一般为{0,N}</p><p>用户相似度计算</p><p>用户向量</p><ol><li>向量的维度就是物品的个数；</li><li>向量是稀疏的，也就是说并不是每个维度上都有数值，原因当然很简单，这个用户并不是消费过所有物品，废话嘛，连我们压箱底的都给用户推荐了，那当然不用再推荐什么了；</li><li>向量维度上的取值可以是简单的 0 或者 1，也就是布尔值，1 表示喜欢过，0 表示没有，当然因为是稀疏向量，所以取值为 0 的就忽略了。</li></ol><p>Jaccard相似性</p><p>$$<br>w_{nv} = \frac{|N(u) \cap |N(v)|}{|N(u) \cup |N(v)|}<br>$$<br>余弦相似性<br>$$<br>w_{nv} = \frac{|N(u) \cap |N(v)|}{\sqrt{|N(u)| |N(v)|}}<br>$$</p><p>用户间相似度改进：惩罚热门</p><p>$$<br>w_{uv} = \frac{\sum_ {i \in N(u) \cap N(v)} \frac{1}{log 1+|N(i)|}}{\sqrt{|N(u)| |N(v)|}}<br>$$<br>两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度</p><p>对两两用户都利用余弦相似度计算相似度。这种方法的时间复杂度是O(|U|*|U|)，这在用户数很大时非常耗时，可以采用物品到用户的倒排表。</p><p>存在问题及解决</p><ol><li>构造矩阵：采用稀疏矩阵存储，COO（行号，列号，数值）</li><li>相似度计算：数据量大时处理</li></ol><p>第一个办法是：将相似度计算拆成 Map Reduce 任务，将原始矩阵 Map 成键为用户对，值为两个用户对同一个物品的评分之积，Reduce 阶段对这些乘积再求和，Map Reduce 任务结束后再对这些值归一化；</p><p>map: &lt; &lt;u1,u2&gt;, r_u1i * r_u2i&gt; ， 各个维度上做乘法</p><p>reduce：求和并归一化</p><p>第二个办法是：不用基于用户的协同过滤，采用基于图的算法。</p><p>另外，这种计算对象两两之间的相似度的任务，如果数据量不大，一般来说不超过百万个，然后矩阵又是稀疏的，那么有很多单机版本的工具其实更快，比如 KGraph、 GraphCHI 等。</p><ol start="3"><li>推荐计算</li></ol><p>$$<br>p(u,i) = \sum_{v \in S(u,K) \cap N(i)} w_{uv} r_{vi}<br>$$<br>得到了用户之间的相似度之后。接下来还有一个硬骨头，计算推荐分数。显然，为每一个用户计算每一个物品的推荐分数，计算次数是矩阵的所有元素个数，这个代价，你当然不能接受啊。</p><p>采用MapReduce</p><ol><li>遍历每个用户喜欢的物品列表；</li><li>获取该用户的相似用户列表；</li><li>把每一个喜欢的物品 Map 成两个记录发射出去，一个是键为 &lt; 相似用户 ID，物品 ID，1&gt; 三元组，可以拼成一个字符串，值为 &lt; 相似度 &gt;，另一个是键为 &lt; 相似用户 ID，物品 ID，0&gt; 三元组，值为 &lt; 喜欢程度 * 相似度 &gt;，其中的 1 和 0 为了区分两者，在最后一步中会用到；</li><li>Reduce 阶段，求和后输出；</li><li>&lt; 相似用户 ID，物品 ID, 0&gt; 的值除以 &lt; 相似用户 ID，物品 ID, 1&gt; 的值</li></ol><p>3:</p><ul><li>&lt; 相似用户 ID，物品 ID，1 &gt;  -&gt; w_{uv}</li><li>&lt; 相似用户 ID，物品 ID，0 &gt;  -&gt; w_{uv}*r_{vi}</li></ul><p>4: reduce应该是对相似用户求和</p><p>5：做归一化处理</p><h3>Item-CF</h3><p>$$<br>p_{ui} = \sum_{j \in N(u) \cap S(i,K) } w_{ij} r_{uj}<br>$$</p><p>物品相似度计算</p><p>注：相似度计算基于的是评分矩阵或者布尔化的行为矩阵</p><p>Jaccard<br>$$ w_{ij} = \frac{ \sum_{u \in U} r_{ui} r_{uj} }{ \sqrt{\sum_{v \in U} r_{vi}^2 \sum_{v \in U} r_{vj}^2 }} $$</p><p>物品去中心化<br>$$ w_{ij} = \frac{ \sum_{u \in U} (r_{ui} - \bar{r}<em>i) (r</em>{uj} - \bar{r}<em>j) }{ \sqrt{ \sum</em>{v \in U} (r_{ui} - \bar{r}<em>i)^2 \sum</em>{v \in U} (r_{uj} - \bar{r}<em>j)^2 } } $$<br>用户去中心化<br>$$ w</em>{ij} = \frac{ \sum_{u \in U} (r_{ui} - \bar{r}_u) (r_{uj} - \bar{r}_u) }{ \sqrt{ \sum_{v \in U} (r_{ui} - \bar{r}<em>u)^2 \sum</em>{v \in U} (r_{uj} - \bar{r}_u)^2 }} $$</p><p>更一般的相似性计算，比如余弦</p><ol><li></li></ol><p>$$<br>w_{ij} = \frac{&lt; r_i, r_j&gt;}{|r_i|}<br>$$</p><ol start="2"><li>热门关联<br>$$<br>w_{ij} = \frac{&lt; r_i, r_j&gt;}{|r_i||r_j|}<br>$$</li><li>对热门的打压<br>$$<br>w_{ij} = \frac{&lt; r_i, r_j&gt;}{|r_i|^{\alpha} |r_j|^{ 1- \alpha }}<br>$$<br>$ \alpha $ 为0 最大限度打压热门，为1 不打压</li><li>用户打压<br>$$<br>&lt;r_i, r_j&gt; = \sum_{u \in U} \frac{r_{ui} r_{uj}}{ \log (1 + N(u))}<br>$$<br>IUF（Inverse User Frequence）</li></ol><ol start="5"><li>热传导</li></ol><p>冷门受益<br>$$<br>w_{ij}^H = \frac{1}{k_i} \sum_{u \in U} \frac{r_{ui} r_{uj}}{k_u}<br>$$<br>热门受益<br>$$<br>w_{ij}^P = \frac{1}{k_j} \sum_{u \in U} \frac{r_{ui} r_{uj}}{k_u}<br>$$<br>看除的分母，热门物品度大，冷门度小</p><p>调和<br>$$<br>w_{ij}^P = \frac{1}{k_i^{ 1- \lambda} k_j^{\lambda}} \sum_{u \in U} \frac{r_{ui} r_{uj}}{k_u}<br>$$</p><h2>矩阵分解/隐语义模型LFM(latent factor model)</h2><blockquote><p>评分预测问题只是很典型，其实并不大众，毕竟在实际的应用中，评分数据很难收集到，属于典型的精英问题；与之相对的另一类问题行为预测，才是平民级推荐问题，处处可见。</p></blockquote><p>近邻模型存在的问题：</p><ol><li>物品之间存在相关性，信息量并不随着向量维度增加而线性增加；</li><li>矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况存在。</li></ol><p>矩阵分解的目的分解评分矩阵A</p><p>$$<br>A_{m \times n} \cong U_{m \times k} V_{n \times k}^T<br>$$</p><p>推荐过程</p><p>$$<br>\widehat{r}_{ui} = p_u q^T_i<br>$$<br>$ p_u $ 用户向量，$ q_i $ 物品向量。</p><h3>基本SVD</h3><p>SVD（奇异值分解）的损失函数：</p><p>$$ \min <em>{q^ *, p^ *} \sum</em>{u,i} (r_{ui} - p_u q_i^T) ^2 + \lambda ( ||q_i||^2 + ||p_u||^2 ) $$<br>SVD学习过程</p><ol><li>准备好用户物品的评分矩阵，每一条评分数据看做一条训练样本；</li><li>给分解后的 U 矩阵和 V 矩阵随机初始化元素值；</li><li>用 U 和 V 计算预测后的分数；</li><li>计算预测的分数和实际的分数误差；</li><li>按照梯度下降的方向更新 U 和 V 中的元素值；</li><li>重复步骤 3 到 5，直到达到停止条件。</li></ol><h3>增加偏置</h3><p>$$<br>\widehat{r}_{ui} = \mu + b_i + b_u + p_u q^T_i<br>$$<br>分别为全局评分、物品偏置、用户偏置</p><p>偏置的计算为，当前评分-对应的平均分</p><p>对应的损失函数<br>$$ \min_{q^<em>, p^</em>} \sum_{u,i} (r_{ui} - \mu - b_i - b_u -  p_u q_i<sup>T)</sup>2 + \lambda ( ||q_i||^2 + ||p_u||^2 + b_i^2 + b_u^2) $$</p><h3>增加隐式数据/历史行为</h3><p>在 SVD 中结合用户的隐式反馈行为和属性，这套模型叫做 SVD++。</p><p>隐式反馈的加入方法：</p><p>除了假设评分矩阵中的物品有一个隐因子向量外，用户有过行为的物品集合也都有一个隐因子向量，维度是一样的。把用户操作过的物品隐因子向量加起来，用来表达用户的兴趣偏好。</p><p>类似的，用户属性，全都转换成 0-1 型的特征后，对每一个特征也假设都存在一个同样维度的隐因子向量，一个用户的所有属性对应的隐因子向量相加，也代表了他的一些偏好。</p><p>综合两者，SVD++ 的目标函数中，只需要把推荐分数预测部分稍作修改，原来的用户向量那部分增加了隐式反馈向量和用户属性向量：<br>$$ \widehat{r}_{ui} = \mu + b_i + b_u + (p_u + \frac{1}{|N(u)|} \sum_{j \in N(u)} x_j + \sum_{a \in Au} y_a ) q_i^T $$</p><h3>加入时间因素</h3><ol><li>对评分按照时间加权，让久远的评分更趋近平均值；</li><li>对评分时间划分区间，不同的时间区间内分别学习出隐因子向量，使用时按照区间使用对应的隐因子向量来计算；</li><li>对特殊的期间，如节日、周末等训练对应的隐因子向量。</li></ol><h3>损失函数优化方法</h3><p>SGD V.S. ALS</p><p>ALS的思想就是固定一个优化另外一个，所以叫交替最小二乘。其好处：</p><ol><li>在交替的其中一步，也就是假设已知其中一个矩阵求解另一个时，要优化的参数是很容易并行化的；</li><li>在不那么稀疏的数据集合上，交替最小二乘通常比随机梯度下降要更快地得到结果，事实上这一点就是我马上要说的，也就是关于隐式反馈的内容。</li></ol><p>相比“预测用户会打多少分”，“预测用户会不会去浏览”更加有意义，而且，用户浏览数据远远多于打分评价数据。也就是说，实际上推荐系统关注的是预测行为，行为也就是一再强调的隐式反馈。</p><p>对隐式反馈的矩阵分解，需要将交替最小二乘做一些改进，改进后的算法叫做加权交替最小二乘：Weighted-ALS。</p><ol><li>如果用户对物品无隐式反馈则认为评分是 0；</li><li>如果用户对物品有至少一次隐式反馈则认为评分是 1，次数作为该评分的置信度。</li></ol>但是这里的数据只有正样本，负样本是我们“认为”的，这个“认为”可能不太准确，这就是One-class问题。所以需要对负样本进行采样。随机采样很不靠谱，一种比较好的方法是，按照物品的热门程度采样。<p>关于负采样方法，word2vec等模型中都有介绍，后续专门写一篇博客。</p><p>按照物品热门程度采样的思想就是：一个越热门的物品，用户越可能知道它的存在。那这种情况下，用户还没对它有反馈就表明：这很可能就是真正的负样本。</p><p>现在的目标函数：</p><p>$$ \min_{q^<em>, p^</em>} \sum_{u,i} c_{ui} (r_{ui} - p_u q<sup>T_i)</sup>2 + \lambda ( ||q_i||^2 + ||p_u||^2 ) $$</p><p>$$ c_{ui} = 1+ \alpha C $$</p><p>C_ui就是置信度，跟反馈次数C有关，$ \alpha $ 默认取40。</p><h2>推荐计算</h2><p>在得到了分解后的矩阵后，每个用户得到了稠密的隐因子向量，同时每个物品也得到了一个稠密向量，代表它的语义或主题。看上去，让用户和物品的隐因子向量两两相乘，计算点积就可以得到所有的推荐结果了。但是实际上复杂度还是很高，尤其对于用户数量和物品数量都巨大的应用，如Facebook，就更不现实。于是 Facebook提出了两个办法得到真正的推荐结果。</p><p>第一种，利用一些专门设计的数据结构存储所有物品的隐因子向量，从而实现通过一个用户向量可以返回最相似的 K 个物品。</p><p>Facebook 给出了自己的开源实现 Faiss，类似的开源实现还有 Annoy，KGraph，NMSLIB。其中 Facebook 开源的 Faiss 和 NMSLIB（Non-Metric Space Library）都用到了 ball tree 来存储物品向量。</p><p>第二种，就是拿着物品的隐因子向量先做聚类，海量的物品会减少为少量的聚类。然后再逐一计算用户和每个聚类中心的推荐分数，给用户推荐物品就变成了给用户推荐物品聚类。得到给用户推荐的聚类后，再从每个聚类中挑选少许几个物品作为最终推荐结果。这样做的好处除了大大减小推荐计算量之外，还可以控制推荐结果的多样性，因为可以控制在每个类别中选择的物品数量。</p><h2>贝叶斯个性化排序BPR</h2><p>矩阵分解，本质上都是在预测用户对一个物品的偏好程度，其实就是做编码embedding。</p><p>得到这样的矩阵分解结果后，常常在实际使用时，又是用这个预测结果来排序。所以，口口声声宣称想要模型的预测误差最小化，结果绕了一大圈最后还是只想要一个好点的排序。</p><p>这种针对单个用户对单个物品的偏好程度进行预测，得到结果后再排序的问题，在排序学习中的行话叫做 point-wise。与之相对的，还有直接预测物品两两之间相对顺序的问题，就叫做 pair-wise。</p><p>矩阵分解都属于 point-wise模型。这类模型的尴尬是：只能收集到正样本，没有负样本，于是认为缺失值就是负样本，再以预测误差为评判标准去使劲逼近这些样本。逼近正样本没问题，但是同时逼近的负样本只是缺失值而已，还不知道真正呈现在用户面前，到底是不喜欢还是喜欢呢？</p><p>虽然这些模型采取了一些措施来规避这个问题，比如负样本采样，但是尴尬还是存在的，为了排序而绕路也是事实。</p><p>贝叶斯个性化排序(Bayesian Personalized Ranking, BPR)就直接采用pair-wise来做矩阵分解。</p><p>在BPR算法中，我们将任意用户u对应的物品进行标记，如果用户u在同时有物品i和j的时候点击了i，那么我们就得到了一个三元组&lt;u, i, j&gt;，它表示对用户u来说，i的排序要比j靠前。</p><p>三元组中，i和j都只能是行为过和未行为过中的一种，不包含都行为和都未行为的情况。</p><p>这样一来，学习的数据是反应用户偏好的相对顺序，而在使用时，面对的是所有用户还没行为过的物品，这些物品仍然可以在这样的模型下得到相对顺序，这就比三元组 point-wise 样本要直观得多。</p><p>现在，每条样本包含的是两个物品，样本预测目标是两个物品的相对顺序。</p><p>用个符号来表示这个差：Xu12，表示的是对用户 u，物品 1 和物品 2 的矩阵分解预测分数差。然后再用 sigmoid 函数把这个分数差压缩到 0 到 1 之间。</p><p>$$<br>\Theta = \frac{1}{1+ e ^{-X_{u12}}}<br>$$<br>也其实就是用这种方式预测了物品 1 排在物品 2 前面的似然概率，所以最大化交叉熵就是目标函数了。</p><p>目标函数通常还要防止过拟合，加上正则项，正则项其实认为模型参数还有个先验概率，这是贝叶斯学派的观点，也是 BPR 这个名字中“贝叶斯”的来历。</p><p>BPR 认为模型的先验概率符合正态分布，对应到正则化方法就是 L2 正则，具体参见<a href="https://www.cnblogs.com/pinard/p/9128682.html" target="_blank" rel="noopener">贝叶斯个性化排序算法小结</a>。</p><p>$$ \prod_{u \in U} P(&gt;<em>u|\theta) P(\theta) = \prod</em>{(u,i,j) \in D}   \sigma(X_{u12})  P(\theta) = \prod_{(u,i,j) \in D} \sigma(\overline{x}_{ui} - \overline{x}_{uj}) P(\theta) $$</p><p>训练方法：梯度下降+mini-batch</p><h1>特征工程</h1><p>实施特征工程之前，需要先理解业务。<br>在推荐场景中</p><p><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/RS/recom_context.png" alt></p><p>特征</p><ul><li>先归一化，统一量纲</li><li>离散化，引入非线性关系</li><li>特征交叉</li><li>GBDT</li></ul><p>无个性化特征信息交叉会造成所有用户结果一样。</p><p>交叉特征：</p><ul><li>性别&amp;分布（转化分布）</li><li>分布&amp;ID</li></ul><p>在样本给定下：</p><ol><li>仔细考虑用何种特征构造方法</li><li>从聚合/内积开始，逐渐增加参数，直到过拟合</li></ol><p>过拟合判断方法：如，训练集AUC远大于测试集AUC</p><h1>模型融合</h1><p>推荐系统在技术实现上一般划分为三个阶段：挖掘、召回、排序。</p><p>挖掘的工作就是对用户和物品做非常深入的结构化分析，庖丁解牛一样，各个角度各个层面的特征都被呈现出来，并且建好索引，供召回阶段使用，大部分挖掘工作都是离线进行的。</p><p>接下来就是召回，为什么会有召回？因为物品太多了，每次给一个用户计算推荐结果时，如果对全部物品挨个计算，那将是一场灾难，取而代之的是用一些手段从全量的物品中筛选出一部分比较靠谱的。</p><p>最后就是排序，针对筛选出的一部分靠谱的做一个统一的排序。</p><p>进一步召回：<br>在召回阶段，其实就是各种简单的、复杂的推荐算法，比如说基于内容的推荐，会产生一些推荐结果，比如基于物品的协同过滤会产生一些结果，矩阵分解会产生一些结果，等等。</p><p>正则化的方法一般是：限定总的树个数、树的深度、以及叶子节点的权重大小。实数特征的分裂。</p><p>推荐系统：唯快不破</p><p>在线算法</p><ul><li><a href="https://zhuanlan.zhihu.com/p/20447450" target="_blank" rel="noopener">FTRL</a></li><li>TG</li><li>FOBOS</li><li>RDA</li></ul><p><a href="http://www.cs.ust.hk/~qyang/Docs/2007/tradaboost.pdf" target="_blank" rel="noopener">Boosting For Transfer Learning</a></p><p>特征组合</p><ul><li>GBDT+LR</li><li>FM(Factorization Machine)：因子分解机</li><li>FFM(Field-aware Factorization Machine）</li><li><a href="https://github.com/tensorflow/models/tree/master/official/wide_deep" target="_blank" rel="noopener">Wide &amp; Deep 模型</a></li></ul><ol><li>深宽模型是一个结合了传统线性模型和深度模型的工程创新。</li><li>这个模型适合高维稀疏特征的推荐场景，稀疏特征的可解释性加上深度模型的泛化性能，双剑合璧。</li><li>这个模型已经开源在 TensorFlow 中。</li><li>为了提高模型的训练效率，每一次并不从头开始训练，而是用上一次模型参数来初始化当前模型的参数。</li><li>将类别型特征先做嵌入学习，再将嵌入稠密向量送入深度模型中。</li><li>为了提高服务的响应效率，对每次请求要计算的多个候选 App 采用并行评分计算的方式，大大降低响应时间。</li></ol><h1>MAB问题</h1><p>多臂赌博机问题 (Multi-armed bandit problem, K-armed bandit problem, MAB)，简称 MAB 问题。</p><p>推荐系统的使命就是：为用户匹配到最佳的物品，在某个时间某个位置为用户选择最好的物品。</p><p>推荐就是选择</p><h2>Bandit 算法</h2><p>小心翼翼地试，越确定某个选择好，就多选择它，越确定某个选择差，就越来越少选择它。</p><p>一种走一步看一步的推荐算法， Bandit 算法。Bandit 算法把每个用户看成一个多变的环境，待推荐的物品就如同赌场里老虎机的摇臂，如果推荐了符合用户心目中喜欢的，就好比是从一台老虎机中摇出了金币一样。</p><p>Bandit 算法有汤普森采样，UCB 算法，Epsilon 贪婪。汤普森采样以实现简单和效果显著而被人民群众爱戴，你需要时不妨首先试试它。</p><p>Bandit解决冷启动</p><ol><li><p>用分类或者 Topic 来表示每个用户兴趣，我们可以通过几次试验，来刻画出新用户心目中对每个 Topic 的感兴趣概率。</p></li><li><p>这里，如果用户对某个 Topic 感兴趣，就表示我们得到了收益，如果推给了它不感兴趣的 Topic，推荐系统就表示很遗憾 (regret) 了。</p></li><li><p>当一个新用户来了，针对这个用户，我们用汤普森采样为每一个 Topic 采样一个随机数，排序后，输出采样值 Top N 的推荐 Item。注意，这里一次选择了 Top N 个候选臂。</p></li><li><p>等着获取用户的反馈，没有反馈则更新对应 Topic 的 b 值，点击了则更新对应 Topic 的 a 值。</p></li></ol><h3>LinUCB</h3><p>“Yahoo!”的科学家们在 2010 年基于 UCB 提出了 LinUCB 算法，它和传统的 UCB 算法相比，最大的改进就是加入了特征信息，每次估算每个候选的置信区间，不再仅仅是根据实验，而是根据特征信息来估算，这一点就非常的“机器学习”了。</p><p>优点：</p><ol><li>由于加入了特征，所以收敛比 UCB 更快，也就是比 UCB 更快见效；</li><li>各个候选臂之间参数是独立的，可以互相不影响地更新参数；</li><li>由于参与计算的是特征，所以可以处理动态的推荐候选池，编辑可以增删文章；</li></ol><p>LinUCB 只是一个推荐框架，可以将这个框架应用在很多地方，比如投放广告，为用户选择兴趣标签等。</p><h3>COFIBA 算法</h3><p>概要：</p><ol><li>在时刻 t，有一个用户来访问推荐系统，推荐系统需要从已有的候选池子中挑一个最佳的物品推荐给他，然后观察他的反馈，用观察到的反馈来更新挑选策略。</li><li>这里的每个物品都有一个特征向量，所以这里的 Bandit 算法是 context 相关的，只不过这里虽然是给每个用户维护一套参数，但实际上是由用户所在的聚类类簇一起决定结果的。</li><li>这里依然是用岭回归去拟合用户的权重向量，用于预测用户对每个物品的可能反馈（payoff），这一点和我们上一次介绍的 LinUCB 算法是一样的。</li></ol><p>与linUCB算法的不同：</p><ol><li>基于用户聚类挑选最佳的物品，即相似用户集体动态决策；</li><li>基于用户的反馈情况调整用户和物品的聚类结果。</li></ol><p>算法流程：</p><ol><li>首先计算用户 i 的 Bandit 参数 W，做法和 LinUCB 算法相同，但是这个参数并不直接参与到选择决策中，注意这和 LinUCB 不同，只是用来更新用户聚类。</li><li>遍历候选物品，每一个物品已经表示成一个向量 x 了。</li><li>每一个物品都对应一个物品聚类类簇，每一个物品类簇对应一个全量用户聚类结果，所以遍历到每一个物品时，就可以判断出当前用户在当前物品面前，自己属于哪个用户聚类类簇，然后把对应类簇中每个用户的 M 矩阵 (对应 LinUCB 里面的 A 矩阵)，b 向量（表示收益向量，对应 LinUCB 里面的 b 向量）加起来，从而针对这个类簇求解一个岭回归参数（类似 LinUCB 里面单独针对每个用户所做），同时计算其收益预测值和置信区间上边界。</li><li>每个待推荐的物品都得到一个预测值及置信区间上界，挑出那个上边界最大的物品作为推荐结果。</li><li>观察用户的真实反馈，然后更新用户自己的 M 矩阵和 b 向量，只更新每个用户，对应类簇里其他的不更新。</li></ol><p>Bandit 算法系列，主要是解决推荐系统中的冷启动和 EE 问题。探索和利用这一对矛盾一直客观存在，而 Bandit 算法是公认的一种比较好的解决 EE 问题的方案。</p><h2>深度学习在推荐上的应用</h2><h2>排行榜的构建</h2><p>热度计算</p><ol><li>Hacker News</li></ol><p>$$<br>\frac{P-1}{(T+2)^G}<br>$$</p><ol><li>P：得票数，去掉帖子作者自己投票。</li><li>T：帖子距离现在的小时数，加上帖子发布到被转帖至 Hacker News 的平均时长。</li><li>G：帖子热度的重力因子。</li></ol><p>公式中，分子是简单的帖子数统计，一个小技巧是去掉了作者自己的投票。分母就是将前面说到的时间因素考虑在内，随着帖子的发表时间增加，分母会逐渐增大，帖子的热门程度分数会逐渐降低。</p><ol start="2"><li>牛顿冷却定律</li></ol><p>$$<br>T(t) = H + C e^{-\alpha t}<br>$$</p><ul><li>H：为环境维度，可以认为是平均票数，比如电商中的平均销量，由于不影响排序，可以不使用。</li><li>C：为净剩票数，即时刻 t 物品已经得到的票数，也就是那个最朴素的统计量，比如商品的销量。</li><li>t：为物品存在时间，一般以小时为单位。</li><li>\alpha：是冷却系数，反映物品自然冷却的快慢。</li></ul><h2>其他算法</h2><h3>加权采样算法</h3><p>有限数据集</p><p>$$<br>S_{i} = R^{\frac{1}{w_{i}}}<br>$$</p><ol><li>wi 是每个样本的权重，比如用户标签权重；</li><li>R 是遍历每个样本时产生的 0 到 1 之间的随机数；</li><li>Si 就是每个样本的采样分数</li></ol><p>你可以看到，每个样本采样概率和它的权重成正比。</p><p>指数分布采样</p><p>无限数据集：蓄水池采样</p><p>内容去重算法</p><ul><li>Simhash</li><li>布隆过滤器</li></ul><h1>工程实践</h1><p>信息流，feed流</p><p>信息流框架</p><p>Netflix架构</p><p>TODO</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近对以前学习推荐系统的知识点笔记的一个汇总。&lt;/p&gt;
    
    </summary>
    
    
      <category term="推荐系统" scheme="https://zekizz.github.io/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="推荐系统" scheme="https://zekizz.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>命名实体识别综述</title>
    <link href="https://zekizz.github.io/ML/NER-survey/"/>
    <id>https://zekizz.github.io/ML/NER-survey/</id>
    <published>2018-10-03T15:54:11.000Z</published>
    <updated>2019-10-13T09:20:48.301Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>命名实体识别（Named Entity Recognition，NER）为自然语言处理（NLP）的基础任务之一，其目标是提取文本中的命名实体并对这些实体进行分类，比如人名、地名、机构、时间、货币和百分比等，广泛用于信息提取、问答系统、句法分析、信息检索和情感分析等任务。</p><p>命名实体识别不仅需要找出实体的位置，还需要对实体进行分类。</p><a id="more"></a><p>具体综述见如下PDF（移动端插件挂的看这个：<a href="https://zekizz.github.io/NER_survey_zeki.pdf">命名实体识别综述</a>）</p><div class="row">    <embed src="../../NER_survey_zeki.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;命名实体识别（Named Entity Recognition，NER）为自然语言处理（NLP）的基础任务之一，其目标是提取文本中的命名实体并对这些实体进行分类，比如人名、地名、机构、时间、货币和百分比等，广泛用于信息提取、问答系统、句法分析、信息检索和情感分析等任务。&lt;/p&gt;
&lt;p&gt;命名实体识别不仅需要找出实体的位置，还需要对实体进行分类。&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="深度学习" scheme="https://zekizz.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="NLP" scheme="https://zekizz.github.io/tags/NLP/"/>
    
      <category term="NER" scheme="https://zekizz.github.io/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>图半监督节点分类之五——实验与总结</title>
    <link href="https://zekizz.github.io/ML/graph-based-semi-supervised-classification-5/"/>
    <id>https://zekizz.github.io/ML/graph-based-semi-supervised-classification-5/</id>
    <published>2018-10-03T11:57:08.000Z</published>
    <updated>2019-10-13T09:20:48.305Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>本文对上几篇文章说诉的方法进行了实验验证和总结。</p><a id="more"></a><h2>数据集简介</h2><p>本文使用了三个基准引用网络数据集：Cora、Citeseer和Pubmed<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，在这三个数据集上验证所前几篇文章中模型的性能。 这三个引用网络数据集包括学术论文（视为图中的节点）和引用链接（视为图中的边）。下表总结了所用数据集的详细统计数据。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/dataset.png" alt="三个基准引用网络数据集"><br>每篇论文（节点）都有一个稀疏的词袋表示的特征向量和一个专家标注的主题，这些主题来自一组预先定义好的类别。在Cora和Citeseer中，节点的特征向量中的元素为0或1的值，表示对应位置上的词是否出现在文档中。特征向量的维度即为词典的大小。在Pubmed中，每篇论文被描述为由500个词组成的词典计算的TF/IDF加权词向量。 与所有基线方法（baseline）一致，本文将由引用链接构建的图作为无向图，并将边权重设置为1。</p><p>如上表所示，本文首先按照Yang<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>的方式，固定训练集、验证集和测试集的划分方式，因为他们给出的划分方式是这些基准数据集的标准划分方法，已经被广泛采纳。在该划分方式中，每类有20个标记节点用于训练集，500个节点用于验证集，1000个节点用于测试集，其余节点都作为未标记节点。其中训练集、验证集和测试集的节点挑选是由Yang<sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2]</a></sup>固定给出的，故叫做固定数据集划分的实验部分。其次，为了验证模型的鲁棒性，本文同样采用了随机数据划分的方式，多次进行实验测试模型对标记样本位置的敏感性。</p><h2>对比方法</h2><p>本文将所提出的模型与Yang<sup class="footnote-ref"><a href="#fn2" id="fnref2:2">[2]</a></sup>和Kipf<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>中描述的几种基线方法以及图卷积网络GCN<sup class="footnote-ref"><a href="#fn3" id="fnref3:1">[3]</a></sup>进行对比。<br>其中有些方法在之前的章节中已经给了详细的解释，下面从利用的数据类型（网络结构 $ A $ 、节点特征 $ X $ 和节点类别标签 $ Y $）的角度将这些方法进行归类并做简单的介绍：</p><ul><li><strong>标签传播方法</strong>:  LP（Label propagation）<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>、ICA（Iterative classification algorithm）<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>和ManiReg（Manifold regularization）<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> 在邻居之间传播标签信息，约束网络中边权重较大的两个节点的标签趋于一致，为非参数化的标签传播算法。ICA通过允许更一般的局部更新来扩展LP，ManiReg将LP中损失函数替换为SVM中的损失函数。这些方法只使用网络结构和节点的标签。</li><li><strong>随机游走方法</strong>:  DeepWalk<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>和node2vec<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>为基于Skip-gram的图编码方法。这类方法通过随机游走生成节点的上下文，在此基础上利用Skip-gram学习图编码。这两个方法只利用网络结构来学习编码，忽略了节点的特征和标签。</li><li><strong>半监督图编码方法</strong>:  SemiEmb（Semi-supervised embedding）<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> 为最早使用神经网络的基于图拉普拉斯正则化的半监督图编码方法。这些方法在有标记样本上计算有监督损失，利用图拉普拉斯正则在所有样本上计算无监督损失。因此，模型训练过程中利用了网络结构、节点特征和节点标签三方面的信息。Planetoid<sup class="footnote-ref"><a href="#fn2" id="fnref2:3">[2]</a></sup> 同样利用了这三个方面的信息，该模型训练了一个编码来联合预测节点的类别标签和图上下文。GCN<sup class="footnote-ref"><a href="#fn3" id="fnref3:2">[3]</a></sup>利用这三个方面的信息来构建一个统一的半监督图卷积神经网络模型，同时完成特征编码和节点分类任务。</li></ul><h2>实验结果</h2><p>本文首先给出实验过程中的一些参数的设置。上文已经指出，训练集是在每个类别上挑选20个样本得到的。本文使用拥有500个样本的验证集来优化参数，比如dropout比率、L2正则项权重和学习率。训练好模型后，对于每个数据集，本文在1000个随机挑选的测试样本上评估预测的正确率。 在三个数据集上，一些共同的参数的设置如下：</p><ul><li>dropout： 0.5</li><li>学习率： 0.01</li><li>最大迭代次数：  200</li><li>编码特征维度：  16</li><li>L2正则项权重：$ 5 \cdot 10^{-4}$</li></ul><p>其中编码特征的维度，也就是隐层特征的维度，这里设置为16是为了与GCN中的设置一致。后文也会讨论编码特征的维度的影响。 虽然最大迭代次数为200，但是模型经常在此之前收敛。为此，本文采用early stopping来及时停止模型的迭代，并设置容忍的迭代次数为10，如果在10次迭代内模型的损失变化小于一定阈值就可认为已经收敛。</p><p>除了以上共同的超参数外，还有一些场景独立的超参数。对于GCN-CRF模型，本文设定其中CRF学习部分的迭代次数在所有数据集上都固定为5次。对于Graph-GRU模型，本文对Pubmed数据集使用4个GRU隐层，对于Cora和Citeseer 数据集使用5个GRU隐层。对于AHOGE模型，本文使用了1个一阶近似层和5个高阶近似层。</p><p>本文通过Tensorflow实现这些模型。所有的实验都是在仅有CPU的环境下进行的： 4-core Intel® Core™ i7-6500U CPU ® 2.5GHz</p><p>本节首先在固定数据集划分的情况下进行实验，分类结果总结在下表中。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/result_fix.png" alt="固定数据集划分时的分类正确率"></p><p>GCN-CRF在数据集Citeseer上取得了最好的结果，但是，相比于Graph-GRU在其他两个数据集上的正确率差不多。不难发现，数据集Citeseer的网络最为稀疏，为了获得较好的性能，Graph-GRU和AHOGE都堆叠多个隐层来获取充足的邻居信息。而GCN-CRF只有两个隐层，其主要发挥作用的是CRF模块。CRF模块对节点的分类结果添加约束，促使相邻且相似的节点分类结果趋于一致。事实证明，该CRF模块是有效的，尤其在稀疏网络结构上。</p><p>再看Graph-GRU和AHOGE，两者都能保持高阶近似，而且AHOGE可以视为是Graph-GRU的改进版，能在自适应地保持高阶近似的同时保留个体局部细节。AHOGE在三个数据集上的正确率均高于Graph-GRU，也证实了这一点。总体来说，AHOGE的性能最好，即使在稀疏的数据集Citeseer上，其分类正确率也与GCN-CRF接近。</p><p>固定数据集划分会造成一定的随机性，不一定能准确的反映模型的性能。因此，本文紧接着对数据集进行多次随机划分，用于观测模型在各方面的性能。</p><p>在该随机划分实验中，本文保持标记样本的数目不变，同样也是每类20个样本。同时，本次实验也保持验证集的大小不变，为500个样本。本次实验的一个目的是观察标记节点的在网络中的位置对模型推理结果的影响，因此本次实验固定测试集的样本不变，与上文固定数据集划分时的划分一致。这样做的目的就是在保持测试样本的位置不变的情况下，随机变动训练集和验证集样本的位置。还有一点与之前的方案不同，对于训练集与验证集的选取，本次实验不是为每个类选择20个样本，而是完全随机选择样本。 由于数据的类别标签的分布是不均匀的，这样随机选择训练样本可以保持分布结构。本次实验具体随机执行了20次不同的训练集和验证集的选取。</p><p>下表给出了20次随机数据集划分情况下的分类正确率和标准误。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/result_random.png" alt="随机数据集划分时的分类正确率"><br>同样地，AHOGE在Cora和Pubmed上取得了最好的结果，GCN-CRF在Citeseer上拥有最佳分类正确率。但是GCN-CRF在Citeseer上分类结果的标准误却大于AHOGE。不难看出，本文提出的三个方法在随机数据集划分情况下，分类正确率和标准误均优于目前的最优方法GCN。Graph-GRU虽然与GCN-CRF在Cora和Pubmed上有相近的分类正确率，Graph-GRU有着更低的标准误，因此Graph-GRU相比于GCN-CRF更稳定。AHOGE在三个数据集上均取得了最低的标准误，说明自适应捕获 高阶依赖使得模型对标记样本位置的影响更鲁棒。</p><p>除了训练正确率，另一个倍受关注的是模型的训练效率，即收敛速度与训练时间。下图给出了在随机20次实验情况下，模型在一次迭代的时间、迭代次数和训练总时间上的平均表现。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/result_efficiency.png" alt="模型在训练时间开销和收敛性上的表现"><br>Graph-GRU和AHOGE平均在每一次迭代上所花的时间要多于GCN，但是模型收敛比GCN快，基本迭代次数为GCN的一半，故在模型训练的总时间上所提出的模型都要优于GCN。这里GCN-CRF的短板就体现出来了，因为需要花费更多的迭代次数去求解CRF模块，虽然迭代次数略微少于GCN，但是总时间在Cora和Citeseer上还是要高于GCN的。其次对于Graph-GRU和AHOGE，AHOGE略微比Graph-GRU收敛慢，但是每次迭代花费更多的时间，故总体上的时间开销都要多一些。</p><h2>敏感性分析</h2><p>本节对模型中的参数做敏感性分析，主要分为三个方面：（1）标记样本的比例；（2）GCN-CRF模型中CRF迭代次数、Graph-GRU和AHOGE的隐层层数；（3）隐层特征数目。<br>根据数据集的细节可以看出，上文实验过程中标记样本的比例是很低的，为此，本小节首先讨论了标记样本比例的影响，实验结果如下图所示。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/influence_label_rate.png" alt="标记样本比例的影响"><br>整体上分类正确率随标记样本比例的升高存在一个先升高再平缓的过程。综合标记成本、模型训练开销和分类正确率，数据集只需要20%的标记便可取得很好的结果，这将大大减少标注任务的工作量。同时，GCN-CRF在样本少、图稀疏且标记样本比例小时性能更优，Graph-GRU在数据集大且具备一定比例的标记样本时优于AHOGE，AHOGE在标记样本比例小时更优。</p><p>其次，给出CRF的迭代次数和Graph-GRU与AHOGE的隐层数目对模型分类正确率的影响。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/influence_layers.png" alt="隐层层数的敏感性分析"><br>如上文所述，CRF在一般在10次迭代内收敛，上图表明模型在2次迭代后分类正确率就趋于稳定，并在第5次迭代处取得最大值。Graph-GCN和AHOGE模型的结果都有一个先上升再平缓最后再下降的过程，这印证了隐层数目过多会造成编码结果在高阶依赖上过于平滑的现象。Graph-GRU在Cora和Citeseer上最优的隐层数目为5层，在Pubmed上为4层。AHOGE在三个数据集上都在5层高阶近似层上取得最大值。</p><p>最后，本文给出隐层特征数目对分类结果的影响。由于Cora数据集与Pubmed数据集的稠密程度相似并有相似的结果趋势，下图以Cora数据集和Citeseer数据集为例展示了隐层特征维度（即编码特征维度）对分类结果的影响。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/influence_features.png" alt="隐层特征维度的敏感性分析"><br>在Cora数据集上，GCN-CRF和AHOGE对隐层特征维度不太敏感，分类正确率相对稳定，而Graph-GRU的分类正确率表现出明显的随隐层特征维度的增加而较少的趋势。 在Citeseer数据集上，三个模型的整体趋势都是先增后降。虽然存在不同维度的特征编码的分类正确率可能一样，但是特征的维度越小，模型的复杂度越低，训练需要的时间相应的越少。</p><h2>总结</h2><p>近年来人们越来越关注于关系型数据的学习，图作为一种很自然的关系结构被广泛的用于这类数据的分析与建模。本文关注的是图结构数据上的半监督节点分类这一具备很强现实意义问题。通过对目前相关工作进行分类总结，本文采用目前热门的图编码方法，从编码过程和分类过程两个方面优化目前主流的方法。</p><p>具体地，本文首先针对分类过程进行优化并在建模过程中同时利用邻居的特征和标签。本文采用图卷积神经网络GCN来利用邻居特征进行编码，采用条件随机场来平滑GCN的分类结果，并提出GCN-CRF模型。通过将CRF的平均场近似推理构建为RNN模块的形式，GCN-CRF能将该模块挂载在softmax层前面，并在一个统一的模型中端到端地训练编码过程和分类过程。</p><p>其次，针对编码过程，为了在特征编码过程中捕获全局信息，本文提出Graph-GRU模型来捕获高阶节点依赖，Graph-GRU通过构建多个前馈RNN层在图上实施GRU来收集邻居的信息。由于节点局部网络结构的差异性，在建模高阶依赖时，容易出现编码的结果过于平滑以致于丢失个体细节特征的现象。基于这个出发点，本文提出了自适应高阶依赖图编码（AHOGE）模型。AHOGE模型首先借用GRU的思想自适应地保留一阶近似，然后堆叠五个高阶接近层来捕获远程节点依赖。 为了自适应地保持高阶近似，AHOGE模型采用高速网络的思想将一阶近似层连接到各个高阶近似层。</p><p>最后，在三个基准图结构数据集上的实验表明，本文提出的三个方法在分类正确率和标准误上均优于目前最先进的方法。GCN-CRF更加适合处理图结构相对稀疏的情况，Graph-GRU的收敛较快并且模型的训练时间较少，AHOGE对标记样本在图中的位置更鲁棒并且分类正确率更为稳定。</p><p>在实验过程中，本文发现标记节点在图中的位置对模型性能有一定的影响，其造成的分类正确率的差异能高达8%。分析标记节点位置的影响是一个有趣的研究课题，这将作为本文进一步的研究方向。该研究的一个典型应用为基于主动学习的样本标注，通过分析样本所处的位置，挑选出最具信息量和影响力的样本用于标注。通过按这种方式额外挑选一小部分样本，使得模型的性能能得到最大限度的提升，因此能极大的减轻繁重的标注任务。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>SEN P, NAMATA G, BILGIC M, et al., 2008. Collective classification in network data[J]. AI magazine, 29(3): 93. <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>YANG Z, COHEN W W, SALAKHUTDINOV R, 2016a. Revisiting semi-supervised learning with graph embeddings[J]. arXiv preprint arXiv:1603.08861. <a href="#fnref2" class="footnote-backref">↩</a> <a href="#fnref2:1" class="footnote-backref">↩</a> <a href="#fnref2:2" class="footnote-backref">↩</a> <a href="#fnref2:3" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>KIPF T N, WELLING M, 2016. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907. <a href="#fnref3" class="footnote-backref">↩</a> <a href="#fnref3:1" class="footnote-backref">↩</a> <a href="#fnref3:2" class="footnote-backref">↩</a></p></li><li id="fn4" class="footnote-item"><p>ZHU X, GHAHRAMANI Z, LAFFERTY J D, 2003. Semi-supervised learning using gaussian fields and harmonic functions[C]//Proceedings of the 20th International conference on Machine learning (ICML-03). 912–919. <a href="#fnref4" class="footnote-backref">↩</a></p></li><li id="fn5" class="footnote-item"><p>LU Q, GETOOR L, 2003. Link-based classification[C]//Proceedings of the 20th International Conference on Machine Learning (ICML-03). 496–503. <a href="#fnref5" class="footnote-backref">↩</a></p></li><li id="fn6" class="footnote-item"><p>BELKIN M, NIYOGI P, SINDHWANI V, 2006. Manifold regularization: A geometric framework for learning from lab <a href="#fnref6" class="footnote-backref">↩</a></p></li><li id="fn7" class="footnote-item"><p>PEROZZI B, AL-RFOU R, SKIENA S, 2014. Deepwalk: Online learning of social representations[C]//Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining.  ACM: 701–710. <a href="#fnref7" class="footnote-backref">↩</a></p></li><li id="fn8" class="footnote-item"><p>GROVER A, LESKOVEC J, 2016. node2vec: Scalable feature learning for networks[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM: 855–864. <a href="#fnref8" class="footnote-backref">↩</a></p></li><li id="fn9" class="footnote-item"><p>WESTON J, RATLE F, MOBAHI H, et al., 2012. Deep learning via semi-supervised embedding[M]//Neural Networks: Tricks of the Trade.  Springer: 639–655. <a href="#fnref9" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文对上几篇文章说诉的方法进行了实验验证和总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="深度学习" scheme="https://zekizz.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="graph embedding" scheme="https://zekizz.github.io/tags/graph-embedding/"/>
    
      <category term="半监督分类" scheme="https://zekizz.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%88%86%E7%B1%BB/"/>
    
      <category term="概率图模型" scheme="https://zekizz.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>图半监督节点分类之四——基于自适应高阶近似编码</title>
    <link href="https://zekizz.github.io/ML/graph-based-semi-supervised-classification-4/"/>
    <id>https://zekizz.github.io/ML/graph-based-semi-supervised-classification-4/</id>
    <published>2018-10-03T04:30:03.000Z</published>
    <updated>2019-10-13T09:20:48.306Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>上一篇文章已经详细描述了在半监督图编码任务中，保持高阶近似的重要性。尤其当图比较稀疏时，节点的一阶和二阶邻居的数目太少以致于不能准确反映节点的上下文。从更细的角度来看，图中不同节点所处的位置不同，其局部图结构也就不同。有些节点处于图中比较稠密的部分，比如社团中部，邻居信息足够，能准确地反映节点的类别分布。但是信息过多也不一定是一件好事，比如有些节点夹在两类节点和两个社团的中间，这些节点的邻居很多但是信息也很杂，此时需要进一步扩大邻居范围，看其偏向哪一方。同时，也有一些节点处于一条长链的局部结构中，这些节点只有一个或两个邻居，就算扩大视野到2步邻居，信息也不够。在这种情况下，需要扩大感受野到整条链甚至到这条链所连接的社团。</p><p>上述描述是为了说明不同位置的节点需要的感受野大小不同（$k$阶邻居的$k$不同）。目前的模型包括之前提出的Graph-GRU都是对所有节点一视同仁，使得每个节点都捕获相同的$k$阶近似。显然，对于有些节点，这个固定的$k$值可能比较小，邻居信息不够；对于有些节点，$k$值可能稍微偏大，邻居信息太多以致于紊乱。因此，本章的出发点是，能否对不同节点自适应的捕获不同的高阶近似？此外，当捕获的$k$阶近似中$k$较大时，编码过程需要在以目标节点为中心的$k$阶子图上对目标节点的编码进行平滑。 此时，学得的编码可能过于平滑以致于丢失局部细节。基于以上出发点，本章提出了一个端到端的自适应高阶图编码（Adaptive High-Order Graph Embedding, AHOGE）神经网络，在自适应地保持高阶近似的同时保留个体细节特征。</p><a id="more"></a><h2>自适应高阶图编码模型</h2><p>下图给出了在一个拥有五个节点的简单图下的AHOGE模型的结构示意图。受Column Networks<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>的启发，AHOGE利用$k$个隐层的堆叠来扩展半径为$k$的邻居感受野以实现 远程依赖（高阶近似）。由于第$i$个隐层捕捉$i$阶近似，所以AHOGE将一阶近似层连接在其后面所有隐层上，使得后面的隐层在捕获$k$阶近似的时候能融入一阶近似的信息，以防止过于平滑的编码结果。通过交叉验证，本文选择AHOGE的隐层数目$k=7$。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/model_ahoge.png" alt="自适应高阶图编码AHOGE模型结构示意图"><br>在AHOGE中，7个隐层有三种不同的类型，图中用不同的颜色区分开。具体为特征变换层、一阶近似层和高阶近似层。 $H^0$为神经网络中常见的特征变换层， $H^0 = \text{ReLU}(W^0X)$。$X \in \mathbb{R}^{N \times d} $ 为输入特征矩阵，其中$ N $为图中节点个数，$d$为节点特征的维度。 $ W^0 \in \mathbb{R}^{d \times h} $将节点的特征从$d$维降到$h$维。后续所有的隐层都是在该$h$维的特征空间中对节点的特征进行调整。 $ H^1 $ 为一阶近似层，层中一阶近似单元自适应地收集一阶邻居的信息。$ H^2 $到$ H^6 $为高阶近似层，层中高阶近似单元自适应地融合$k$阶邻居的信息。<br>为了减少模型参数和防止过拟合，本文采用了参数共享和高速网络<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>的思想。首先，模型中相同类型的隐层共享相同类型的参数。其次，隐层$H^1$通过使用高速网络的方式连接到后面的高阶近似层。由于高速网络的性质，一个门$ \alpha_g^k $ 部分开放，让$H^1$能突破层间隔的束缚在整个网络上传播：<br>$$ h^k = \alpha_g^k \circ h^1 +(1-\alpha_g^k) \circ \widehat{h}^k $$</p><p>$\widehat{h}^k $ 为不使用高速网络时隐层 $ H^k $ 的输出。 请注意，本文对网络中的每个节点都定义了这样一类门，本文为了简洁起见省略了$\alpha _{g,i}^k $ 中的$i$，对此类门做一般性描述。 这里门$ \alpha _g^k $不是固定的，是根据$ H^1 $ 的输出和当前隐层$ H^k $ 的输入来共同决定的。并且门$ \alpha _g^k $也是一个$h$维的向量，也就是说在隐层特征的各个维度上各有一个属于$(0,1)$范围的值来控制信息的通过。当$k$较小时，也就是在高阶近似层的前几层，$ h^k $ 与 $ h^1 $ 的差异较小，因此可以减小门$ \alpha _g^k $，更多的让 $ h^k $ 的信息传递进来，捕获高阶近似。当$k$较大时，也就是高阶近似层的后面的一些层，$ h^k $ 可能已经在较大范围的邻居上过于平滑了，丢失了节点个体信息，因此与 $ h^1 $ 偏差较大。此时应该增大门 $ \alpha  _g^k $ ，让之前的一阶邻近 $ h^1 $ 占据更多的主动权，将局部细节补充进来。由于门的计算是自适应的，AHOGE的建模方式使其能够自适应地捕获高阶近似，编码的结果既包含了局部细节又融入了全局结构。 接下来，本章具体对模型中的一阶近似单元和高阶近似单元的结构进行介绍。</p><h2>近似单元内部结构</h2><h3>一阶近似单元</h3><p>图卷积神经网络（GCN）提出了一个简单但是效果很好的分层前向传播规则，用于解决图半监督节点分类问题。Kipf<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>给出了一个两层的GCN，其核心是图卷积运算：<br>$$H^{(l+1)} = \sigma ( \widehat{A} H^{(l)} W^{(l)})$$</p><p>其中$\widehat{A}$为再归一化后的图拉普拉斯矩阵。 AHOGE同样也采用了GCN中利用$\widehat{A}$来收集局部一阶邻近信息的思想。</p><p>此外，在保持一阶近似的基础上，AHOGE还能自适应地保留个体细节。 在一阶近似单元中，GRU中的重置门和更新门的思想被采纳，通过这两个门来自适应地融合个体细节特征和捕获的一阶近似。下图展示了一阶近似单元的内部结构。请注意，在AHOGE中只有一层一阶近似单元，图z只是给出了其一般形式。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/block1.png" alt="一阶近似单元的内部结构"></p><p>首先，该单元使用GCN中的技术收集一阶邻居的信息：<br>$$ h_{nb,i}^t = \sum_{j \in N_i \cap i} \widehat{A}_{ij} h_{j}^{t-1} $$</p><p>其中$N_i$为节点$v_i$的直接邻居的集合，$h_j^{t-1}$为第$t-1$个隐层中节点$v_j$的输出。在一阶近似中，$j$、$k$和$m$示意了节点$v_i$的邻居。 $ h_{nb,i}^t $加权求和了节点$v_i$的一阶邻居，因此可以视为节点$v_i$在此时的上下文，保持了一阶近似。</p><p>其次，该单元使用重置门来控制前一隐层的输出$ h^{t-1} $中有多少信息需要被保留，并使用tanh激活函数做非线性变换得到预输出$ \overline{h}_i^t $。<br>$$ r^t_i  =  \sigma (W^r h_i^{t-1} + U^r h_{nb,i}^t)  $$<br>$$ \overline{h}_i^t  = \text{tanh} ( h_{nb,i}^t + r^t_i \circ h_i^{t-1} ) $$</p><p>$ W^r \in \mathbb{R}^{h \times h}$和$U^r \in \mathbb{R}^{h \times h} $为该重置门的可训练参数，控制每个维度上的权重。$\circ$为矩阵上的按位乘法。 $ \overline{h}_i^t $可以视为此一阶近似单元的隐状态。</p><p>最后，更新门$z^t$决定了需要遗忘多少上一层的信息，合并单元隐状态$ \overline{h} _i^t$ 得到最终的输出：<br>$$ z^t_i  =  \sigma (W^z h_i^{t-1} + U^z h_{nb,i}^t) $$<br>$$ h_i^t  =  z^t_i \circ h_i^{t-1} + (1-z^t_i) \circ \overline{h}_i^t $$</p><p>$W^z \in \mathbb{R}^{h \times h}$和$U^z \in \mathbb{R}^{h \times h}$为该更新门的可训练参数。</p><p>在AHOGE中，此时的$ h^{t-1} $ 为 $ h^0 $  ，也就是输入特征经过特征变换层后的结果。$h^0$的计算尚未利用邻居的信息，因此可以认为 $h^0$ 保留了个体细节特征。 而$ h_{nb,i}^t $利用$ \widehat{A}_{ij}  $对邻居的 $h^0$ 加权求和，因此可以认为其捕获了一阶近似。不难发现，更新门和遗忘门从保留 $h^0$ 的信息以获取当前单元隐状态和遗忘$h^0$的信息得到最终输出两个角度，通过以 $h^0$ 和 $h_{nb,i}^t$ 作为输入自适应计算得到的各个维度的权重，来自适应地融合这两部分的信息。因此，一阶近似层$H^0$在保持一阶近似的同时自适应地保留了局部个体细节。</p><p>一阶近似单元有两个输出，将求得的 $h_i^t$ 分别传递到下一个隐层 $H^2$ 和各个高阶近似层 $H^k$ 。前者为神经网络中普通的前向传播，后者为采用高速网络的方式将隐层 $H^1$ 与后面的高阶近似层连接。后者的做法有两个目的：其一，将保留了个体细节的一阶近似通过高速网络自适应地合并入各个高阶近似中。其二，高速网络的提出是为了解决模型过深造成的梯度爆炸和梯度消息问题，AHOGE这样处理可以使得模型变得更深，捕获更高阶的信息，同时解决反向梯度求解的问题。</p><h3>高阶近似单元</h3><p>接下来，本节具体介绍高阶近似单元。下图说明了节点$v_i$对应的高阶近似单元的内部结构。该单元有三个输入：节点$v_i$自身上一层的输出$ h^{t-1}_i $、上一层邻居的输出$ h_j^{t-1} $和节点$v_i $在一阶近似层的输出$ h^1_i $。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/block2.png" alt="高阶近似单元的结构图"></p><p>与上文的公式一致，此时的 $ h^t_{nb,i} $ 为一阶邻居的加权求和。高阶近似单元同样也有两个门：重置门和自适应门。重置门的定义与一阶近似单元中的一样，此处为了简练起见不再赘述。值得注意的是，一阶近似中两个门都是为了控制一阶近似和个体细节的信息的合并，这种定义方式是为了与GRU中的单元保持一致。但是，本文认为一个门就足够，具体的例子为高速网络。高速网络只使用一个门来控制两个输入的信息的合并。此处，为了减少模型参数，降低模型的复杂性，本文同样只采用一个门，也就是只使用重置门来控制节点 $v_i$ 自身的输入 $h^t_i$ 和其上下文 $h^t_{nb,i}$ 。省下的更新门替换为自适应门，自适应门控制需要融合多少 $h_i^1$ 的信息。其定义为：<br>$$ a^t_i  =  \sigma (W^a h_i^{1} + U^a h_{nb,i}^t) $$</p><p>在自适应门的影响下，高阶近似单元的输出$h^i_t $能自适应保留一阶近似 $ h_i^1$：<br>$$ h_i^t  =   a^t_i \circ h_i^{1} + (1-a^t_i) \circ \overline{h}_i^t $$</p><p>根据上一章的描述，这种堆叠$k$个隐层的方式能使得第$k$个隐层的输出保持$k$阶近似。故该高阶近似单元的隐状态$\overline{h}_i^t$ 捕获了$k$阶近似。输出$h_t^i $在自适应保留一阶近似$ h_i^1 $信息的同时能保持高阶近似。而且，$h_i^1$在保持一阶近似的同时自适应保留了个体细节，所以AHOGE最后一个高阶近似层的输出 $h^6$（模型最终得出的图编码）能够自适应捕获高阶近似，并且自适应地融合了个体的细节特征。</p><p>在模型的最后是一个多分类softmax分类器，这里与GCN等大多数图编码方法一致。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>PHAM T, TRAN T, PHUNG D Q, et al., 2017. Column networks for collective classification.[C]//AAAI. 2485–2491. <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>SRIVASTAVA R K, GREFF K, SCHMIDHUBER J, 2015. Highway networks[J]. arXiv preprint arXiv:1505.00387. <a href="#fnref2" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>KIPF T N, WELLING M, 2016. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907. <a href="#fnref3" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章已经详细描述了在半监督图编码任务中，保持高阶近似的重要性。尤其当图比较稀疏时，节点的一阶和二阶邻居的数目太少以致于不能准确反映节点的上下文。从更细的角度来看，图中不同节点所处的位置不同，其局部图结构也就不同。有些节点处于图中比较稠密的部分，比如社团中部，邻居信息足够，能准确地反映节点的类别分布。但是信息过多也不一定是一件好事，比如有些节点夹在两类节点和两个社团的中间，这些节点的邻居很多但是信息也很杂，此时需要进一步扩大邻居范围，看其偏向哪一方。同时，也有一些节点处于一条长链的局部结构中，这些节点只有一个或两个邻居，就算扩大视野到2步邻居，信息也不够。在这种情况下，需要扩大感受野到整条链甚至到这条链所连接的社团。&lt;/p&gt;
&lt;p&gt;上述描述是为了说明不同位置的节点需要的感受野大小不同（$k$阶邻居的$k$不同）。目前的模型包括之前提出的Graph-GRU都是对所有节点一视同仁，使得每个节点都捕获相同的$k$阶近似。显然，对于有些节点，这个固定的$k$值可能比较小，邻居信息不够；对于有些节点，$k$值可能稍微偏大，邻居信息太多以致于紊乱。因此，本章的出发点是，能否对不同节点自适应的捕获不同的高阶近似？此外，当捕获的$k$阶近似中$k$较大时，编码过程需要在以目标节点为中心的$k$阶子图上对目标节点的编码进行平滑。 此时，学得的编码可能过于平滑以致于丢失局部细节。基于以上出发点，本章提出了一个端到端的自适应高阶图编码（Adaptive High-Order Graph Embedding, AHOGE）神经网络，在自适应地保持高阶近似的同时保留个体细节特征。&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="深度学习" scheme="https://zekizz.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="graph embedding" scheme="https://zekizz.github.io/tags/graph-embedding/"/>
    
      <category term="半监督分类" scheme="https://zekizz.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%88%86%E7%B1%BB/"/>
    
      <category term="RNN" scheme="https://zekizz.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>图半监督节点分类之三——基于循环神经网络</title>
    <link href="https://zekizz.github.io/ML/graph-based-semi-supervised-classification-3/"/>
    <id>https://zekizz.github.io/ML/graph-based-semi-supervised-classification-3/</id>
    <published>2018-10-03T03:13:05.000Z</published>
    <updated>2019-10-13T09:20:48.297Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>高阶依赖建模一直是图编码研究的热门话题，通过高阶依赖建模能捕获更长的网络结构依赖，保持节点之间的高阶近似。直观上，两个节点没有边相连，并不意味着这两个节点不相似。相反，如果两个节点在网络上通过某些节点能建立很强的联系，这两个节点仍然可以被认为相似。上一章提出的GCN-CRF模型，是基于GCN做图编码的。但是GCN是基于图上谱卷积的一阶近似方法，从而只能捕获一阶依赖。本章的目标是对更远的依赖范围进行建模。接下来的部分将具体给出针对高阶依赖进行建模的思考与建模过程。</p><a id="more"></a><h1>高阶依赖建模</h1><p>本文认为不同节点之间的$k$阶关系信息（对于不同的$k$值）揭示了与该图相关的有用的全局结构信息，并且在学习一个良好的图表示时，显式充分利用这一点非常重要。 下图给出了一些说明性的例子<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，展示了对于节点$A_1$和$A_2$，需要捕获$k$阶（对于$k=2,3,4$）关系的重要性。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/high-order.png" alt="在图中捕获高阶依赖的示意图。"><br>在该图中，粗线表示两个顶点之间的强关系，而细线表示较弱的关系。图a)示意了2阶信息，其中节点$A_1$和节点$A_2$没有直接相连，但是共享了很多邻居。显然，两阶信息对于捕捉两个节点之间的连接强度是非常重要的，它们共享的邻居越多，它们之间的关系也就越强。图b)展示了3阶邻居信息。节点$B$和节点$A_2$之间的大量共同邻居，增强了节点$A_1$和节点$A_2$之间的联系。图c)描述了4阶邻居的情况。从节点$A_1$到节点$A_2$需要走4步，但是$A_1$和$A_2$分别与$B_1$和$B_2$有很强的连接关系而且$B_1$和$B_2$共享了4个邻居，这使得$A_1$和$A_2$之间的关系很明显是很强的。显然，为了利用全局结构信息学得更好的图表达，高阶依赖的信息必不可少。</p><p>图中的高阶近似度量一般可以表示为非对称的概率转移矩阵$S$，反映节点之间的相关程度。一种直观的方式便是学习得到该概率转移矩阵，然后用该矩阵$S$替代上一篇文章中分类任务的邻接矩阵$A$。常见的概率转移矩阵$S$的计算方式有：Katz Index、Rooted PageRank、Common Neighbors和Adamic-Adar等。以上高阶近似的度量方式虽然简单，但是需要计算两个大矩阵之间的乘法，当网络中的节点数目$N$较大时，时间和空间的开销都较大。因此，本文并没有采用以上方式来对高阶依赖进行建模。受CLN（Column Network）<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>的启发，本文采用多个隐层堆叠的方式来对高阶依赖建模。下图展示了CLN是如何通过堆叠多个隐层来捕获高阶依赖的：<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/CLN_high-order.png" alt="一个展示CLN如何捕获高阶依赖的示意图"><br>在图中，左侧是一个简单的拥有4个节点的图结构，右侧是其对应的CLN模型。CLN根据邻接矩阵$A$来对构建层与层之间的连接关系。图中高亮了推理节点$v_4$的过程。这里的示意图有两个隐层，因此能捕获二阶依赖。从推理结果$y_4$反向看过来，首先 $y_4$ 是基于隐层 $ h_4^2 $（编码结果）通过分类器得到的，而 $ h_4^2$ 通过邻接矩阵$A$接受前一个隐层的输入 $ h_3^1 $ 和其本身 $h_4^1$ 。该过程为通过邻接矩阵$A$捕获的一阶依赖。通过两个隐层的堆叠，最终$y_4$的计算依赖于图中所有节点：$ x_1 $ 到 $ x_4 $ ，因此隐式捕获了二阶依赖。 堆叠更多的隐层将会捕获更高阶的依赖关系。</p><h1>Graph-GRU</h1><h2>模型结构与解释</h2><p>然而，CLN本质上仍是全连接的前馈神经网络，只是根据邻接矩阵舍弃了一些连接，因此隐层的数目不能太多，太多不利于梯度的反向传播，造成梯度消失或者梯度爆炸。从CLN中不难发现，从节点特征$X$到推理结果$y_4$的过程可以视为四个前向序列。循环神经网络（Recurrent Neural Networks, RNN）为一种常用的处理序列数据的方案，最常使用的RNN模型为LSTM（Long-Short-Term-Memories）模型。为了捕获长距离依赖，LSTM引入门（Gate）的机制来解决序列过长造成的梯度爆炸和梯度消失问题。 LSTM的单元模块中有三个门：遗忘门$f_t$、输入门$i_t$和输出门$o_t$。遗忘门$f_t$控制忘记多少上一时刻的单元状态$c_{t-1}$，并将剩余的信息累积到当前时刻的单元状态$c_{t}$中。输入门$i_t$控制当前时刻的信息有多少可以融入cell状态$c_t$中。输出门$o_t$控制当前时刻的单元状态$c_t$有哪些信息需要输出到当前时刻隐状态$h_t$。LSTM中最核心的是单元状态$c_t$。$c_t$ 像一个传送带，直接打通整个LSTM序列。在整个前向传播的过程中只在$c_t$上进行一些简单的线性操作，通过门来控制$c_t$中信息的增减，这使得信息可以沿着它以不变的方式流过序列。由于是线性操作，在反向求梯度的时候，将一般多层前馈求梯度中的乘法变成了加减法，从而解决累乘造成的梯度爆炸和消失问题。</p><p>RNN一般用于语音、词序列和时间序列等一维的序列结构，但是本文现在要处理的问题是二维的网络结构，两者的共同之处是，本文同样需要学习长的邻居依赖。这个模型的迁移就是本章工作的要点。近期，Liang<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>提出了Graph LSTM，扩展传统的LSTM模型，使其可以应用在图结构的数据上。Graph LSTM通过以语义上的超像素为节点，节点之间的空间邻近关系作为边，构建无向图。与传统LSTM向固定的序列邻居结构传递消息不同的是，Graph LSTM的每个超像素节点向图结构上的其他邻近超像素节点传递消息，不同节点的邻居结构是不同的。在节点的信息更新上，首先通过自适应的方式选出一个开始节点，然后按照信心驱动方案（Confidence-driven Scheme）来确定更新序列。这样确实是一种常规的方案，但是在模型的训练过程中需要提前训练好前面的CNN层，然后再更新LSTM节点，最后用梯度下降来调整参数。虽然是端到端的模型，但是需要两次训练，而且更新LSTM节点的过程，不仅在实现上比较麻烦，而且计算过程也会比较费时。因为必须按顺序更新，不是一个同步的过程。借鉴于LSTM的思想，本章提出了另外一种在图结构上实施RNN的方案。为了进一步减少模型参数，本文采用了LSTM的一种变体GRU（Gated Recurrent Units）<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>作为基本单元模块，并提出了图门控循环单元模型，Graph-GRU（Graph Gated Recurrent Units），采用RNN来保持$k$阶近似。</p><p>如果把构建的二维无向图平放在一个平面上，Graph LSTM方案可以说是在水平方向上构建RNN模型。那么本章提出的Graph-GRU可以视为在“竖直”方向构建RNN，通过复制构建的图使得每个隐层代表一个图，隐层与隐层之间通过邻接矩阵$A$构建连接，就像CLN那样。与CLN不同的是，Graph-GRU将隐层中的每个节点替换成了RNN单元，在隐层之间构建GRU，从而解决梯度爆炸和梯度消失的问题。这使得Graph-GRU将RNN变成在隐层之间的前馈过程，并能构建更深的模型，捕获更长范围的依赖，构建保持更高阶近似的模型。结构如下图：<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/model_graph-gru.png" alt="一个拥有两个隐层的Graph-GRU模型的模型结构"><br>假设有一个包含5个节点的无向图，如图中左边部分所示，其对应的Graph-GRU模型的模型结构见图中右边部分。图中矩形框为GRU单元，一个GRU单元代表网络中的一个节点。该单元可以视为一系列运算的集合，只是这些运算在不同节点上重复出现，因此将其模块化来简化求解。可以看到，每一层有5个节点，代表左边无向图中的5个节点，层与层之间的连接反映了左边无向图的图结构。通过构建多层网络，最后输出层的每个节点可以捕获多步邻居的依赖，这个具体已经在上一小节中给出了解释。当然，随着层数的增多，会使模型变复杂，训练更费时，求解更困难。通过实验分析，本文最终选择的隐层层数为5-6层。</p><p>按照上一篇文章中的公式定义，$X \in \mathbb{R}^{N \times d}$为输入特征矩阵，$W^1  \in \mathbb{R}^{d \times h}$ 和 $W^2  \in \mathbb{R}^{h \times c}$为可以训练的隐层权重矩阵，其中$N$为网络中节点个数，$d$为输入特征维度，$h$为编码空间的维度，$c$为类别个数。$H^i \in \mathbb{R}^{N \times h}$ 为第$i$个隐层的输出。在Graph-GRU中，输入特征首先被映射到一个$h$维的隐空间：<br>$$ H^0 = \sigma (XW^1) $$<br>其中$\sigma$为非线性激活函数，本文选用ReLu激活函数。接下来，信息在隐层之间传播。隐藏层之间的连接由图结构（邻接矩阵$A$）引导。 也就是说，如果图中节点 $ v_i $ 与 $ v_j $ 有边相连，第$t-1$个隐层中的第$i$个节点 $h_i^{t-1}$ 将连接下一隐层的第$j$个节点 $ h_j^t $ 。</p><p>为了区分不同结构的邻居，本文采用了GCN中的处理技巧，使用再归一化后的图拉普拉斯矩阵$\widehat{A} $来作为图中边的连接权重。同时，节点$ h_i^{t-1} $也与节点 $ h_i^t $ 相连，来获取节点 $ v_i $ 自身在前一个隐层的输出。因为隐层之间为RNN的连接关系，$ h^t $ 便对应了RNN单元的单元输出。通过多个隐层的编码，最后一个隐层 $ H^t $ 的输出即为保持了高阶近似的图节点编码结果。基于该编码结果，最后连接一个softmax分类器来输出模型的分类结果 $ Y \in \mathbb{R}^{N \times c} $ ：</p><p>$$ Y = \text{softmax} (H^t W^2) $$</p><h2>RNN模块</h2><p>接下来，本节对Graph-GRU中的RNN单元进行介绍，并解释信息是如何在隐层间前向传播的。首先需要注意的是，标准GRU单元模块有两个输入，当前时刻的输入特征$x_t$和前一时刻的隐状态$h_t$。但是，在本文处理的问题中，每个节点有多个邻居的输入和其自身的前一时刻的状态输入。为了弥补这一差异，本文将标准的GRU单元进行了一定的修改，将汇聚了邻居信息的$g^{t}$作为输入特征$x_t$并将上一个隐层的输出作为$h_t$。下描述了使用的RNN单元的内部结构。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/rnn_cell.png" alt="Graph-GRU中RNN单元的结构图"></p><p>图示的RNN单元为第$t$个隐层的第$i$个RNN单元，对应于图中的节点$v_i$。该单元首先会接收节点$v_i$的上一个隐层的输出$ h^{t-1}_i $ ，这个过程与标准的GRU模型保持一致。不同的是GRU单元中的当前时刻的特征输入$x_t$，本文的GRU层是纵向前馈的，而且需要汇聚邻居的信息。在GCN中，节点汇聚了邻居的信息后，结合自己的旧信息，加权求和得到了自己的新信息。所以加权求和后的邻居信息一定程度反映了目标节点的信息，加权求和后的邻居信息也可以视为目标节点的上下文，故可以作为当前时刻的特征输入。于是，本文将$g^{t}$定义为：<br>$$ g_i^t = \sum_{j \in N(i) } a_{ij} h_j^{t-1} $$</p><p>其中$N(i)$为节点$v_i$的邻居集合，$a_{ij}$为节点$v_i$与节点$v_j$之间边的权重。这里本文采用GCN中计算方式：<br>$$\widehat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} $$</p><p>使用$a_{ij} = \tilde{A}_{ij}$来度量节点间的相似性。上图中$h_m<sup>{t-1}$和$h_n</sup>{t-1}$示意节点$v_i$的邻居。与节点 $ h_i^{t-1} $ 一样，$ h_m^{t-1} $和$ h_n^{t-1} $也是RNN单元的输出，这里为了简洁起见，省略了它们的单元框。</p><p>得到了$ g^t $ 后，$ g^t $ 连同 $h_i^{t-1}$ 一起被送入GRU单元。在GRU单元中有两个门，重置门 $r^t$ 和更新门 $z^t $，其计算方式为：</p><p>$$ r^t_i = \sigma (W^r g^t_i + U^r h^{t-1}_i) $$<br>$$ z^t_i = \sigma (W^z g^t_i + U^z h^{t-1}_i) $$</p><p>其中$W^r, U^r, W^z, U^z \in \mathbb{R}^{h \times h}$ 为各个门的可以训练的权重矩阵。这些权重在所有RNN单元上共享，增加GRU隐层不会增加模型参数。得到两个门后， 首先通过重置门 $r^t$ 来控制当前时刻需要保留多少上一时刻的信息，并得到候选隐状态 $\widetilde{h}^t$ ，等待输出。<br>$$ \widetilde{h}^t_i = \text{tanh}(g^t_i  +r^t_i h^{t-1}_i) $$<br>注意这里的计算与标准的GRU不同，如果按照原本的GRU，其计算方式为$\widetilde{h}^t_i = \text{tanh}(Wg^t_i  +r^t_i U h^{t-1}_i)$ 。 此处，本文省略了权重矩阵$W$和$U$来减少模型的参数。事实上，实验表明，这样的处理能提升推理正确率。 其次更新门 $ z_i^t $ 选择性地遗忘前一时刻的隐状态$h^{t-1}_i$，并加入当前时刻的隐状态信息，得到最终的输出 $h_i^t$ ：<br>$$ h^t_i = z^t_i \circ h_i^{t-1} + (1-z^t_i) \circ \widetilde{h}^t_i $$</p><p>这里$\circ$为按位乘运算。由于$W$和$U$都是$h \times h$ 的矩阵，得到的门 $ r_i^t $ 与 $ z_i^t $ 均为$h$维的向量。也就是说，在特征的每个维度上都有个权重，控制每个维度信息的通过。</p><h1>模型训练</h1><p>Graph-GRU损失函数的定义与GCN-CRF的定义一致，本章不再赘述。对于模型的训练，本节更加关注的是模型的训练方式：Full Batch和Mini-batch。</p><h2>Full Batch</h2><p>Full Batch方式，顾名思义，每次进行训练时输入全部的训练样本，在全部样本上计算损失函数，然后针对每个参数计算梯度。全批次方式一般用于数据量较少的情况，由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向 。数据量较大时，输入全部样本会造成内存开销太大和训练时间长等问题。但是，在图结构数据上，采用全批次方式似乎是最优的方法。由于图的整体结构的约束，去除任何样本，或者任何对样本的划分方式，都会造成网络结构的不完整，影响推理的准确性。因此目前基本所有图编码方法都是采用的全批次方式，上一章描述的GCN-CRF也是采用全批次方式进行训练的。</p><p>上文所示的GRU模型结构示意图就是全批次方式。由所有样本的特征组成的特征矩阵$X$被用作输入，通过Graph-GRU学得所有样本的类别标签$Y$。然后，部分节点（有标记的节点）的输出$y_i$会反向传播有监督损失，剩余节点保持不动。由于网络的连接，反向的梯度能传到网络中未标记节点。</p><h2>Mini-batch</h2><p>在Mini-batch方式中，每次挑选一部分（batch大小）样本用作训练，而不是全部数据，通过多次分批多次调整模型参数。如此一来，每次参与训练的数据量可以调控，不至于太大导致内存不够用，减少计算量，提高内存利用率。同时每次挑选一批样本而不是一个样本，可以减少梯度下降的随机性。因此小批次方式在大样本情况下得到了广泛的应用。<br>值得注意的是，全批次和小批次方式一般都用于有监督学习，在半监督模型上采用小批次将会面临如何挑选合适的有标记和无标记样本的难题。倒是全批次方式不必担心这个问题，因为它将所有样本都用于了训练。根据上文所述，任何数据子集的选取方式都会造成网络的完整，故基于图半监督学习的小批次方式的构建是一个很有挑战和意义的问题。</p><p>针对以上问题，本文给出了一种简单的小批次训练方式。对于每个batch，首先挑选一个目标节点集合（该batch的种子样本），这些节点最好是从一个社区或者紧密连接的节点群体中挑选。然后，所有种子节点的$k$阶邻居（这里$k$对应于Graph-GRU中隐层的数目）被挑选入当前batch的训练集合，并由这些节点构建一个局部子图。极端情况下，可以将整个网络中的每一个联通子图作为一个batch。<br>这样的挑选方式将使每个节点在模型中的计算方式（具体指依赖的$k$阶邻居）与全批次方式一致，同时也减少了每次参与训练的节点数目。最后在构建的子图上训练模型。不难看出，相同的节点可能出现在不同的batch中，那么节点的类别标签推理结果可以有两种方式确定：其一，对每个batch中的推理结果求平均、取最大或者投票的方式。其二，小批次只是用来调整模型中的参数，小批次训练结束后，将所有节点喂入模型，只计算前向过程，预测得到所有样本的类别标签。</p><h2>参考文献</h2><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>CAO S, LU W, XU Q, 2015. Grarep: Learning graph representations with global structural information[C]//Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. [S.l.]: ACM: 891–900. <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>PHAM T, TRAN T, PHUNG D Q, et al., 2017. Column networks for collective classification.[C]//AAAI. [S.l.: s.n.]: 2485–2491. <a href="#fnref2" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>LIANG X, SHEN X, FENG J, et al., 2016. Semantic object parsing with graph lstm[C]//European Conference on Computer Vision. [S.l.]: Springer: 125–143. <a href="#fnref3" class="footnote-backref">↩</a></p></li><li id="fn4" class="footnote-item"><p>CHO K, VAN MERRIËNBOER B, GULCEHRE C, et al., 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078. <a href="#fnref4" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;高阶依赖建模一直是图编码研究的热门话题，通过高阶依赖建模能捕获更长的网络结构依赖，保持节点之间的高阶近似。直观上，两个节点没有边相连，并不意味着这两个节点不相似。相反，如果两个节点在网络上通过某些节点能建立很强的联系，这两个节点仍然可以被认为相似。上一章提出的GCN-CRF模型，是基于GCN做图编码的。但是GCN是基于图上谱卷积的一阶近似方法，从而只能捕获一阶依赖。本章的目标是对更远的依赖范围进行建模。接下来的部分将具体给出针对高阶依赖进行建模的思考与建模过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="深度学习" scheme="https://zekizz.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="graph embedding" scheme="https://zekizz.github.io/tags/graph-embedding/"/>
    
      <category term="半监督分类" scheme="https://zekizz.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%88%86%E7%B1%BB/"/>
    
      <category term="RNN" scheme="https://zekizz.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>图半监督节点分类之二——基于条件随机场</title>
    <link href="https://zekizz.github.io/ML/graph-based-semi-supervised-classification-2/"/>
    <id>https://zekizz.github.io/ML/graph-based-semi-supervised-classification-2/</id>
    <published>2018-10-02T13:57:27.000Z</published>
    <updated>2019-10-13T09:20:48.298Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>本文关注于采用半监督图编码方法来对图中节点分类，根据上文的介绍，半监督图编码方法有两个过程：节点特征编码和节点分类。本文介绍基于条件随机场的图卷积网络（Graph Convolutional Network with Conditional Random Field, GCN-CRF）模型，关注于优化节点分类过程。GCN-CRF通过在softmax层前添加条件随机场模块来平滑GCN的分类结果。</p><a id="more"></a><h2>条件随机场</h2><p>条件随机场（Conditional Random Field, CRF）已经成功地应用到了图像分割和图像标注等问题上。通常的做法是在像素点或者图像区块上定义条件随机场<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>，然后做最大后验（MAP）推理。CRF通过引入一些约束来平滑分类结果，这些约束倾向于减少对象边缘附近的错误分类。全连接CRF已经成功地被用来改善卷积神经网络CNN的语义标记结果<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>。接下来本文以图像像素级标注问题为例，简要介绍条件随机场模型。</p><p>令$Y_i$为隶属于像素点$i$的随机变量，表示赋予像素点$i$的类别标签。$Y_i$的取值范围为预先定义的标签集合$\mathbb{L} = {l_1, l_2,\cdots,  l_c}$ ，$c$为类别的个数。给定一张有$N$个像素点的图像，以及图像的全局观测$X$（每个像素点的特征，这里一般为RGB三通道的值），观测标签对 $(X,Y)$ 可以被建模为一个形式为吉布斯分布 $P(Y=y|X) = \frac{1}{Z(X)} \exp(-E(y|X))$ 的条件随机场。这里$E(y|X)$为标记$y \in \mathcal{L}^N$的Gibbs能量。从现在开始，为了方便起见，我们在公式中去除了条件于$X$，例如用$E(y)$替代$E(y|X)$。</p><p>在全连接的成对CRF模型中<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>，类别标签指派$y$的能量由下式给出：<br>$$ E(y) = \sum_{i} \psi_u(y_i) + \sum_{i&lt;j} \psi_p(y_i, y_j) \tag{1} $$</p><p>其中$i$和$j$的范围都是从1到$N$。一元能量部分$\psi_u(y_i)$度量像素点$i$的标签分配$y_i$的逆似然（inverse likelihood）。在图像分割中，该一元能量是通过CNN获得的。粗略地说，CNN在预测像素的标签时并未考虑标签分配的平滑性和一致性。 本文采用<a href="https://github.com/lucasb-eyer/pydensecrf" target="_blank" rel="noopener">PyDenseCRF</a>中的方法从分类器输出的类别概率分布计算该一元能量。公式中第二部分的成对（pairwise）能量部分，本文称之为二元能量。二元能量提供像素点间的平滑项，鼓励将相似标签分配给具有相似属性的像素。其常用的定义为加权高斯：<br>$$ \psi_p(y_i, y_j) = \mu (y_i, y_j) \sum_{m=1} ^M w^{(m) } k_G^{ (m) }(f_i, f_j)  \tag{2}$$</p><p>其中每个$k_G^{(m)}$ 是作用于特征向量的高斯核。像素点$i$的特征向量$f_i$源自图像特征，比如空间位置和RGB值。函数$\mu ( \cdot,\cdot )$为标签兼容性函数，用于捕获不同标签之间的兼容性。最小化上述CRF能量$E(y)$将为图像产生最可能的标签分配$y^*$。</p><h2>基于条件随机场的图卷积网络</h2><p>受上述图像中像素级标注任务的启发，本文想要借用成对条件随机场来改善GCN的分类结果，就像图像中采用条件随机场来平滑CNN的结果一样。 基于这个出发点，本章提出了端到端的基于条件随机场的图卷积网络——GCN-CRF（Graph Convolutional Network with Conditional Random Field）神经网络模型。 值得注意的是，图像中的像素点之间是严格的栅格结构，每个像素点周围的像素点的结构是固定的。而且图像中每一小块都是由一堆像素点堆叠组合而成的，所以局部邻近像素点之间存在一定的关联性，但是只考虑一阶邻居，信息量太少，所以之前的工作在所有像素点上构建全连接的成对条件随机场。在实际网络数据中，比如学术网络、社交网络等，网络是任意的二维结构，每个节点的邻居的数量与局部结构都是不一样的。但是与图像中的像素点不同的是，实际网络中的网络结构显示的指定了节点之间的关系。因此，我们可以在实际的网络结构上构建条件随机场。由于实际中网络比较稀疏，相对于全连接的情况，网络中的边的数目大幅度地减少，这使得模型的求解变得更加快速。 由于图片的栅格结构本质上也是一个图结构，本节仍可以采用上一小节CRF的符号定义，接下来的部分将对此详细介绍。</p><h3>图上条件随机场的定义</h3><p>相比于图像中的RGB观测值，定义中的$X$此时为节点特征矩阵。节点间的连接关系也从图像中像素点上的全连接变成了图结构$A$。一元能量函数可以从GCN的分类结果中得到。对于二元能量函数，本文借鉴了Krähenbühl<sup class="footnote-ref"><a href="#fn4" id="fnref4:1">[4]</a></sup>中的定义，其定义如下：<br>$$ k(f_i,f_j) = w^{(1)} \exp (-\frac{ |p_i - p_j|^2 }{ 2\theta_{ \alpha }^2} - \frac{ |I_i - I_j|^2 }{ 2\theta_{\beta}^2 } ) +  w^{(2)} \exp (-\frac{|p_i - p_j |^2 }{2 \theta_{\gamma}^2}) \tag{3}$$</p><p>其中$I_i$为像素点$i$的RGB三通道颜色值，$p_i$为像素点$i$在图片中二维坐标，因此能比较方便的计算距离，而且距离的计算也具备一定的意义。但是，如果扩展到任意的网络结构数据上，节点到节点之间的距离计算可以通过最短路径的长度来表示，但是可以想象计算量的巨大。像素点的RBG三通道可以扩展成任意维度的节点特征，但是这里需要计算节点间的相似性，开销是平方级的，因此Krähenbühl利用高纬度滤波（High-Dimension Filtering）近似计算将开销降为线性。与图像不同的是，图像的观测是RGB三通道的值，这属于自然观测的ground truth，而非提取出的特征，因此能反映真实情况，具备准确性。然而，如果是基于抽取出来的特征的话，由于特征的抽取会存在特征选取的合理性、抽取过程的准确性等因素，抽取的特征存在太多噪声和不准确性，并不能准确地反映样本的真实观测，因此在抽取的特征上我们无法像公式(3)那样添加特征上的依赖。</p><p>综上所述，本质上核函数是为了计算节点之间的相似性，相似性越大，核函数的值越大。由于CRF是加在GCN的后面，要编码的特征已经融合了邻居信息，因此本文的目标是让具有相似特征的邻近节点其标签倾向于相似。同时，为了降低模型的计算复杂度并方便与GCN衔接，本文将二元能量函数定义为：<br>$$ \psi_p(y_i, y_j)  =  \mu(y_i, y_j)k(f_i, f_j)  = \mu(y_i, y_j) \exp( - \frac{ \widehat{A}_{ij} }{2\alpha^2}) \tag{4} $$</p><p>其中$\widehat{A}$为经过图拉普拉斯变换后的邻接矩阵，与GCN中的处理方式保持一致，具体已经在上一章进行了详细的解释。$\widehat{A}$可以视为节点间在结构上的距离。本文也尝试了像公式(3)一样引入在节点特征上的距离的方案，但是实验结果并没有想象的好，因为编码的特征和原始特征都存在误差，不能像RGB颜色一样反映自然观测。因此，本文采纳了公式(3)的定义来提高正确率和减小计算复杂度。</p><p>本文采纳了一种简单的标签兼容性函数$\mu$，其定义为$ \mu(y_i, y_j) = [ y_i \neq y_j]$ 。这样的定义方式会给网络结构上邻近但是标签不同的节点对引入惩罚。这种简单的定义方式在实际中产生了较好的效果<sup class="footnote-ref"><a href="#fn4" id="fnref4:2">[4]</a></sup><sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1]</a></sup>。在GCN的处理过程中，每个节点收集邻居的信息并将其编码进入一个隐空间。如此，有许多共同邻居的邻近节点倾向于拥有相似的特征编码。二元能量函数提供了一个平滑项，该平滑项鼓励将相似类别分配给具备相似编码的邻近节点。</p><h3>条件随机场的推理算法</h3><p>介绍了CRF的定义，接下来的是CRF的推理求解。精确地最小化能量$E(y)$是困难的，通常的做法是使用平均场（mean-field）近似推理。 Krähenbühl<sup class="footnote-ref"><a href="#fn4" id="fnref4:3">[4]</a></sup>提出了一种高效的平均场近似推理算法，该算法为一种迭代的消息传播算法。具体的算法步骤总结如下：<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/mean_field.png" alt="全连接CRF中的平均场近似推理算法"><br>注意$k^{(m)}(f_i, f_j)$是第$m$个核函数而且在本文的定义中只有一个核函数。 第一步的初始化，可以看作是在负的一元能量上应用softmax函数，用来做归一化处理。消息传播是通过对$Q$值应用$M$个高斯滤波器来实现的。紧接着对上一步的$M$个高斯滤波的结果进行加权求和，并实施兼容性转换，使加权求和的结果以不同程度在不同标签之间共享。最后加上负的一元能量并再次做归一化。如此循环直到$Q$值收敛。</p><h3>模型结构</h3><p>Zheng<sup class="footnote-ref"><a href="#fn1" id="fnref1:2">[1]</a></sup>指出，以上条件随机场的平均场推理算法可以重新表述为循环神经网络（Recurrent Neural Network, RNN）的形式，并且将这种RNN结构命名为CRF-RNN。本章使用了一个与CRF-RNN相似的RNN模块，并基于此模块提出了GCN-CRF（Graph Convolutional Network with Conditional Random Field）模型。下图给出了GCN-CRF模型的模型结构的示意图。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/models_gcn-crf.png" alt="GCN-CRF的模型结构"></p><p>在CRF模块前，是拥有两层图卷积的GCN。第一个GCN层用来做特征编码，其中$X \in \mathbb{R}^{N \times d}$为节点的输入特征，$W_0 \in \mathbb{R}^{d \times h}$ 将原始输入的$d$维特征映射到$h$维的编码空间，用矩阵$A \in \mathbb{R}^{N \times N}$对感受野（一阶邻居）内节点的特征进行加权求和（卷积过程），$\sigma$为非线性激活函数，比如ReLu函数、Sigmoid函数和tanh函数等，本文采用了ReLu函数。紧接着第一个GCN层，第二个GCN层使用$W_1 \in \mathbb{R}^{h \times c}$对编码特征$H_1$进行分类，当然这里的分类结果$H_3$为未使用softmax函数进行归一化处理时的中间结果。GCN-CRF的目的就是在GCN后面的softmax层之前添加CRF模块来平滑GCN的分类结果。由于Zheng<sup class="footnote-ref"><a href="#fn1" id="fnref1:3">[1]</a></sup>将CRF表述为RNN模块，本文便可以很方便直接将该CRF模块加载在GCN后面，构建出一个端到端的深度神经网络模型，并采用标准的反向传播来训练模型，同时调整CRF和GCN中参数。值得注意的是，CRF中只有一个参数$\alpha$，这大大减轻了模型的训练难度。图中的CRF模块是基于算法 1 设计的，CRF中的能量函数的定义参照公式(1)和公式(4)。接下来，本文结合算法1 具体解释GCN-CRF模型结构图中的CRF模块。</p><p>首先需要计算一元能量$U$，由于GCN的输出为类别的概率分布，本文采用<a href="https://github.com/lucasb-eyer/pydensecrf" target="_blank" rel="noopener">PyDenseCRF</a>中的方法将类别概率分布转化为能量$U$，对应CRF模块中的第一层。CRF模块中的第二、三层对应于算法1中的$Q$的初始化，紧接着$ M = \widehat{A}Q $ 对应于消息传播过程。$P = MC $ 对应兼容性转换，其中 $C \in \mathbb{R}^{c \times c}$ 为兼容性矩阵，其定义为：<br>$$ C_{ij} =<br>\begin{cases}<br>1      &amp; \quad \text{if } i \neq j  \quad \<br>0     &amp; \quad \text{if } i=j<br>\end{cases} \tag{5} $$</p><p>对角为0表示一条边上的两个节点如果取相同的标签就不引入误差，否则给出惩罚，这里惩罚都取1。$Q = -U -P$ 对应算法1循环中的第三步，最后再归一化$Q$进入循环。Krähenbühl<sup class="footnote-ref"><a href="#fn4" id="fnref4:4">[4]</a></sup>指出，其提出的平均场推理算法在求解全连接CRF时一般在10次迭代内收敛（CRF模块中的$T$）。在本章提出的GCN-CRF模型中，处理的图是稀疏的而且实验表明5次迭代就已经足够，增加迭代次数并不会提升分类性能。</p><h3>参数估计</h3><p>本文采用了GCN<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>的思想，使用神经网络模型$f(X,A)$直接对图结构进行编码，并在有监督损失$\mathcal{L}_{label}$上进行训练，从而避免了在损失函数中显式的添加基于图的正则化。使函数$f(\cdot)$条件于图的邻接矩阵将允许模型从有监督损失$\mathcal{L}_{label}$中分配梯度信息，并且使模型同时学习具备和不具备标签的节点的特征表示。具体地，对于半监督节点多分类问题，损失函数为定义在所有标记样本上的交叉熵。交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小，一般用来做为神经网络的损失函数，其形式为：<br>$$ \mathcal{L}  = \sum_{i \in V^L} \sum_{j=1}^C Y_{ij} \ln \frac{1}{ \widehat{Y}_{ij} }  = - \sum_{i \in V^L} \sum_{j=1}^C Y_{ij} \ln \widehat{Y}_{ij} \tag{6}$$<br>其中$V^L$为标记节点集合，$C$为类别个数，$\widehat{Y}$为推理的类别标签，$Y$为从数据集中获得的真实标签。由于从数据集中获得类别标签$Y$一般是硬编码的，也就是在类别上的one-hot编码，并且在一个类别上为1，在剩余的类别上为0。比如节点$v_i$的真实类别为$k$，那么$Y_{ik} = 1$， 并且在剩余的类别上$Y_{ij} = 0$。  根据交叉熵的定义，对于节点$v_i$，只会在$k$这一类上计算损失，因为其他的$Y_{ij} = 0$。 那么最小化损失函数将导致推理结果在第$k$类上尽可能大，即使$\widehat{Y}_{ik}$足够大，对其他类别没有约束。</p><p>对于模型的训练，本文采用了全批次随机梯度算法，每次输入全部样本进行训练。为了减轻存储的开销和加速计算，本文使用了稀疏矩阵来表示邻接矩阵$A$，使其内存需求变为$\mathcal{O}(|\mathcal{E}|)$，也就是与图中边的数目成线性关系。实验结构将在后文统一给出。</p><p><strong>参考文献</strong></p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>ZHENG S, JAYASUMANA S, ROMERA-PAREDES B, et al., 2015. Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE International Conference on Computer Vision. 1529–1537. <a href="#fnref1" class="footnote-backref">↩</a> <a href="#fnref1:1" class="footnote-backref">↩</a> <a href="#fnref1:2" class="footnote-backref">↩</a> <a href="#fnref1:3" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>FULKERSON B, VEDALDI A, SOATTO S, 2009. Class segmentation and object localization with uperpixel neighborhoods[C]//Computer Vision, 2009 IEEE 12th International Conference on. IEEE: 670–677. <a href="#fnref2" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>CHEN L C, PAPANDREOU G, KOKKINOS I, et al., 2016. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs[J]. arXiv preprint arXiv:1606.00915. <a href="#fnref3" class="footnote-backref">↩</a></p></li><li id="fn4" class="footnote-item"><p>KRÄHENBÜHL P, KOLTUN V, 2011. Efficient inference in fully connected crfs with gaussian edge potentials[C]//Advances in neural information processing systems. 109–117. <a href="#fnref4" class="footnote-backref">↩</a> <a href="#fnref4:1" class="footnote-backref">↩</a> <a href="#fnref4:2" class="footnote-backref">↩</a> <a href="#fnref4:3" class="footnote-backref">↩</a> <a href="#fnref4:4" class="footnote-backref">↩</a></p></li><li id="fn5" class="footnote-item"><p>KIPF T N, WELLING M, 2016. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907. <a href="#fnref5" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文关注于采用半监督图编码方法来对图中节点分类，根据上文的介绍，半监督图编码方法有两个过程：节点特征编码和节点分类。本文介绍基于条件随机场的图卷积网络（Graph Convolutional Network with Conditional Random Field, GCN-CRF）模型，关注于优化节点分类过程。GCN-CRF通过在softmax层前添加条件随机场模块来平滑GCN的分类结果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="深度学习" scheme="https://zekizz.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="graph embedding" scheme="https://zekizz.github.io/tags/graph-embedding/"/>
    
      <category term="半监督分类" scheme="https://zekizz.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%88%86%E7%B1%BB/"/>
    
      <category term="CRF" scheme="https://zekizz.github.io/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>折腾公式+参考文献</title>
    <link href="https://zekizz.github.io/Blog/math-reference/"/>
    <id>https://zekizz.github.io/Blog/math-reference/</id>
    <published>2018-10-02T04:53:02.000Z</published>
    <updated>2019-10-13T09:20:48.302Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>折腾公式+参考文献</p><a id="more"></a><h2>公式</h2><p><strong>不兼容问题</strong><br>采用kramed</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">uninstall</span> hexo-renderer-marked <span class="comment">--save</span></span><br><span class="line">npm <span class="keyword">install</span> hexo-renderer-kramed <span class="comment">--save</span></span><br></pre></td></tr></table></figure><p>同时修改<code>\node_modules\kramed\lib\rules\inline.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//11行</span></span><br><span class="line"><span class="comment">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line"><span class="built_in">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br><span class="line"></span><br><span class="line"><span class="comment">//21行</span></span><br><span class="line"><span class="comment">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><p>配置MathJax</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- MathJax配置，可通过单美元符号书写行内公式等 --&gt;</span><br><span class="line">script(type=<span class="string">'text/javascript'</span>, src=<span class="string">'cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML'</span>)</span><br><span class="line"></span><br><span class="line">script(type=<span class="string">'text/x-mathjax-config'</span>).</span><br><span class="line">  MathJax.Hub.Config(&#123;</span><br><span class="line">    tex2jax: &#123;</span><br><span class="line">    inlineMath: [[<span class="string">'$'</span>,<span class="string">'$'</span>], [<span class="string">'\\('</span>,<span class="string">'\\)'</span>]],</span><br><span class="line">    processEscapes: <span class="literal">true</span>,</span><br><span class="line">    ignoreClass: <span class="string">"tex2jax_ignore|dno"</span>,</span><br><span class="line">    skipTags: [<span class="string">'script'</span>, <span class="string">'noscript'</span>, <span class="string">'style'</span>, <span class="string">'textarea'</span>, <span class="string">'pre'</span>, <span class="string">'code'</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">    TeX: &#123;</span><br><span class="line">    equationNumbers: &#123; <span class="attr">autoNumber</span>: <span class="string">"AMS"</span> &#125;,</span><br><span class="line">    noUndefined: &#123;<span class="attr">attributes</span>: &#123; <span class="attr">mathcolor</span>: <span class="string">"red"</span>, <span class="attr">mathbackground</span>: <span class="string">"#FFEEEE"</span>, <span class="attr">mathsize</span>: <span class="string">"90%"</span> &#125;&#125;,</span><br><span class="line">    Macros: &#123;<span class="attr">href</span>: <span class="string">"&#123;&#125;"</span>&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    messageStyle: <span class="string">"none"</span>,</span><br><span class="line">    <span class="string">"HTML-CSS"</span>: &#123;</span><br><span class="line">    preferredFont: <span class="string">"TeX"</span>,</span><br><span class="line">    availableFonts: [<span class="string">"STIX"</span>,<span class="string">"TeX"</span>],</span><br><span class="line">    linebreaks: &#123; <span class="attr">automatic</span>:<span class="literal">true</span> &#125;,</span><br><span class="line">    EqnChunk: (MathJax.Hub.Browser.isMobile ? <span class="number">10</span> : <span class="number">50</span>) </span><br><span class="line">    &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">&lt;!-- 给MathJax元素添加has-jax <span class="class"><span class="keyword">class</span> --&gt;</span></span><br><span class="line"><span class="class"><span class="title">script</span>(<span class="title">type</span></span>=<span class="string">'text/x-mathjax-config'</span>).</span><br><span class="line"> MathJax.Hub.Queue(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> all = MathJax.Hub.getAllJax(), i;</span><br><span class="line">  <span class="keyword">for</span>(i=<span class="number">0</span>; i &lt; all.length; i += <span class="number">1</span>) &#123;</span><br><span class="line">    all[i].SourceElement().parentNode.className += <span class="string">' has-jax'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">&lt;!-- script(type=<span class="string">'text/javascript'</span>, src=<span class="string">'cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML'</span>) --&gt;</span><br><span class="line">&lt;!-- script(type=<span class="string">'text/javascript'</span>, src=<span class="string">'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'</span>) --&gt;</span><br></pre></td></tr></table></figure><p><strong>这里有个坑需注意</strong><br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/math.png" alt><br>千万别好奇选择了<code>MathML</code></p><h2>参考文献</h2><p>采用<strong>hexo-renderer-markdown-it</strong>插件</p><p>参见<a href="https://www.jianshu.com/p/588ab3d22eb8" target="_blank" rel="noopener">hexo-renderer-markdown-it 插件 详解</a></p><p>对应的maupassant主题css配置</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.footnote-item</span>&#123;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">15px</span>;</span><br><span class="line">    <span class="attribute">line-height</span>: <span class="number">1.77</span>; </span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#2097d2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.footnotes</span> <span class="selector-class">.footnotes-list</span> <span class="selector-class">.footnote-item</span> <span class="selector-tag">p</span>&#123;</span><br><span class="line">    <span class="attribute">margin</span>:<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="selector-class">.footnote-backref</span>&#123;</span><br><span class="line">    <span class="attribute">display</span>: none;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;折腾公式+参考文献&lt;/p&gt;
    
    </summary>
    
    
      <category term="Blog" scheme="https://zekizz.github.io/categories/Blog/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="深度学习" scheme="https://zekizz.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="graph embedding" scheme="https://zekizz.github.io/tags/graph-embedding/"/>
    
      <category term="半监督分类" scheme="https://zekizz.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%88%86%E7%B1%BB/"/>
    
      <category term="概率图模型" scheme="https://zekizz.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>图半监督节点分类之一——相关技术</title>
    <link href="https://zekizz.github.io/ML/graph-based-semi-supervised-classification-1/"/>
    <id>https://zekizz.github.io/ML/graph-based-semi-supervised-classification-1/</id>
    <published>2018-09-26T15:06:40.000Z</published>
    <updated>2019-10-13T09:20:48.305Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a><h1>问题描述</h1><p>在如今的大数据时代，社会的各个方面时时刻刻产生着大量的数据。很多时候，数据之间并不是独立的，而是建立了方方面面的联系。这种类型的数据被称作“关系型数据”。对于数据间的联系，图是一种有效的表达方式。一些数据间的关系结构，比如语音或文本中的序列结构和图片中的栅格结构，都可以视为一种特殊的图。</p><p>作为一个典型的例子，社交网络分析已经被用于朋友推荐、内容推荐和计算广告等具体业务。在基于图的机器学习中，每个样本作为图中的一个节点，样本之间的相似性将作为样本之间的关系反映到图中的边上，相似性越大，边的权重也就越大。</p><p>在一些情况下，可以通过节点活动产生的自然关系来构建图，例如社交网络中的关注和转发行为，以及引用网络中的引用关系。这种自然关系提供了节点特征中不存在的附加信息。利用这些信息，研究者可以更好地掌握数据背后的规律并提高模型的推理性能。但是在一些其他情况下，这些产生关系的行为缺失了或者不存在。 一个常用的方法是构建K近邻图（K-Nearest Neighbor Graph, K-NNG）<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>。暴力K近邻图构造方法的计算复杂度为$O(n^2)$，这对于大规模数据集来说并不实用。一些近似方法被提出来，在降低复杂度的同时保持高质量的图结构<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>。本文关注的是图已经显式给定的情况。</p><p>在一个典型的图结构数据集中，除了图的结构外，还有两个重要的方面：<strong>节点的特征</strong>和<strong>节点的标签</strong>。有监督学习方法需要在训练集（标记数据）上训练，然后在测试集上做推断。因此，模型的性能很大程度上受训练集大小的影响。当训练数据不够时，很容易出现过拟合问题，模型的泛化性能不能得到保障。 然而，实际中的样本并不都是有标记的，由于繁重的标注过程，实际上只有极少的样本被标记，大量未知标签的数据每时每刻都在生成。再者，因为网络结构的限制，无法随意地把未标记的样本扔出训练集，这样会照成网络的不完整。由于未标记的样本是很好获得的，在图上做半监督学习是一个很自然并且很重要的任务。</p><p>本文研究了基于图的半监督节点分类问题。下图给出了该问题的任务描述：对一个部分标注图中的未标记节点进行分类。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/task_2.png" alt="基于图的半监督节点分类的任务描述"><br>采用半监督学习方法，大量的未标记样本也被利用到模型训练过程中来。事实上，因为网络结构从某种程度上反映了节点之间的相似性，图上大量未标记的样本也能提供有助于分类的信息，比如上图中虚线框所示的社区结构信息，相同社区内的节点倾向于具备相同的类别。借助网络上的社区结构，需要的标记样本数量将会大大较少，分类结果将会更加准确。直接用节点特征训练分类器的传统方法，会受限于特征的质量和训练样本的数目。合理利用网络结构这一异构信息，在少量标记样本下做半监督学习，利用大量的未标记样本，使得模型获得更好的分类性能，是一个很有现实意义和迫切需求的研究工作。</p><p>可能的应用场景有：</p><ul><li><strong>数据标注</strong>：极大减少标注成本</li><li><strong>节点分类</strong>：用户画像、图像描述、异常点检测</li><li><strong>链接预测</strong>：推荐系统、网络重构</li><li><strong>社区发现</strong>：人群发现、社会网络分析、广告投放</li></ul><h1>符号定义</h1><p>本节首先给出关于半监督图节点分类问题的一些定义和符号表达。<br>图结构可以表述为$G = (V,E)$，其中$V$为图中节点的集合，$E$为图中边的集合。在实际中，相关方法更多地采用邻接矩阵$A \in \mathbb{R}^{n \times n}$来反映图的结构。在本文处理的问题中，除了图结构以外，图中节点还拥有特征$X$和标签$Y$这两类信息。更加具体地，本文处理的是部分标记的图。令$L$和$U$分别表示标记节点集合和未标记节点集合，并且$L \cup U = V$。对应地，$X^L$ 和 $Y^L$ 为标记节点的特征和标签。一个部分标记属性图的定义如下：</p><blockquote><p><strong>定义一： 部分标记属性图</strong><br>一个完整部分标记属性图被定义为$G_p = (X^L, X^U,  A, Y^L)$，图中节点拥有特征 $X = X^L \cup X^U$ ，部分标记的节点$V^L$还拥有类别标签信息 $Y^L$ 。</p></blockquote><p>基于定义一，给定一个部分标记属性图$G_p$，半监督节点分类的目的是通过学习一个预测函数$f$来推断未标记节点$V^U$的类别标签：<br>$$f : G_p = (X^L, X^U, A, Y^L) \rightarrow Y^U$$<br>值得注意的是，并不是所有方法都是基于这个完整的部分标记属性图$G_p$，在有些情况下，样本特征矩阵是缺失的。这个时候样本特征矩阵$X$就等同于邻接矩阵$A$，用连接关系这一上下文来代表样本的特征。</p><p>不难发现，图半监督方法的核心是通过邻接矩阵来利用未标记样本的信息。在图未知时，邻接矩阵$A$是通过计算节点特征间的相似性得到的相似性矩阵。本文关注的是图已经被显式给出的情况，此时边的权重$a_{ij}$反映了节点$v_i$和节点$v_j$之间的独立于样本特征之外的相似性（自然相关性），边权重越大，节点之间就越相似。不同方法对图结构的利用的一个很重要的不同之处在于利用多大范围的邻居信息，一般方法仅仅只利用直接相连的邻居的信息，这部分仅仅反映图的局部结构。由于实际中图的稀疏性，一阶邻居的信息可能并不能充分的代表节点的上下文，因此二阶邻居甚至更高阶的邻居被考虑进来，这部分信息反映了图的全局结构。本文首先给出一阶近似与二阶近似的定义。</p><blockquote><p><strong>定义二: 一阶近似</strong><br>一阶近似指的是网络中节点之间的局部成对相似性，但是仅限于通过边连接的节点对。对于每个节点对$(v_i, v_j)$，如果$(v_i, v_j) \in E$，节点$v_i$和节点$v_j$之间的一阶近似就是边的权重$a_{ij}$，其他情况下一阶近似为0。一阶近似捕获节点之间的直接相邻关系，也就是一阶相邻关系。</p><p><strong>定义三： 二阶近似</strong><br>如果令$N_u = {a_{u,1}, \cdots, a_{u,|V|}}$表示节点$v_u$与其他节点的一阶近似，那么节点$v_i$和节点$v_j$之间的二阶近似就是$N_i$和$N_j$之间的相似性。二阶近似捕获每对节点之间的二阶关系。一种简单的计算方式是统计两个节点的共同的邻居数目，另一种方式计算从节点$v_i$随机游走到$v_j$的转移概率。</p></blockquote><p>下图，给出了一阶近似与二阶近似对邻居利用范围的示意。图中红色节点是目标节点$v_i$，蓝色节点是其需要汇集信息的一阶和二阶近邻节点。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/first_second_order.png" alt="一阶近似与二阶近似利用邻居范围的示意图"></p><p>自然地，模型必须维持一阶近似，因为它意味着现实网络中的两个节点如果被观测到有边相连，那么它们总会在某些方面相似。例如，如果一篇论文引用了另一篇论文，它们应该存在一些共同的主题。直观上，二阶近似假设如果两个节点拥有很多共同的邻居，它们往往是相似的。这样的假设在许多领域已被证明是合理的。例如，在语言学中，如果单词总是被类似的语境所包围，那么它们将会是相似的<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>。如果两个人有许多共同的朋友，他们很有可能成为朋友<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>。二阶近似已经被证明是定义一对节点相似性的一个很好的度量，即使这对节点没有直接相连<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>，因此二阶近似可以高度丰富节点间的关系。</p><p>除了一阶近似和二阶近似外，还有更高阶的近似。结合二阶近似的定义，不难给出高阶近似的定义：</p><blockquote><p><strong>定义四：高阶近似</strong><br>与二阶近似相比，高阶近似的目的是捕获更大范围的全局结构（节点间的$k$阶关系），比如社团结构。对于节点对$(v_i, v_j)$，他们之间的高阶近似是通过计算两个节点间的$k$阶（$k\geq 3$）转移概率。</p></blockquote><p>为了保持一阶近似，目前的一些方法大多通过图拉普拉斯正则化来约束节点之间的特征或者类别分布。对于二阶近似和高阶近似，图编码方法（Graph Embedding）是最近比较流行的方法。需要强调的是，起初的图编码的目的是将单纯的图结构这一信息编码入节点的向量表达，编码过程只采用了图结构，并有利用节点本身的特征。节点带有特征的图叫做属性图，属性图是本文的工作关注的。在属性图中，图编码函数$f_{embed}$不仅仅是将图结构编码进节点的特征表达，节点原本的特征也被利用起来。</p><p>下面给出更一般的在属性图上的图编码的定义：</p><blockquote><p><strong>定义五：图编码</strong><br>给定一个属性图$G=(V,E,X)$，图编码的目的是学习一个映射函数：<br>$ f_{embed}: (G, v_i) \rightarrow g_i \in \mathbb{R}^{d}, \forall i \in [n] $。其中$d\ll |V|$，图编码 $ f_{embed} $ 将节点编码到一个低维特征空间并且保持了某些定义在图上的节点间的相似性，比如一阶近似、二阶近似和高阶近似。</p></blockquote><p>图编码的思想是使相似的节点在编码空间上邻近，从而将不同类型的节点区分开。由于其较好的节点特征表示学习性能，图编码方法已经被广泛应用于图结构数据的各个应用场景。</p><h1>图拉普拉斯正则</h1><p>在半监督图节点分类任务中，图中节点是部分标记的。基于图的半监督学习通过某种形式的的基于图的正则化将标签信息在图上进行平滑，因此，大多数图半监督学习方法的损失函数具有如下的形式：<br>$$ \mathcal{L} = \mathcal{L}_{label} + \lambda \mathcal{L}_{reg} $$</p><p>其中，$\mathcal{L}_{label} = \sum_{i \in V^L} l(Y_i, f(X_i))$ 表示采用损失函数$l$的有监督损失，$\mathcal{L}_{reg}$ 为基于图的正则项， $\lambda$是正则权重参数。这里的$f$就是上节公式中的预测函数，为了简练起见省略了部分标记属性图$G_P$ 。损失函数$l$就是常见的损失函数，比如平方损失、log损失、hinge损失和交叉熵等。不同图半监督学习方法在损失函数上的差异主要体现在预测函数$f$和图正则化的选取。基于图的正则化依赖的假设是：图中相连的节点倾向于拥有相同的类别标签。最常用的是图拉普拉斯正则，其具体表达如下式：</p><p>$$\mathcal{L}_{reg} = \sum_{i,j} A_{ij} \Vert f(X_i) - f(X_j) \Vert ^2 = f(X)^T \Delta f(X)$$</p><p>$\Delta = D - A$ 为图$G$的非标准化图拉普拉斯算子， 其中$A$为邻接矩阵（二值或加权矩阵），$ D_{ii} = \sum_j A_{ij}$ 是一个对角矩阵，反映节点的度信息，$X_i$为节点$v_i$的特征向量。早期的方法是非参数化的，比较著名的是标签传播算法（Label Propagation, LP）<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup><sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>。标签传播算法迫使$f$与有标记节点的类别$Y^L$一致，$f$是图中未标记节点的标签查找表，可以用封闭形式的解决方案获得， 并且采用上诉公式所示的图拉普拉斯正则，当具有较大边权重$A_{ij}$的节点对被预测为具有不同的标签$f(X_i) \neq f(X_j)$时，产生较大的惩罚。ManiReg<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>用支持向量机中的hinge损失来替换LP中的有监督损失。ICA<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>通过允许更一般的本地更新来推广标签传播算法。ICA使用以相邻节点的标签作为输入的本地分类器，并且在估计本地分类器和分配新标签之间采用迭代过程。</p><p>标签传播算法是在标签层面上利用网络结构的信息，最近比较流行的图编码方法首先在特征层面上利用图结构信息，学习得到节点的低维特征表示，使得网络相连的相似的节点在该特征空间上邻近。图编码方法主要可以分为神经网络方法和矩阵分解方法，但是其损失函数具备同样的形式。对于基于图的正则化，不同方法也有一些细微的不同，但是基于的假设不变，都是促使网络上相连的相似的节点在编码空间上邻近。此时，图拉普拉斯正则变为：</p><p>$$\mathcal{L}_{reg} = \sum_{i,j} A_{ij} \Vert g(X_i) - g(X_j) \Vert ^2$$</p><p>其中$g(\cdot)$为特征编码函数，结合网络结构将原始特征$X$编码到低维空间。与上文图拉普拉斯公式不同的是，图编码方法将正则约束作用在了编码后的特征$g(X_i)$上，而不是推理出的分类结果$f(X_i)$上。图编码的这种处理方式显然更合理，节点间有边相连，自然的反映了节点之间存在必然的某方面的相似性，但是这个相似性是否体现为类别标签一致就不得而知了。约束编码后的特征相似，并不意味着约束分类结果一致，特征相似但是也存在一些细微差别，这些差别可以使相连的两个节点分类为两个不同的类别。本文所提出的三种方法同样也都是基于该图编码方式。</p><h1>研究现状</h1><p>基于图的半监督学习是一种典型的半监督学习方法，其将整个数据集映射成一个图，图中的每个点代表一个样本，图中边的权重与样本之间的相似性成正比。 基于图的方法与一般机器学习方法不同的地方在于对图结构这一样本间关系的利用，不同方法从不同角度以不同的方式利用图结构、节点的类别标签和节点的特征，具体可以分为三大类：</p><ul><li><strong>标签传播方法</strong>：标签传播算法（Label Propagation, LP）<sup class="footnote-ref"><a href="#fn8" id="fnref8:1">[8]</a></sup>将有标记样本的标签信息在图上传播，并在邻近节点的标签上定义图拉普拉斯正则，该正则用于约束具有较大边权重的相邻节点拥有相同的类别。这类方法只使用网络结构和节点的标签。</li><li><strong>无监督图编码方法</strong>：无监督图编码方法利用网络结构和节点的特征来学习节点的特征表示，忽略了节点的标签。当学得编码后，标准有监督学习应用于这些编码特征以训练模型对节点进行分类。<ul><li>基于因式分解的方法</li><li>基于随机游走的方法</li><li>基于自动编码机的方法</li></ul></li><li><strong>半监督图编码方法</strong>： 半监督图编码方法大多采用神经网络模型来进行特征编码。这些方法在有标记样本上计算有监督损失，利用图拉普拉斯正则在所有样本上计算无监督损失。因此，模型训练过程中利用了网络结构、节点特征和节点标签三方面的信息。<ul><li>Planetoid模型：单输入双输入神经网络</li><li>SEANO模型：双输入双输出神经网络</li><li>图卷积神经网络GCN</li></ul></li></ul><p>图编码方法关注于利用邻居特征将节点编码到一个低维空间，学习节点的特征编码（feature embedding），然后基于学得的特征编码构建分类器。 用上下文训练得到的编码能够用来提高相关任务的性能。典型地，从语言模型训练的词编码可以应用于词性标注、情感分类和命名实体识别。 值得注意的是，并不是所有图编码方法都是半监督的。事实上，早期的图编码方法大多是无监督的，这些方法把编码任务和分类任务放在两个不同的模型中，先采用无监督编码学习到节点特征表示，再基于学习到的特征训练分类器。这种流水线的方式使得图编码方法广泛应用于图分析的各类任务，比如节点分类<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>、链接预测<sup class="footnote-ref"><a href="#fn6" id="fnref6:1">[6]</a></sup>、社团检测<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>、推荐系统<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>和可视化<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>。但是这种方式在训练过程中缺乏有监督信息的调整，编码得到的特征不是针对类别标签的，可能使不同类别的节点在特征空间上还是难以分开。近期的一些图编码方法，将编码和分类这两个任务合并在一个统一的端到端模型中，提出半监督图编码方法。实验结果表明，在图节点分类任务上，半监督图编码只需要极少比例的标记样本就能取得不错的效果。接下来具体对这两类图编码方法进行介绍。</p><h2>无监督图编码</h2><p>根据编码过程利用的信息，无监督图编码又可以分为两类：一类只利用网络结构$A$，另一类在利用网络结构$A$的同时，也考虑节点本身特征属性$X$的影响。早期图编码方法大多数为第一类，其目的是将图结构进行编码，使得编码后的特征保持原图的连接关系，即连接越紧密的节点和局部结构越相似的节点在编码后的特征空间上越邻近。下图给出了一个简单的示意。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/network_embedding.png" alt="一个保持社团结构的图编码的例子"><br>图中不同结构的节点被编码到二维空间上的不同位置。节点4、5和6有相同的图结构，因此被编码到相同的位置。节点1和节点3的局部图结构也相同，但是其分别属于两个社团，因此编码结果分别分布在节点2的两侧，并各自与同社团中的节点邻近（比如节点1和节点4）。如果按社团对节点进行分类，平行于横轴画两条直线就可以将节点分为3类；如果按局部结构对节点进行分类，平行于纵轴画两条线也可以很好的将节点分开。因此，图编码可以学习到效果很好的节点特征，得到的特征也更有解释性，因此广泛用于可视化等应用。接下来，本文分别介绍这两类无监督图编码方法的一些典型工作。</p><p>基于因式分解的方法将节点间的连接关系表述为矩阵的形式，并且通过分解该矩阵来获得编码。用来表达连接关系的矩阵可以是邻接矩阵、拉普拉斯矩阵、节点转移概率矩阵和Katz相似性矩阵等。不同的分解该表达矩阵的方法根据矩阵性质的不同而不同。如果获得的矩阵是半正定的，例如拉普拉斯矩阵，就可以使用特征值分解。对于非结构矩阵，可以使用梯度下降法在线性时间复杂度上得到编码结果。<br>局部线性编码（Locally Linear Embedding, LLE）假设在特征空间上每个节点的特征是其邻居特征的线性加和。如果采用邻接矩阵$A$表达图中节点间的连接权重，并且采用$H_i$来表达节点$v_i$的编码，那么该假设可以表述为：</p><p>$$H_i \approx \sum_j A_{ij}H_j \hspace{10pt} \forall i \in V$$</p><p>因此，为了得到编码$H^{N \times d}$，可以最小化下式：</p><p>$$\phi (H) = \sum_i  |H_i - \sum_j A_{ij}H_j |^2$$</p><p>为了去除退化解，编码的方差被限制为$\frac{1}{N} H^T H = I$。为了进一步消除平移不变性，编码被中心化：$\sum_i H_i = 0$ 。 上述约束优化问题可以归结为一个特征值问题，其解是取稀疏矩阵$(I-A)^T(I-A)$底部的$d+1$个特征向量，并丢弃对应于最小特征值的特征向量。<br>不同于LLE的假设，拉普拉斯特征映射（Laplacian Eigenmaps）的目的是在边权$A_{ij}$较高时保持这两个节点在编码上紧密。此时，其最小化的目标函数为 ：<br>$$ \begin{aligned}<br>\phi (H) &amp; =  \frac{1}{2} \sum_{i,j}(H_i - H_j)^2 A_{ij} \<br> &amp; =  H^T L H<br>\end{aligned} $$<br>其中$L$为图的拉普拉斯矩阵，通过取归一化拉普拉斯$L_{norm} = D^{-1/2} L D^{-1/2}$ 的最小的d个特征根对应的特征向量即可得到问题的解。</p><p>因式分解（Graph Factorization, GF）提出另一种损失函数，约束两个节点编码结果的距离与连接权重一致：<br>$$ \phi(H, \lambda) = \frac{1}{2} \sum_{i,j}(A_{ij} - &lt;H_i, H_j&gt;)^2 +\frac{\lambda}{2} \sum_i \Vert H_i \Vert ^2 $$<br>其中$\lambda$为正则化系数。<br>GraRep<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>将节点间转移概率定义为$T = D^{-1}A$，并引入$k$阶概率转移矩阵<br>$$<br>A^k = \underbrace{A \cdots A}_{k}<br>$$<br>为了减少误差和方便计算，GraRep通过 $A^k$ 计算得到正值 $k$ 阶对数概率矩阵 $X^k$ ，并采用奇异值分解（SVD）来求解目标函数 $ \Vert X^k - H^k_i H^k_j \Vert^2_F $ ，得到$k$阶特征编码。最后将所有$H^k_i$合并得到节点$v_i$的最终编码$H_i$。<br>HOPE<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>采用了相似的目标函数来保持高阶近似，$ \Vert S - H_i H_j\Vert^2_F $。 $S$为相似度矩阵，通过对其的计算来保持高阶近似。HOPE采用了一些流行的近似度度量方法进行实验。它发现图中的许多高阶邻近度量可以反映不对称传递性，此外，它们大多共享一个通用公式，这将有助于近似这些近似值：$S = M_g^{-1} \cdot M_l$，其中$M_g$和$ M_l$都是稀疏矩阵。这使得HOPE能够利用广义奇异值分解来有效的求解并得到编码结果。</p><p>受word2vec<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>成功的影响，目前越来越多的研究关注于利用类似skip-gram的模型来做图编码。Skip-gram是一种softmax的变种,给定一个样本和其上下文，skip-gram的目标通常是最小化预测上下文的log损失，其中预测上下文利用的是样本的编码作为输入。具体地，设${(i,c)}$是样本$i$和其上下文$c$的组合对的集合，log损失函数能写成：</p><p>$$ \sum_{(i,c)} \log p(c|i) = - \sum_{(i,c)} ( w^T_c e_i - \log \sum_{c’ \in \mathcal{C}} \exp (w^T_{c’} e_i)) $$</p><p>其中，$\mathcal{C}$是所有可能上下文的集合，$w$是模型参数，$e_i$是样本$i$的编码。Skip-gram模型最开始用于学习词的向量表示，也就是word2vec。在word2vec中，每个训练对$(i,c)$，表示此时要去估计样本$i$的编码，上下文 $c$ 是词$i$在序列中给定窗口内的词，上下文集合空间$\mathcal{C}$为语料库中所有的词。目前许多基于随机游走的方法采用skip-gram模型来学习图节点编码。DeepWalk<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>使用节点的编码来预测其在图中的上下文，其上下文是通过随机游走生成的。对于每个训练对$(i,c)$，样本$i$为当前节点，$c$为样本$i$在一定窗口大小的邻居中采用随机游走采样生成的序列，也就是说c的长度是固定的，为窗口的大小。LINE<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>扩展了DeepWalk，使其具备多个上下文空间$\mathcal{C}$，用于建模一阶和二阶近似。Node2vec<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup>采用带偏置的随机游走，在宽度优先（BFS）和深度优先（DFS）搜索之间提供权衡，因此可生成比DeepWalk更高质量和更丰富信息的编码。选择合适的平衡可以使node2vec保留社区结构以及节点之间的结构对等性。</p><p>虽然类似skip-gram的模型最近获取了很大的关注，其他神经网络上的方法同样也被扩展到图的编码中来，比如自动编码机（Autoencoder）。自动编码机是一种典型的无监督神经网络，主要用于高维数据降维和特征学习，已经在各种各样的实际问题中得到了广泛的应用。SDNE（Structural Deep Network Embedding）<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup>采用自动编码机来对网络中节点进行编码。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/SDNE.png" alt="SDNE模型框架"><br>在SDNE中，$x_i = A_i$为邻接矩阵$A$的第$i$行，反映节点$v_i$的网络结构向量。通过在$x_i$上施行自动编码机，使得学到的编码$h_i = y_i^{(K)}$能够保持高阶依赖。因为自动编码机约束输出$\widehat{x_i}$与输入$x_i$一致，这使得编码$h_i$能够推理节点的上下文（网络连接关系），从而保持高阶依赖。不难看出，SDNE在一般自动编码机的基础上，加上了网络结构的约束。这里采用的约束是之前介绍的拉普拉斯特征映射（Laplacian Eigenmaps），约束网络上相近的节点有相近的编码结果。因此，SDNE能同时保持一阶近似和高阶近似，其损失函数为：<br>$$<br>\mathcal{L}_{mix}  =  \mathcal{L}_{2nd} + \alpha \mathcal{L}_{1st} + \nu \mathcal{L}_{reg}  = \Vert (\widehat{X} - X) \odot B \Vert_F^2 + \alpha \sum_{i,j}^n A_{ij}\Vert h_i - h_j \Vert_2^2 + \nu \mathcal{L}_{reg}  $$</p><p>其中$\mathcal{L}_{2nd}$对原始的自动编码机的损失做了一定修改，引入惩罚项$B$, 对非零元素的重构误差施加了比零元素更多的惩罚，使得模型更容易重构$X$中的1而不是0。对于$B$，SDNE的定义为：如果 $A_{ij} = 0$，$b_{ij} = 1$，否则$b_{ij} = \beta &gt; 1$。设计B的出发点是，有边相连表示两点相似，但是没有边相连，不一定代表其不相似。这样一来就能重构高阶邻居的结构信息。 与SDNE直接在邻接矩阵上施行自动编码机不同的是，DNGR<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup>结合了随机游走和自动编码机来完成图编码。DNGR首先利用随机游走方法生成带有高阶图结构信息的表达矩阵，然后计算PPMI（positive pointwise mutual information，一种词向量表达方式）矩阵，最后在PPMI矩阵的基础上利用自动编码机替换SVD得到最终的编码结果。</p><p>以上第一类无监督图编码方法在编码过程中没有用到节点的特征属性，只编码了网络结构。然而，在实际中，网络中节点往往拥有丰富的信息，比如社交网络中用户发布的微博内容。节点的特征属性更准确的反映了节点的信息，这对于节点的分类任务尤为重要。TADW<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup>认识到这一点，同时将网络结构和节点特征进行编码。TADW证明了DeepWalk本质上等同于矩阵分解，并在矩阵分解的框架下将节点的文本特征合并到网络表示学习中。</p><p>以上无监督图编码方法固然有其普适性等优点，但在图节点分类任务上，它们并没有将节点的标签信息用作编码，而且分类器的训练是在编码的学习之后。所以编码的结果并不是针对该特定的任务的，这也导致该类方法在图节点分类任务上并未达到最先进的结果。</p><h2>半监督图编码</h2><p>很多应用被建模和分析为属性图，其中节点表示具备特征的实体，边表示实体之间的交互或关系。在许多情况下，人们还知道属性图中某些节点的标签。这种网络提供了更多的信息，但分析起来也更具挑战性。半监督图编码方法就是在这种情况下进行图编码与节点分类任务。此类方法与前一小节的无监督图编码方法最大的不同有两点：（1）通过集成异构信息（包括图结构、节点特征和部分可用标签）来学习图编码；（2）将图编码和分类这两个过程构建在一个统一的模型中，统一训练，相互补充与调整。</p><p>Planetoid<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup>为图中每个节点训练一个特征编码来联合预测图中的类别标签和邻域上下文。具体地，Planetoid提出了直推（transductive）和归纳（inductive）两种形式的半监督模型，前者假定学习过程中所考虑的未标记样本是待预测数据（图中已观测到的节点，基于“封闭世界”），学习的目的就是在这些未标记样本上获得最优泛化性能，而后者假定训练数据中的未标记样本并非仅仅是待预测数据（包括未观测的节点，基于“开放世界”）。下图给出了这两种方式的网络结构。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/planetoid_ab.png" alt="Planetoid的模型结构：直推 v.s. 归纳"><br>图中直线箭头表示直接相连，虚线箭头表示一个或者多个前馈层。在直推方式中，类别标签由学习的编码和输入的特征向量同时决定。但是在归纳方式中，未标记样本的编码并不能提前学习到，只有在图中观测到的节点才能学习到，因此归纳方式对直推方式的结构做了点修改，将编码定义为特征向量的的参数化函数并且类别标签只依赖于特征向量，这样便可以对训练中未观测到的样本做出预测。</p><p>SEANO<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup>拥有相似的模型结构，不同的是SEANO设计了一个双输入双输出深度神经网络来归纳学习节点编码。其网络结构见下图。<br><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/SEANO.png" alt="SEANO网络结构"><br>在这里，输入不仅仅是当前节点的特征，还有该节点邻居的特征的加权求和。他们通过相同的一组非线性映射函数，并通过加权求和在编码中进行聚合。  图中的左输出层预测输入节点的类标签，而右输出层产生网络输入的上下文，两者通过共享编码层使得得到的编码能捕获节点特征、网络结构和类别标签三个方面的信息。但是这里有两个输出层，需要分步进行调整，对模型的训练造成了一定不便。</p><p>借鉴卷积神经网络的思想，将节点周围的一阶邻居看成其感受野，GCN<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup>提出了一种直接在图上操作的卷积神经网络的有效变体。这是一种简单有效的基于神经网络的分层传播规则。GCN的核心是其卷积方式，具体的卷积过程为：<br>$$ H^{l+1} = \sigma (\widehat{A} H^{l} W^{l}) $$</p><p>其中$W^{l}$ 为第$l$层的模型参数，$H^{l}$为第$l$层的输出，$\widehat{A}$的为采用了再归一化技巧后的图拉普拉斯矩阵。具体地，$ \widehat{A} = \widetilde{D}^{ -\frac{1}{2} } \widetilde{A} \widetilde{D}^{ -\frac{1}{2} }$， 其中，$\widetilde{A} = A + I_N $，$\widetilde{D}_{ii} = \sum_j   \widetilde{A}_{ij}$。 该卷积将每个点的特征变换成了自己和邻居的特征的加权求和，即$\widehat{A}X$。由于$\widehat{A}$中所有元素的值都小于1，$\widehat{A} X$ 的意义就是在特征空间上节点向其邻居所在的方向加权移动。理想情况下，如果一个节点特征不好分，但是其邻居都远离分界面，那么该点向邻居的移动会使其远离分界面，变得可分。所以$\widehat{A} X$ 使得节点的特征与邻居的特征的差异性一定程度减小，也就是做了局部平滑。基于上式，GCN提出了一个两层的半监督神经网络模型：</p><p>$$ Z = f(X,A) = \text{softmax}(\widehat{A}\hspace{2pt} \text{ReLU}(\widehat{A}XW^{(0)})\hspace{2pt} W^{(1)})  $$</p><p>第一层用来做编码，第二层用来做分类。相比于SEANO，GCN更加简单有效而且同时利用了部分标记的标签和网络结构。<br>值得注意的是，GCN只需要少量的标记样本就能在整个网络上取得很好的分类结果。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>MILLER G L, TENG S H, THURSTON W, et al., 1997. Separators for sphere-packings and nearest neighbor graphs[J]. Journal of the ACM (JACM), 44(1): 1–29. <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>DONG W, MOSES C, LI K, 2011. Efficient k-nearest neighbor graph construction for generic similarity measures[C]//Proceedings of the 20th international conference on World wide web. ACM: 577–586. <a href="#fnref2" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>ZHANG Y M, HUANG K, GENG G, et al., 2013. Fast knn graph construction with locality sensitive hashing[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer: 660–674. <a href="#fnref3" class="footnote-backref">↩</a></p></li><li id="fn4" class="footnote-item"><p>DASH N S, 2008. Context and contextual word meaning.[J]. SKASE Journal of Theoretical Linguistics. <a href="#fnref4" class="footnote-backref">↩</a></p></li><li id="fn5" class="footnote-item"><p>JIN E M, GIRVAN M, NEWMAN M E, 2001. Structure of growing social networks[J]. Physical review E, 64(4): 046132. <a href="#fnref5" class="footnote-backref">↩</a></p></li><li id="fn6" class="footnote-item"><p>LIBEN-NOWELL D, KLEINBERG J, 2007. The link-prediction problem for social networks[J]. journal of the Association for Information Science and Technology, 58(7): 1019–1031. <a href="#fnref6" class="footnote-backref">↩</a> <a href="#fnref6:1" class="footnote-backref">↩</a></p></li><li id="fn7" class="footnote-item"><p>ZHU X, GHAHRAMANI Z, 2002. Learning from labeled and unlabeled data with label propagation[Z]. Citeseer <a href="#fnref7" class="footnote-backref">↩</a></p></li><li id="fn8" class="footnote-item"><p>ZHU X, GHAHRAMANI Z, LAFFERTY J D, 2003. Semi-supervised learning using gaussian fields and harmonic functions[C]//Proceedings of the 20th International conference on Machine learning (ICML-03). 912–919. <a href="#fnref8" class="footnote-backref">↩</a> <a href="#fnref8:1" class="footnote-backref">↩</a></p></li><li id="fn9" class="footnote-item"><p>BELKIN M, NIYOGI P, SINDHWANI V, 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples[J]. Journal of machine learning research, 7(Nov): 2399–2434. <a href="#fnref9" class="footnote-backref">↩</a></p></li><li id="fn10" class="footnote-item"><p>LU Q, GETOOR L, 2003. Link-based classification[C]//Proceedings of the 20th International Conference on Machine Learning (ICML-03). 496–503. <a href="#fnref10" class="footnote-backref">↩</a></p></li><li id="fn11" class="footnote-item"><p>BHAGAT S, CORMODE G, MUTHUKRISHNAN S, 2011. Node classification in social networks[M]//Social network data analytics.  Springer: 115–148. <a href="#fnref11" class="footnote-backref">↩</a></p></li><li id="fn12" class="footnote-item"><p>CAVALLARI S, ZHENG V W, CAI H, et al., 2017. Learning community embedding with community detection and node embedding on graphs[C]//Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM: 377–386. <a href="#fnref12" class="footnote-backref">↩</a></p></li><li id="fn13" class="footnote-item"><p>YU X, REN X, SUN Y, et al., 2014. Personalized entity recommendation: A heterogeneous information network approach[C]//Proceedings of the 7th ACM international conference on Web search and data mining. ACM: 283–292. <a href="#fnref13" class="footnote-backref">↩</a></p></li><li id="fn14" class="footnote-item"><p>MAATEN L V D, HINTON G, 2008. Visualizing data using t-sne[J]. Journal of machine learning research, 9(Nov): 2579–2605. <a href="#fnref14" class="footnote-backref">↩</a></p></li><li id="fn15" class="footnote-item"><p>CAO S, LU W, XU Q, 2015. Grarep: Learning graph representations with global structural information[C]//Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM: 891–900. <a href="#fnref15" class="footnote-backref">↩</a></p></li><li id="fn16" class="footnote-item"><p>OU M, CUI P, PEI J, et al., 2016. Asymmetric transitivity preserving graph embedding[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM: 1105–1114. <a href="#fnref16" class="footnote-backref">↩</a></p></li><li id="fn17" class="footnote-item"><p>MIKOLOV T, SUTSKEVER I, CHEN K, et al., 2013. Distributed representations of words and phrases and their compositionality[J]. neural information processing systems: 3111–3119. <a href="#fnref17" class="footnote-backref">↩</a></p></li><li id="fn18" class="footnote-item"><p>PEROZZI B, AL-RFOU R, SKIENA S, 2014. Deepwalk: Online learning of social representations [C]//Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM: 701–710. <a href="#fnref18" class="footnote-backref">↩</a></p></li><li id="fn19" class="footnote-item"><p>TANG J, QU M, WANG M, et al., 2015. Line: Large-scale information network embedding[C]//Proceedings of the 24th International Conference on World Wide Web. [S.l.]: International World Wide Web Conferences Steering Committee: 1067–1077. <a href="#fnref19" class="footnote-backref">↩</a></p></li><li id="fn20" class="footnote-item"><p>GROVER A, LESKOVEC J, 2016. node2vec: Scalable feature learning for networks[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining.  ACM: 855–864. <a href="#fnref20" class="footnote-backref">↩</a></p></li><li id="fn21" class="footnote-item"><p>WANG D, CUI P, ZHU W, 2016. Structural deep network embedding[C]//Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM: 1225–1234. <a href="#fnref21" class="footnote-backref">↩</a></p></li><li id="fn22" class="footnote-item"><p>CAO S, LU W, XU Q, 2016. Deep neural networks for learning graph representations.[C]//AAAI. 1145–1152. <a href="#fnref22" class="footnote-backref">↩</a></p></li><li id="fn23" class="footnote-item"><p>YANG C, LIU Z, ZHAO D, et al., 2015. Network representation learning with rich text information. [C]//IJCAI. 2111–2117. <a href="#fnref23" class="footnote-backref">↩</a></p></li><li id="fn24" class="footnote-item"><p>YANG Z, COHEN W W, SALAKHUTDINOV R, 2016a. Revisiting semi-supervised learning with graph embeddings[J]. arXiv preprint arXiv:1603.08861. <a href="#fnref24" class="footnote-backref">↩</a></p></li><li id="fn25" class="footnote-item"><p>LIANG J, JACOBS P, PARTHASARATHY S, 2017. Seano: semi-supervised embedding in attributed networks with outliers[J]. arXiv preprint arXiv:1703.08100. <a href="#fnref25" class="footnote-backref">↩</a></p></li><li id="fn26" class="footnote-item"><p>KIPF T N, WELLING M, 2016. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907. <a href="#fnref26" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1&gt;问题描述&lt;/h1&gt;
&lt;p&gt;在如今的大数
      
    
    </summary>
    
    
      <category term="ML" scheme="https://zekizz.github.io/categories/ML/"/>
    
    
      <category term="machine learning" scheme="https://zekizz.github.io/tags/machine-learning/"/>
    
      <category term="深度学习" scheme="https://zekizz.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="graph embedding" scheme="https://zekizz.github.io/tags/graph-embedding/"/>
    
      <category term="半监督分类" scheme="https://zekizz.github.io/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%88%86%E7%B1%BB/"/>
    
      <category term="概率图模型" scheme="https://zekizz.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>hexo搭建github个人博客简明教程</title>
    <link href="https://zekizz.github.io/Blog/install-hexo-blog/"/>
    <id>https://zekizz.github.io/Blog/install-hexo-blog/</id>
    <published>2018-09-21T13:35:31.000Z</published>
    <updated>2019-10-13T09:20:48.306Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>从零开始搭建github个人博客，基于window系统（linux同理），适合大众。<br>整体上，在Github Pages的基础上使用hexo框架来搭建博客，代码存本地，上传github,方便迁移。</p><p>搭建好后，使用Markdown来书写博客，如果不懂Markdown，没关系，很好入门。</p><h1>安装git</h1><p>去<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">git官网</a>下载对应的版本，点击安装就好。</p><p>安装完成后，在cmd上输入 <code>git version</code> 查看是否安装成功，成功会输出对应的git版本。</p><p>之后使用的时候，只需要在需要操作的文件目录下点击鼠标右键，菜单栏中就会出现<code>Git GUI here</code> 和 <code>Git Bash Here</code>，一般我们选择 <code>Git Bash Here</code>来进行命令行操作。 <strong>后续的命令行操作都是在 <code>Git Bash</code>中进行了，防止出错。</strong></p><h1>安装node.js</h1><p>去<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">node官网</a> 下载对应的版本，win最好选64位的msi, 双击安装就OK了。</p><h1>创建Github仓库</h1><ol><li>首先需要一个github账号，邮箱注册并验证；</li><li>创建Repositories，登入个人账户后，有个绿色的<code>new repository</code>按钮，点击创建创库，或者输入：<a href="https://github.com/new" target="_blank" rel="noopener">https://github.com/new</a><br>注意： 创建的仓库名为：<a href="http://xn--github-on9im33ani7aou3bged.github.io" target="_blank" rel="noopener">你的github用户名.github.io</a>，<a href="http://xn--zekizz-hh4kn97ctwu1ny.github.io" target="_blank" rel="noopener">比如我的zekizz.github.io</a></li><li>仓库创建成功不会立即生效，需要过一段时间，创建成功后默认会在你这个仓库里生成一些示例页面，以后你的网站所有代码都是放在这个仓库里啦。</li></ol><h1>配置SSH key</h1><p>配置的目的是方便提交代码，否则每次都需要输入用户名和密码很麻烦。</p><p>首先在任意位置右键打开<code>Git Bash</code>, 输入</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> ~<span class="string">/.ssh</span> <span class="comment">#检查本机是否已存在的ssh密钥</span></span><br></pre></td></tr></table></figure><p>如果提示：<code>No such file or directory</code>，说明是第一次使用git，如果是，生成key</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -<span class="built_in">t</span> rsa -C <span class="string">"你的github注册邮件地址"</span></span><br></pre></td></tr></table></figure><p>一路回车，最终会在 <code>C:\Users\你的用户名\</code>文件夹下生成密钥文件。</p><p>用文本编辑器（或记事本）打开<code>.ssh\id_rsa.pub</code>, 复制里面的内容</p><p>打开你的github主页，进入 <code>setting -&gt; SSH and GPG keys -&gt; New SSH key</code></p><p>将刚才复制的内容粘贴到Key中，title随便填，保存。</p><p>测试是否成功</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ssh</span> -T git<span class="variable">@github</span>.com <span class="comment"># 注意邮箱地址不用改</span></span><br></pre></td></tr></table></figure><p>如果提示<code>Are you sure you want to continue connecting (yes/no)?</code>，输入yes，然后会看到</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi XXX! You<span class="symbol">'ve</span> successfully authenticated, but GitHub does <span class="keyword">not</span> provide shell <span class="keyword">access</span>.</span><br></pre></td></tr></table></figure><p>说明SSH配置成功</p><p>最后配置git用户信息</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user<span class="selector-class">.name</span> <span class="string">"username"</span><span class="comment">// 你的github用户名，非昵称</span></span><br><span class="line"></span><br><span class="line">git config --global user<span class="selector-class">.email</span>  <span class="string">"email"</span><span class="comment">// 填写你的github注册邮箱</span></span><br></pre></td></tr></table></figure><h1>安装hexo</h1><p>安装只需要一条命令，在git bash中输入</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install</span> -g hexo</span><br></pre></td></tr></table></figure><p>下一步是创建博客项目和初始化</p><p>首先，在你喜欢的地方创建一个文件夹，用于存放博客项目代码，比如 <code>E:\git_code\blog\Hexo</code>，路径上最好不要有中文吧</p><p>然后在该目录下，右键打开<code>git bash</code>, 输入</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo init</span></span><br></pre></td></tr></table></figure><p>hexo会自动下载一些文件到这个目录，我们首先不管内容</p><p>继续在<code>git bash</code>中输入</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo</span> g <span class="comment"># 生成</span></span><br><span class="line"></span><br><span class="line">hexo s <span class="comment"># 启动服务</span></span><br></pre></td></tr></table></figure><p><code>hexo s</code>是开启本地预览服务，于是可以打开浏览器，访问 <a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a>，预览生成的博客。</p><p>然后…</p><p>你就会看到一个巨丑的博客页面！！！</p><p>这个页面是hexo默认生成的，主题很丑，所以我们需要修改主题。</p><p>Hexo提供了一系列主题，进入<a href="https://hexo.io/themes/" target="_blank" rel="noopener">主题专栏</a>，挑选主题。</p><p>挑好主题后，进入主题的github页，需要克隆主题项目，比如主题<a href="https://github.com/Haojen/hexo-theme-Anisina" target="_blank" rel="noopener">Anisina</a></p><p>进入Hexo文件夹下的themes目录，右键打开<code>git bash</code>，输入</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/Haojen/hexo-theme-Anisina.git(此处地址替换成你需要使用的主题的地址)</span><br></pre></td></tr></table></figure><p><strong>注意！！！</strong><br>这里clone项目后，需要修改下文件夹的名字，比如目前的文件夹为<code>hexo-theme-Anisina</code>,需要修改为<code>Anisina</code>,因为主题名为<code>Anisina</code>，防止之后设置主题的时候出错或无效。</p><p>然后修改hexo目录下的<code>_config.yml</code>，将<code>theme: landscape</code>修改为<code>theme: Anisina</code>，然后重新<code>hexo g</code>来生成新的文件。</p><p>如果出现一些莫名其妙的问题，可以先执行<code>hexo clean</code>来清理一下public的内容，然后再来重新生成和发布。</p><p>其次，按照主题的github Readme来设置主题的一部分参数，这里暂不做描述，自行按照喜好配置，配置的文件为<code>themes</code>目录下，主题文件夹下的<code>_config.yml</code>。<br>需要特别注意的地方是，冒号后面必须有一个空格，否则可能会出问题。</p><p>这里唯一需要注意的是关于留言页的设置，我倾向于配置为<code>gitalk</code>或者<code>gitment</code>，这个要看主题是否支持，这里放在之后叙述。</p><h1>上传github</h1><p>首先配置hexo目录下的<code>_config.yml</code>，<br>修改<code>deploy</code>部分</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">deploy</span>:</span><br><span class="line">  <span class="attribute">type</span>: git</span><br><span class="line">  <span class="attribute">repository</span>: git<span class="variable">@github</span>.<span class="attribute">com</span>:zekizz/zekizz.github.io.git</span><br><span class="line">  <span class="attribute">branch</span>: master</span><br></pre></td></tr></table></figure><p><strong>这里替换成你自己的</strong>，在clone or download那里</p><p>其次，安装一个插件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install</span> hexo-deployer-git <span class="comment">--save</span></span><br></pre></td></tr></table></figure><p>如果前面都OK了，在hexo目录下，打开<code>git bash</code>，然后输入</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo d</span></span><br></pre></td></tr></table></figure><p>就OK了。</p><p>一般上传可以采用组合键</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo</span> d -g <span class="comment">#生成并上传</span></span><br></pre></td></tr></table></figure><p>这里给出常用hexo命令</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="string">"postName"</span> <span class="meta">#新建文章</span></span><br><span class="line">hexo <span class="keyword">new</span> page <span class="string">"pageName"</span> <span class="meta">#新建页面</span></span><br><span class="line">hexo generate <span class="meta">#生成静态页面至public目录</span></span><br><span class="line">hexo <span class="keyword">server</span> <span class="meta">#开启预览访问端口（默认端口4000，'ctrl + c'关闭server）</span></span><br><span class="line">hexo deploy <span class="meta">#部署到GitHub</span></span><br><span class="line">hexo help  <span class="meta"># 查看帮助</span></span><br><span class="line">hexo version  <span class="meta">#查看Hexo的版本</span></span><br></pre></td></tr></table></figure><p>对应的简写</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo n == hexo new</span><br><span class="line">hexo g == hexo generate</span><br><span class="line">hexo s == hexo server</span><br><span class="line">hexo d == hexo deploy</span><br></pre></td></tr></table></figure><h1>写新博客并上传</h1><p>首先定位到hexo根目录，打开<code>git bash</code></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new '<span class="keyword">my</span>-<span class="keyword">first</span>-blog' <span class="comment"># 替换为你的取得博客文件名</span></span><br></pre></td></tr></table></figure><p>然后就会在<code>source\_posts</code>文件夹下生成<code>my-first-blog.md</code>文件，打开该文件进行书写。</p><p>这里书写采用的是markdown，不熟的，这里给出两个教程<br><a href="https://sspai.com/post/25137" target="_blank" rel="noopener">认识与入门 Markdown</a><br><a href="https://www.zhihu.com/question/20409634" target="_blank" rel="noopener">怎样引导新手使用 Markdown？</a></p><p>书写markdown的工具<br>win: MarkdownPad<br>mac: Mou(目前最新mac版本好像不支持)、MacDown</p><p>写完后，组合键</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo</span> d -g <span class="comment">#生成并上传</span></span><br></pre></td></tr></table></figure><p>完成上传，或者</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo clean</span></span><br><span class="line"><span class="attribute">hexo g</span></span><br><span class="line"><span class="attribute">hexo g</span></span><br><span class="line"><span class="attribute">hexo d</span></span><br></pre></td></tr></table></figure><h1>博客内容</h1><p>最前面的页面设置，举一个栗子</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">your</span> <span class="string">title</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-09</span><span class="bullet">-17</span> <span class="number">22</span><span class="string">:50:10</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">machine</span> <span class="string">learning</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">blog</span> </span><br><span class="line"><span class="attr">mathjax2:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">ML</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="string">line</span> <span class="number">1</span> <span class="string">xxx</span></span><br><span class="line"><span class="string">line</span> <span class="number">2</span> <span class="string">xxx</span></span><br><span class="line"></span><br><span class="line"><span class="string">&lt;!--more--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="string">line</span> <span class="number">3</span> <span class="string">xxx</span></span><br></pre></td></tr></table></figure><p>这里<code>&lt;!--more--&gt;</code>是分割摘要的，<code>line 1 xxx</code>和<code>line 2 xxx</code>将作为摘要显示在主页中。</p><h1>配置留言功能</h1><p>我使用的主题是<a href="https://github.com/tufu9441/maupassant-hexo" target="_blank" rel="noopener">maupassant</a>，主题中已经支持了多种留言方式，只需要对应配置而已，这里推荐使用gitalk。<br>如果主题未支持，参考：</p><ul><li><a href="https://github.com/gitalk/gitalk/wiki/%E5%9C%A8hexo%E4%B8%AD%E4%BD%BF%E7%94%A8gitalk" target="_blank" rel="noopener">在hexo中使用gitalk</a></li><li><a href="https://zhoushuo.me/blog/2018/03/22/hexo-gitalk/" target="_blank" rel="noopener">为博客添加Gitalk评论插件</a></li><li><a href="https://asdfv1929.github.io/2018/01/20/gitalk/" target="_blank" rel="noopener">Hexo NexT主题中集成gitalk评论系统</a></li></ul><p>如果像主题<strong>maupassant</strong>这样已经提供了如下的gitalk功能支持，只需要简单配置一下。如果gitalk功能自己已配置好，后续的配置也同理</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">disqus:</span> <span class="comment">## Your disqus_shortname, e.g. username</span></span><br><span class="line"><span class="attr">uyan:</span> <span class="comment">## Your uyan_id. e.g. 1234567</span></span><br><span class="line"><span class="attr">livere:</span> <span class="comment">## Your livere data-uid, e.g. MTAyMC8zMDAxOC78NTgz</span></span><br><span class="line"><span class="attr">changyan:</span> <span class="comment">## Your changyan appid, e.g. cyrALsXc8</span></span><br><span class="line"><span class="attr">changyan_conf:</span> <span class="comment">## Your changyan conf, e.g. prod_d8a508c2825ab57eeb43e7c69bba0e8b</span></span><br><span class="line"><span class="attr">gitment:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span> <span class="comment">## If you want to use Gitment comment system please set the value to true.</span></span><br><span class="line"><span class="attr">  owner:</span> <span class="comment">## Your GitHub ID, e.g. username</span></span><br><span class="line"><span class="attr">  repo:</span> <span class="comment">## The repository to store your comments, make sure you're the repo's owner, e.g. imsun.github.io</span></span><br><span class="line"><span class="attr">  client_id:</span> <span class="comment">## GitHub client ID, e.g. 75752dafe7907a897619</span></span><br><span class="line"><span class="attr">  client_secret:</span> <span class="comment">## GitHub client secret, e.g. ec2fb9054972c891289640354993b662f4cccc50</span></span><br><span class="line"><span class="attr">gitalk:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span> <span class="comment">## If you want to use Gitment comment system please set the value to true.</span></span><br><span class="line"><span class="attr">  owner:</span>  <span class="comment">## Your GitHub ID, e.g. username</span></span><br><span class="line"><span class="attr">  repo:</span>  <span class="comment">## The repository to store your comments, make sure you're the repo's owner, e.g. gitalk.github.io</span></span><br><span class="line"><span class="attr">  client_id:</span>  <span class="comment">## GitHub client ID, e.g. 75752dafe7907a897619</span></span><br><span class="line"><span class="attr">  client_secret:</span>  <span class="comment">## GitHub client secret, e.g. ec2fb9054972c891289640354993b662f4cccc50</span></span><br><span class="line"><span class="attr">  admin:</span>  <span class="comment">## Github repo owner and collaborators, only these guys can initialize github issues.</span></span><br><span class="line"><span class="attr">valine:</span> <span class="comment">## https://valine.js.org</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span> <span class="comment">## If you want to use Valine comment system, please set the value to true.</span></span><br><span class="line"><span class="attr">  appid:</span> <span class="comment">## Your LeanCloud application App ID, e.g. pRBBL2JR4N7kLEGojrF0MsSs-gzGzoHsz</span></span><br><span class="line"><span class="attr">  appkey:</span> <span class="comment">## Your LeanCloud application App Key, e.g. tjczHpDfhjYDSYddzymYK1JJ</span></span><br><span class="line"><span class="attr">  notify:</span> <span class="literal">false</span> <span class="comment">## Mail notifier, see https://github.com/xCss/Valine/wiki/Valine-评论系统中的邮件提醒设置</span></span><br><span class="line"><span class="attr">  verify:</span> <span class="literal">false</span> <span class="comment">## Validation code.</span></span><br><span class="line"><span class="attr">  placeholder:</span> <span class="string">Just</span> <span class="string">so</span> <span class="string">so</span> <span class="comment">## Comment box placeholders.</span></span><br><span class="line"><span class="attr">  avatar:</span> <span class="string">'mm'</span> <span class="comment">## Gravatar type, see https://github.com/xCss/Valine/wiki/avatar-setting-for-valine</span></span><br><span class="line"><span class="attr">  pageSize:</span> <span class="number">10</span> <span class="comment">## Number of comments per page.</span></span><br><span class="line"><span class="attr">  guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment">## Attributes of reviewers.</span></span><br></pre></td></tr></table></figure><p>首先创建新应用OAuth application和Personal access tokens，具体位置在<br><code>Settings -&gt; Developer settings</code></p><ol><li><p>点击<code>new OAuth Apps</code>创建新应用<br>参数说明</p><ul><li>Application name： 随便填</li><li>Homepage URL： 网站URL，<a href="https://xn--github-on9im33ani7aou3bged.github.io" target="_blank" rel="noopener">https://你的github用户名.github.io</a></li><li>Application description ： 随便填</li><li>Authorization callback URL： 网站URL，<a href="https://xn--github-on9im33ani7aou3bged.github.io" target="_blank" rel="noopener">https://你的github用户名.github.io</a></li></ul></li><li><p>注册成功后，就会得到<code>Client ID</code>和<code>Client Secret</code>，这个填到上文所示的配置中</p></li><li><p>一般以上配置好后，第一次需要手动登陆初始化一下就OK了，如果有的主题配置完后出现gitalk未初始化，参考 <a href="https://draveness.me/git-comments-initialize" target="_blank" rel="noopener">自动初始化 Gitalk 和 Gitment 评论</a>，这个时候就需要获取token了。</p><p>**注意：**这里的配置需要sitemap，hexo默认没有添加sitemap功能，需要添加一下，<br>参考 <a href="https://www.jianshu.com/p/9c2d6db2f855" target="_blank" rel="noopener">hexo(3)-生成sitemap站点地图</a></p></li></ol><h1>推送到百度和谷歌检索</h1><p><a href="https://www.jianshu.com/p/f8ec422ebd52" target="_blank" rel="noopener">Hexo博客提交百度和Google收录</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css&quot;&gt;&lt;p&gt;从零开始搭建github个人博客，基于window系统（linux同理），适
      
    
    </summary>
    
    
      <category term="Blog" scheme="https://zekizz.github.io/categories/Blog/"/>
    
    
      <category term="博客" scheme="https://zekizz.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
