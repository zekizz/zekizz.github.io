

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.png">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="zekizz">
  <meta name="keywords" content="">
  <title>推荐系统学习笔记 - 给荔枝打气</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_6peoq002giu.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="给荔枝打气" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>给荔枝打气</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/bg/bg1.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
                推荐系统学习笔记
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2018-10-06 20:36" pubdate>
      2018年10月6日 晚上
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      8.6k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      95
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto" id="post">
            <!-- SEO header -->
            <h1 style="display: none">推荐系统学习笔记</h1>
            
            <div class="markdown-body" id="post-body">
              <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>最近对以前学习推荐系统的知识点笔记的一个汇总。</p>
<a id="more"></a>
<h1>概念</h1>
<p>你需要推荐系统吗？</p>
<ol>
<li>看看产品的目的：建立越多连接越好。</li>
<li>看产品现有的连接：产品的数量</li>
</ol>
<p>一个简单指标</p>
<p>$$<br>
\frac{\Delta connection}{ \Delta user \times \Delta item}<br>
$$</p>
<p>分子是增加的连接数，分母是增加的活跃用户数和增加的有效物品数。<br>
如果增加的连接数主要靠增加的活跃用户数和增加的物品数贡献，则该值较小，不适合加入推荐系统。反之，如果增加的连接数和新增活跃用户和物品关系不大，说明连接数已经有自发生长的趋势，适合加入推荐系统加速这个过程。</p>
<p>推荐系统的问题/任务</p>
<ol>
<li>评分预测</li>
<li>行为预测</li>
</ol>
<h2 id="评分预测">评分预测</h2>
<p>显示打分，目标减小预测分数与实际分数之间的误差，回归问题。</p>
<p>评价标准：RMSE、MAE</p>
<p>$$ RMSE = \sqrt{ \frac{\sum_{t=1}^n (\widehat{y}_t - y_t)^2}{n} }  $$</p>
<p>$$ MAE = \frac{\sum_{t=1}^n | \widehat{y}_t - y_t| }{n} $$</p>
<p>评分预测存在的问题：</p>
<ol>
<li>数据不易收集</li>
<li>数据质量不能保证</li>
<li>评分分布不稳定</li>
</ol>
<p>显示反馈很少，更多的是隐式反馈，通常为各类用户行为。行为预测更多地利用这部分数据。</p>
<h2 id="行为预测">行为预测</h2>
<p>隐式反馈：登陆刷新、购买、收藏、浏览、点击等</p>
<p>行为预测有两种方式：</p>
<ol>
<li>直接预测用户行为：CTR预估</li>
<li>预测物品的相对排序：learning2rank</li>
</ol>
<p>隐式数据的好处：</p>
<ol>
<li>比显式更加稠密</li>
<li>隐式更加代表用户的真实想法</li>
<li>隐式反馈常常和模型的目标函数关联更密切，也因此通常更容易在 AB 测试中和测试指标挂钩。</li>
</ol>
<p>推荐系统中几个普遍的问题</p>
<ol>
<li>冷启动问题</li>
<li>探索与利用问题：Exploit 和 Explore （EE问题）</li>
<li>安全问题
<ol>
<li>给出不靠谱的推荐结果，影响用户体验并最终影响品牌形象；</li>
<li>收集了不靠谱的脏数据，这个影响会一直持续留存在产品中，很难完全消除；</li>
<li>损失了产品的商业利益，这个是直接的经济损失。</li>
</ol>
</li>
</ol>
<h1>推荐方法</h1>
<h2 id="基于内容推荐">基于内容推荐</h2>
<h3 id="用户画像">用户画像</h3>
<blockquote>
<p>用户画像应该给机器看，而不是给人看。</p>
</blockquote>
<p>用户画像是将用户向量化后的结果，其关键因素：维度和量化。</p>
<p>维度：</p>
<ol>
<li>每个维度的名称是可理解的</li>
<li>维度的数目是拍脑袋决定的</li>
<li>维度的筛选也是拍脑袋决定的</li>
</ol>
<p>维度越多越精细，但是计算代价会变大，同时也会引入噪声</p>
<p>量化：<br>
不要为了用户画像而用户画像，它只是推荐系统的一个副产品，所以要根据推荐效果（排序好坏、召回覆盖等指标）来指导用户画像量化。</p>
<p>用户画像构建方法</p>
<ol>
<li>直接使用原始数据。比如人口统计学信息、购买历史、浏览历史等。</li>
<li>堆数据。堆积历史数据，做统计工作，常用的比如兴趣标签。</li>
<li>机器学习方法，比如隐语义模型、矩阵分解等embedding，构建无法直观理解的稠密向量。</li>
</ol>
<p>从文本数据中挖掘用户画像</p>
<ul>
<li>用户：昵称、姓名、性别、动态、评论等</li>
<li>物品：标题、描述、内容等</li>
</ul>
<p>构建用户画像步骤</p>
<ol>
<li>分析用户的文本和物品的文本，使其结构化；</li>
<li>标签选择，为用户挑选有信息量的结构化数据，作为其画像内容。</li>
</ol>
<p><strong>结构化文本算法</strong></p>
<ol>
<li>关键词提取：TF-IDF、TextRank。</li>
<li>实体识别NER：常用基于词典的方法结合 CRF 模型。</li>
<li>内容分类：将文本按照频道体系分类，用分类来表达较粗粒度的结构化信息。短文本常用Facebook 开源的 <strong>FastText</strong>。</li>
<li>聚类：无监督聚类，分簇，使用编号</li>
<li>主题模型：LDA</li>
<li>编码embedding</li>
</ol>
<p>$$ TF = count(w) $$</p>
<p>$$ IDF = log \frac{N}{n+1} $$</p>
<p>实体识别还有比较实用化的非模型做法：词典法。提前准备好各种实体的词典，使用trie-tree结构存储，拿着分好的词去词典寻找。<br>
工业级工具：spaCy</p>
<p>LDA工具：Gensim和PLDA等</p>
<p><strong>标签选择</strong></p>
<p>通过户端的文本，物品端的文本如何结构化，得到了诸如标签（关键词、分类等）、主题、词嵌入向量。接下来就是第二步：如何把物品的结构化信息给用户呢？</p>
<p>我们把用户对物品的行为，消费或者没有消费看成是一个分类问题。用户用实际行动帮我们标注了若干数据，那么挑选出他实际感兴趣的特性就变成了特征选择问题。</p>
<p>最常用的是两个方法：卡方检验（CHI）和信息增益（IG）。基本思想是：</p>
<ol>
<li>把物品的结构化内容看成文档；</li>
<li>把用户对物品的行为看成是类别；</li>
<li>每个用户看见过的物品就是一个文本集合；</li>
<li>在这个文本集合上使用特征选择算法选出每个用户关心的东西。</li>
</ol>
<p>卡方检验</p>
<table>
<thead>
<tr>
<th>卡方检验</th>
<th>属于类别C_j</th>
<th>不属于类别C_j</th>
<th>总计</th>
</tr>
</thead>
<tbody>
<tr>
<td>包含词W_i</td>
<td>A</td>
<td>B</td>
<td>A+B</td>
</tr>
<tr>
<td>不包含词W_i</td>
<td>C</td>
<td>D</td>
<td>C+D</td>
</tr>
<tr>
<td>总计</td>
<td>A+C</td>
<td>B+D</td>
<td>N = A+B+C+D</td>
</tr>
</tbody>
</table>
<p>计算每一个词和每一个类别的卡方值：</p>
<p>$$ \chi^2 (W_i, C_j) = \frac{N(AD-BC)^2}{ (A+C)(A+B)(B+D)(C+D)} $$</p>
<ol>
<li>每个词和每个类别都要计算，只要对其中一个类别有帮助的词都应该留下；</li>
<li>由于是比较卡方值的大小，所以公式中的 N 可以不参与计算，因为它对每个词都一样，就是总的文本数；</li>
<li>卡方值越大，意味着偏离“词和类别相互独立”的假设越远，靠“词和类别互相不独立”这个备择假设越近。</li>
</ol>
<p><strong>误区：</strong> 基于内容的推荐系统，标签只是很小一部分。而且就算是标签，衡量质量的方式也不是数目够不够。</p>
<p>所谓的基于内容推荐，通俗一点来讲，就是一个包装成推荐系统的信息检索系统。这听上去有点残酷，但通常一个复杂的推荐系统很可能是从基于内容推荐成长起来的。</p>
<p>为什么基于内容的推荐系统这么重要呢？因为内容数据非常易得，哪怕是在一个产品刚刚上线，用心找的话总能找到一些可以使用的内容，不需要有用户行为数据就能够做出推荐系统的第一版。</p>
<p>要把基于内容的推荐做好，需要做好“抓、洗、挖、算”四门功课。它们分别是：</p>
<ol>
<li>抓：一直持续抓数据丰富自己的内容，所以做好一个基于内容的推荐，抓取数据补充内容源，增加分析的维度，两者必不可少。</li>
<li>洗：冗余的内容、垃圾内容、政治色情等敏感内容等等都需要被洗出去。</li>
<li>挖：很多推荐系统提升效果并不是用了更复杂的推荐算法，而是对内容的挖掘做得更加深入。</li>
<li>算：匹配用户的兴趣和物品的属性，计算出更合理的相关性，这是推荐系统本身的使命，不仅仅是基于内容的推荐才要做的。</li>
</ol>
<p>结合基于内容推荐的框架看上诉几个步骤</p>
<p><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/RS/ContentConfig.png" srcset="/img/loading.gif" alt=""></p>
<p>内容这一端：内容源经过内容分析，得到结构化的内容库和内容模型，也就是物品画像。用户这一端：用户看过推荐列表后，会产生用户行为数据，结合物品画像，经过用户分析得到用户画像。</p>
<p>内容分析的产出</p>
<ol>
<li>结构化内容库</li>
<li>内容分析模型<br>
结构化的内容库，最重要的用途是结合用户反馈行为去学习用户画像。容易被忽略的是第二个用途，在内容分析过程中得到的模型，比如说：</li>
</ol>
<ol>
<li>分类器模型；</li>
<li>主题模型；</li>
<li>实体识别模型；</li>
<li>嵌入模型。</li>
</ol>
<p>这些模型主要用在：当新的物品刚刚进入时，需要实时地被推荐出去，这时候对内容的实时分析，提取结构化内容，再于用户画像匹配。</p>
<p>内容推荐算法</p>
<ol>
<li>直接计算相似度，BM25F算法</li>
<li>转为预估问题、分类问题</li>
</ol>
<p>一种最典型的场景：提高某种行为的转化率，如点击、收藏、转发等。那么标准的做法是：收集这类行为的日志数据，转换成训练样本，训练预估模型。</p>
每一条样本由两部分构成：一部分是特征，包含用户端的画像内容，物品端的结构化内容，可选的还有日志记录时一些上下文场景信息，如时间、地理位置、设备等等，另一部分就是用户行为，作为标注信息，包含“有反馈”和“无反馈”两类。
<p>二分类：LR+GBDT</p>
<h2 id="协同过滤">协同过滤</h2>
<h3 id="User-based-CF">User-based CF</h3>
<p>$$<br>
p(u,i) = \sum_{v \in S(u,K) \cap N(i)} w_{uv} r_{vi}<br>
$$</p>
<p>$S(u, K)$ 和用户 $ u $ 兴趣最接近的K个用户,$N(i)$ 是对物品i有过行为的用户集合，$w_{uv} $ 用户u和v之间的相似度，$ r_{ui} $ 用户v对物品i的兴趣，一般为{0,N}</p>
<p>用户相似度计算</p>
<p>用户向量</p>
<ol>
<li>向量的维度就是物品的个数；</li>
<li>向量是稀疏的，也就是说并不是每个维度上都有数值，原因当然很简单，这个用户并不是消费过所有物品，废话嘛，连我们压箱底的都给用户推荐了，那当然不用再推荐什么了；</li>
<li>向量维度上的取值可以是简单的 0 或者 1，也就是布尔值，1 表示喜欢过，0 表示没有，当然因为是稀疏向量，所以取值为 0 的就忽略了。</li>
</ol>
<p>Jaccard相似性</p>
<p>$$<br>
w_{nv} = \frac{|N(u) \cap |N(v)|}{|N(u) \cup |N(v)|}<br>
$$<br>
余弦相似性<br>
$$<br>
w_{nv} = \frac{|N(u) \cap |N(v)|}{\sqrt{|N(u)| |N(v)|}}<br>
$$</p>
<p>用户间相似度改进：惩罚热门</p>
<p>$$<br>
w_{uv} = \frac{\sum_ {i \in N(u) \cap N(v)} \frac{1}{log 1+|N(i)|}}{\sqrt{|N(u)| |N(v)|}}<br>
$$<br>
两个用户对冷门物品采取过同样的行为更能说明他们兴趣的相似度</p>
<p>对两两用户都利用余弦相似度计算相似度。这种方法的时间复杂度是O(|U|*|U|)，这在用户数很大时非常耗时，可以采用物品到用户的倒排表。</p>
<p>存在问题及解决</p>
<ol>
<li>构造矩阵：采用稀疏矩阵存储，COO（行号，列号，数值）</li>
<li>相似度计算：数据量大时处理</li>
</ol>
<p>第一个办法是：将相似度计算拆成 Map Reduce 任务，将原始矩阵 Map 成键为用户对，值为两个用户对同一个物品的评分之积，Reduce 阶段对这些乘积再求和，Map Reduce 任务结束后再对这些值归一化；</p>
<p>map: &lt; &lt;u1,u2&gt;, r_u1i * r_u2i&gt; ， 各个维度上做乘法</p>
<p>reduce：求和并归一化</p>
<p>第二个办法是：不用基于用户的协同过滤，采用基于图的算法。</p>
<p>另外，这种计算对象两两之间的相似度的任务，如果数据量不大，一般来说不超过百万个，然后矩阵又是稀疏的，那么有很多单机版本的工具其实更快，比如 KGraph、 GraphCHI 等。</p>
<ol start="3">
<li>推荐计算</li>
</ol>
<p>$$<br>
p(u,i) = \sum_{v \in S(u,K) \cap N(i)} w_{uv} r_{vi}<br>
$$<br>
得到了用户之间的相似度之后。接下来还有一个硬骨头，计算推荐分数。显然，为每一个用户计算每一个物品的推荐分数，计算次数是矩阵的所有元素个数，这个代价，你当然不能接受啊。</p>
<p>采用MapReduce</p>
<ol>
<li>遍历每个用户喜欢的物品列表；</li>
<li>获取该用户的相似用户列表；</li>
<li>把每一个喜欢的物品 Map 成两个记录发射出去，一个是键为 &lt; 相似用户 ID，物品 ID，1&gt; 三元组，可以拼成一个字符串，值为 &lt; 相似度 &gt;，另一个是键为 &lt; 相似用户 ID，物品 ID，0&gt; 三元组，值为 &lt; 喜欢程度 * 相似度 &gt;，其中的 1 和 0 为了区分两者，在最后一步中会用到；</li>
<li>Reduce 阶段，求和后输出；</li>
<li>&lt; 相似用户 ID，物品 ID, 0&gt; 的值除以 &lt; 相似用户 ID，物品 ID, 1&gt; 的值</li>
</ol>
<p>3:</p>
<ul>
<li>&lt; 相似用户 ID，物品 ID，1 &gt;  -&gt; w_{uv}</li>
<li>&lt; 相似用户 ID，物品 ID，0 &gt;  -&gt; w_{uv}*r_{vi}</li>
</ul>
<p>4: reduce应该是对相似用户求和</p>
<p>5：做归一化处理</p>
<h3 id="Item-CF">Item-CF</h3>
<p>$$<br>
p_{ui} = \sum_{j \in N(u) \cap S(i,K) } w_{ij} r_{uj}<br>
$$</p>
<p>物品相似度计算</p>
<p>注：相似度计算基于的是评分矩阵或者布尔化的行为矩阵</p>
<p>Jaccard<br>
$$ w_{ij} = \frac{ \sum_{u \in U} r_{ui} r_{uj} }{ \sqrt{\sum_{v \in U} r_{vi}^2 \sum_{v \in U} r_{vj}^2 }} $$</p>
<p>物品去中心化<br>
$$ w_{ij} = \frac{ \sum_{u \in U} (r_{ui} - \bar{r}<em>i) (r</em>{uj} - \bar{r}<em>j) }{ \sqrt{ \sum</em>{v \in U} (r_{ui} - \bar{r}<em>i)^2 \sum</em>{v \in U} (r_{uj} - \bar{r}<em>j)^2 } } $$<br>
用户去中心化<br>
$$ w</em>{ij} = \frac{ \sum_{u \in U} (r_{ui} - \bar{r}_u) (r_{uj} - \bar{r}_u) }{ \sqrt{ \sum_{v \in U} (r_{ui} - \bar{r}<em>u)^2 \sum</em>{v \in U} (r_{uj} - \bar{r}_u)^2 }} $$</p>
<p>更一般的相似性计算，比如余弦<br>
1.<br>
$$<br>
w_{ij} = \frac{&lt; r_i, r_j&gt;}{|r_i|}<br>
$$<br>
2. 热门关联<br>
$$<br>
w_{ij} = \frac{&lt; r_i, r_j&gt;}{|r_i||r_j|}<br>
$$<br>
3. 对热门的打压<br>
$$<br>
w_{ij} = \frac{&lt; r_i, r_j&gt;}{|r_i|^{\alpha} |r_j|^{ 1- \alpha }}<br>
$$<br>
$ \alpha $ 为0 最大限度打压热门，为1 不打压<br>
4. 用户打压<br>
$$<br>
&lt;r_i, r_j&gt; = \sum_{u \in U} \frac{r_{ui} r_{uj}}{ \log (1 + N(u))}<br>
$$<br>
IUF（Inverse User Frequence）</p>
<ol start="5">
<li>热传导</li>
</ol>
<p>冷门受益<br>
$$<br>
w_{ij}^H = \frac{1}{k_i} \sum_{u \in U} \frac{r_{ui} r_{uj}}{k_u}<br>
$$<br>
热门受益<br>
$$<br>
w_{ij}^P = \frac{1}{k_j} \sum_{u \in U} \frac{r_{ui} r_{uj}}{k_u}<br>
$$<br>
看除的分母，热门物品度大，冷门度小</p>
<p>调和<br>
$$<br>
w_{ij}^P = \frac{1}{k_i^{ 1- \lambda} k_j^{\lambda}} \sum_{u \in U} \frac{r_{ui} r_{uj}}{k_u}<br>
$$</p>
<h2 id="矩阵分解-隐语义模型LFM-latent-factor-model">矩阵分解/隐语义模型LFM(latent factor model)</h2>
<blockquote>
<p>评分预测问题只是很典型，其实并不大众，毕竟在实际的应用中，评分数据很难收集到，属于典型的精英问题；与之相对的另一类问题行为预测，才是平民级推荐问题，处处可见。</p>
</blockquote>
<p>近邻模型存在的问题：</p>
<ol>
<li>物品之间存在相关性，信息量并不随着向量维度增加而线性增加；</li>
<li>矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况存在。</li>
</ol>
<p>矩阵分解的目的分解评分矩阵A</p>
<p>$$<br>
A_{m \times n} \cong U_{m \times k} V_{n \times k}^T<br>
$$</p>
<p>推荐过程</p>
<p>$$<br>
\widehat{r}_{ui} = p_u q^T_i<br>
$$<br>
$ p_u $ 用户向量，$ q_i $ 物品向量。</p>
<h3 id="基本SVD">基本SVD</h3>
<p>SVD（奇异值分解）的损失函数：</p>
<p>$$ \min <em>{q^ *, p^ *} \sum</em>{u,i} (r_{ui} - p_u q_i^T) ^2 + \lambda ( ||q_i||^2 + ||p_u||^2 ) $$<br>
SVD学习过程</p>
<ol>
<li>准备好用户物品的评分矩阵，每一条评分数据看做一条训练样本；</li>
<li>给分解后的 U 矩阵和 V 矩阵随机初始化元素值；</li>
<li>用 U 和 V 计算预测后的分数；</li>
<li>计算预测的分数和实际的分数误差；</li>
<li>按照梯度下降的方向更新 U 和 V 中的元素值；</li>
<li>重复步骤 3 到 5，直到达到停止条件。</li>
</ol>
<h3 id="增加偏置">增加偏置</h3>
<p>$$<br>
\widehat{r}_{ui} = \mu + b_i + b_u + p_u q^T_i<br>
$$<br>
分别为全局评分、物品偏置、用户偏置</p>
<p>偏置的计算为，当前评分-对应的平均分</p>
<p>对应的损失函数<br>
$$ \min_{q^<em>, p^</em>} \sum_{u,i} (r_{ui} - \mu - b_i - b_u -  p_u q_i<sup>T)</sup>2 + \lambda ( ||q_i||^2 + ||p_u||^2 + b_i^2 + b_u^2) $$</p>
<h3 id="增加隐式数据-历史行为">增加隐式数据/历史行为</h3>
<p>在 SVD 中结合用户的隐式反馈行为和属性，这套模型叫做 SVD++。</p>
<p>隐式反馈的加入方法：</p>
<p>除了假设评分矩阵中的物品有一个隐因子向量外，用户有过行为的物品集合也都有一个隐因子向量，维度是一样的。把用户操作过的物品隐因子向量加起来，用来表达用户的兴趣偏好。</p>
<p>类似的，用户属性，全都转换成 0-1 型的特征后，对每一个特征也假设都存在一个同样维度的隐因子向量，一个用户的所有属性对应的隐因子向量相加，也代表了他的一些偏好。</p>
<p>综合两者，SVD++ 的目标函数中，只需要把推荐分数预测部分稍作修改，原来的用户向量那部分增加了隐式反馈向量和用户属性向量：<br>
$$ \widehat{r}_{ui} = \mu + b_i + b_u + (p_u + \frac{1}{|N(u)|} \sum_{j \in N(u)} x_j + \sum_{a \in Au} y_a ) q_i^T $$</p>
<h3 id="加入时间因素">加入时间因素</h3>
<ol>
<li>对评分按照时间加权，让久远的评分更趋近平均值；</li>
<li>对评分时间划分区间，不同的时间区间内分别学习出隐因子向量，使用时按照区间使用对应的隐因子向量来计算；</li>
<li>对特殊的期间，如节日、周末等训练对应的隐因子向量。</li>
</ol>
<h3 id="损失函数优化方法">损失函数优化方法</h3>
<p>SGD V.S. ALS</p>
<p>ALS的思想就是固定一个优化另外一个，所以叫交替最小二乘。其好处：</p>
<ol>
<li>在交替的其中一步，也就是假设已知其中一个矩阵求解另一个时，要优化的参数是很容易并行化的；</li>
<li>在不那么稀疏的数据集合上，交替最小二乘通常比随机梯度下降要更快地得到结果，事实上这一点就是我马上要说的，也就是关于隐式反馈的内容。</li>
</ol>
<p>相比“预测用户会打多少分”，“预测用户会不会去浏览”更加有意义，而且，用户浏览数据远远多于打分评价数据。也就是说，实际上推荐系统关注的是预测行为，行为也就是一再强调的隐式反馈。</p>
<p>对隐式反馈的矩阵分解，需要将交替最小二乘做一些改进，改进后的算法叫做加权交替最小二乘：Weighted-ALS。</p>
<ol>
<li>如果用户对物品无隐式反馈则认为评分是 0；</li>
<li>如果用户对物品有至少一次隐式反馈则认为评分是 1，次数作为该评分的置信度。</li>
</ol>
但是这里的数据只有正样本，负样本是我们“认为”的，这个“认为”可能不太准确，这就是One-class问题。所以需要对负样本进行采样。随机采样很不靠谱，一种比较好的方法是，按照物品的热门程度采样。
<p>关于负采样方法，word2vec等模型中都有介绍，后续专门写一篇博客。</p>
<p>按照物品热门程度采样的思想就是：一个越热门的物品，用户越可能知道它的存在。那这种情况下，用户还没对它有反馈就表明：这很可能就是真正的负样本。</p>
<p>现在的目标函数：</p>
<p>$$ \min_{q^<em>, p^</em>} \sum_{u,i} c_{ui} (r_{ui} - p_u q<sup>T_i)</sup>2 + \lambda ( ||q_i||^2 + ||p_u||^2 ) $$</p>
<p>$$ c_{ui} = 1+ \alpha C $$</p>
<p>C_ui就是置信度，跟反馈次数C有关，$ \alpha $ 默认取40。</p>
<h2 id="推荐计算">推荐计算</h2>
<p>在得到了分解后的矩阵后，每个用户得到了稠密的隐因子向量，同时每个物品也得到了一个稠密向量，代表它的语义或主题。看上去，让用户和物品的隐因子向量两两相乘，计算点积就可以得到所有的推荐结果了。但是实际上复杂度还是很高，尤其对于用户数量和物品数量都巨大的应用，如Facebook，就更不现实。于是 Facebook提出了两个办法得到真正的推荐结果。</p>
<p>第一种，利用一些专门设计的数据结构存储所有物品的隐因子向量，从而实现通过一个用户向量可以返回最相似的 K 个物品。</p>
<p>Facebook 给出了自己的开源实现 Faiss，类似的开源实现还有 Annoy，KGraph，NMSLIB。其中 Facebook 开源的 Faiss 和 NMSLIB（Non-Metric Space Library）都用到了 ball tree 来存储物品向量。</p>
<p>第二种，就是拿着物品的隐因子向量先做聚类，海量的物品会减少为少量的聚类。然后再逐一计算用户和每个聚类中心的推荐分数，给用户推荐物品就变成了给用户推荐物品聚类。得到给用户推荐的聚类后，再从每个聚类中挑选少许几个物品作为最终推荐结果。这样做的好处除了大大减小推荐计算量之外，还可以控制推荐结果的多样性，因为可以控制在每个类别中选择的物品数量。</p>
<h2 id="贝叶斯个性化排序BPR">贝叶斯个性化排序BPR</h2>
<p>矩阵分解，本质上都是在预测用户对一个物品的偏好程度，其实就是做编码embedding。</p>
<p>得到这样的矩阵分解结果后，常常在实际使用时，又是用这个预测结果来排序。所以，口口声声宣称想要模型的预测误差最小化，结果绕了一大圈最后还是只想要一个好点的排序。</p>
<p>这种针对单个用户对单个物品的偏好程度进行预测，得到结果后再排序的问题，在排序学习中的行话叫做 point-wise。与之相对的，还有直接预测物品两两之间相对顺序的问题，就叫做 pair-wise。</p>
<p>矩阵分解都属于 point-wise模型。这类模型的尴尬是：只能收集到正样本，没有负样本，于是认为缺失值就是负样本，再以预测误差为评判标准去使劲逼近这些样本。逼近正样本没问题，但是同时逼近的负样本只是缺失值而已，还不知道真正呈现在用户面前，到底是不喜欢还是喜欢呢？</p>
<p>虽然这些模型采取了一些措施来规避这个问题，比如负样本采样，但是尴尬还是存在的，为了排序而绕路也是事实。</p>
<p>贝叶斯个性化排序(Bayesian Personalized Ranking, BPR)就直接采用pair-wise来做矩阵分解。</p>
<p>在BPR算法中，我们将任意用户u对应的物品进行标记，如果用户u在同时有物品i和j的时候点击了i，那么我们就得到了一个三元组&lt;u, i, j&gt;，它表示对用户u来说，i的排序要比j靠前。</p>
<p>三元组中，i和j都只能是行为过和未行为过中的一种，不包含都行为和都未行为的情况。</p>
<p>这样一来，学习的数据是反应用户偏好的相对顺序，而在使用时，面对的是所有用户还没行为过的物品，这些物品仍然可以在这样的模型下得到相对顺序，这就比三元组 point-wise 样本要直观得多。</p>
<p>现在，每条样本包含的是两个物品，样本预测目标是两个物品的相对顺序。</p>
<p>用个符号来表示这个差：Xu12，表示的是对用户 u，物品 1 和物品 2 的矩阵分解预测分数差。然后再用 sigmoid 函数把这个分数差压缩到 0 到 1 之间。</p>
<p>$$<br>
\Theta = \frac{1}{1+ e ^{-X_{u12}}}<br>
$$<br>
也其实就是用这种方式预测了物品 1 排在物品 2 前面的似然概率，所以最大化交叉熵就是目标函数了。</p>
<p>目标函数通常还要防止过拟合，加上正则项，正则项其实认为模型参数还有个先验概率，这是贝叶斯学派的观点，也是 BPR 这个名字中“贝叶斯”的来历。</p>
<p>BPR 认为模型的先验概率符合正态分布，对应到正则化方法就是 L2 正则，具体参见<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/9128682.html">贝叶斯个性化排序算法小结</a>。</p>
<p>$$ \prod_{u \in U} P(&gt;<em>u|\theta) P(\theta) = \prod</em>{(u,i,j) \in D}   \sigma(X_{u12})  P(\theta) = \prod_{(u,i,j) \in D} \sigma(\overline{x}_{ui} - \overline{x}_{uj}) P(\theta) $$</p>
<p>训练方法：梯度下降+mini-batch</p>
<h1>特征工程</h1>
<p>实施特征工程之前，需要先理解业务。<br>
在推荐场景中</p>
<p><img src="https://picbed-1252770021.cos.ap-chengdu.myqcloud.com/RS/recom_context.png" srcset="/img/loading.gif" alt=""></p>
<p>特征</p>
<ul>
<li>先归一化，统一量纲</li>
<li>离散化，引入非线性关系</li>
<li>特征交叉</li>
<li>GBDT</li>
</ul>
<p>无个性化特征信息交叉会造成所有用户结果一样。</p>
<p>交叉特征：</p>
<ul>
<li>性别&amp;分布（转化分布）</li>
<li>分布&amp;ID</li>
</ul>
<p>在样本给定下：</p>
<ol>
<li>仔细考虑用何种特征构造方法</li>
<li>从聚合/内积开始，逐渐增加参数，直到过拟合</li>
</ol>
<p>过拟合判断方法：如，训练集AUC远大于测试集AUC</p>
<h1>模型融合</h1>
<p>推荐系统在技术实现上一般划分为三个阶段：挖掘、召回、排序。</p>
<p>挖掘的工作就是对用户和物品做非常深入的结构化分析，庖丁解牛一样，各个角度各个层面的特征都被呈现出来，并且建好索引，供召回阶段使用，大部分挖掘工作都是离线进行的。</p>
<p>接下来就是召回，为什么会有召回？因为物品太多了，每次给一个用户计算推荐结果时，如果对全部物品挨个计算，那将是一场灾难，取而代之的是用一些手段从全量的物品中筛选出一部分比较靠谱的。</p>
<p>最后就是排序，针对筛选出的一部分靠谱的做一个统一的排序。</p>
<p>进一步召回：<br>
在召回阶段，其实就是各种简单的、复杂的推荐算法，比如说基于内容的推荐，会产生一些推荐结果，比如基于物品的协同过滤会产生一些结果，矩阵分解会产生一些结果，等等。</p>
<p>正则化的方法一般是：限定总的树个数、树的深度、以及叶子节点的权重大小。实数特征的分裂。</p>
<p>推荐系统：唯快不破</p>
<p>在线算法</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/20447450">FTRL</a></li>
<li>TG</li>
<li>FOBOS</li>
<li>RDA</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://www.cs.ust.hk/~qyang/Docs/2007/tradaboost.pdf">Boosting For Transfer Learning</a></p>
<p>特征组合</p>
<ul>
<li>GBDT+LR</li>
<li>FM(Factorization Machine)：因子分解机</li>
<li>FFM(Field-aware Factorization Machine）</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/official/wide_deep">Wide &amp; Deep 模型</a></li>
</ul>
<ol>
<li>深宽模型是一个结合了传统线性模型和深度模型的工程创新。</li>
<li>这个模型适合高维稀疏特征的推荐场景，稀疏特征的可解释性加上深度模型的泛化性能，双剑合璧。</li>
<li>这个模型已经开源在 TensorFlow 中。</li>
<li>为了提高模型的训练效率，每一次并不从头开始训练，而是用上一次模型参数来初始化当前模型的参数。</li>
<li>将类别型特征先做嵌入学习，再将嵌入稠密向量送入深度模型中。</li>
<li>为了提高服务的响应效率，对每次请求要计算的多个候选 App 采用并行评分计算的方式，大大降低响应时间。</li>
</ol>
<h1>MAB问题</h1>
<p>多臂赌博机问题 (Multi-armed bandit problem, K-armed bandit problem, MAB)，简称 MAB 问题。</p>
<p>推荐系统的使命就是：为用户匹配到最佳的物品，在某个时间某个位置为用户选择最好的物品。</p>
<p>推荐就是选择</p>
<h2 id="Bandit-算法">Bandit 算法</h2>
<p>小心翼翼地试，越确定某个选择好，就多选择它，越确定某个选择差，就越来越少选择它。</p>
<p>一种走一步看一步的推荐算法， Bandit 算法。Bandit 算法把每个用户看成一个多变的环境，待推荐的物品就如同赌场里老虎机的摇臂，如果推荐了符合用户心目中喜欢的，就好比是从一台老虎机中摇出了金币一样。</p>
<p>Bandit 算法有汤普森采样，UCB 算法，Epsilon 贪婪。汤普森采样以实现简单和效果显著而被人民群众爱戴，你需要时不妨首先试试它。</p>
<p>Bandit解决冷启动</p>
<ol>
<li><p>用分类或者 Topic 来表示每个用户兴趣，我们可以通过几次试验，来刻画出新用户心目中对每个 Topic 的感兴趣概率。</p>
</li>
<li><p>这里，如果用户对某个 Topic 感兴趣，就表示我们得到了收益，如果推给了它不感兴趣的 Topic，推荐系统就表示很遗憾 (regret) 了。</p>
</li>
<li><p>当一个新用户来了，针对这个用户，我们用汤普森采样为每一个 Topic 采样一个随机数，排序后，输出采样值 Top N 的推荐 Item。注意，这里一次选择了 Top N 个候选臂。</p>
</li>
<li><p>等着获取用户的反馈，没有反馈则更新对应 Topic 的 b 值，点击了则更新对应 Topic 的 a 值。</p>
</li>
</ol>
<h3 id="LinUCB">LinUCB</h3>
<p>“Yahoo!”的科学家们在 2010 年基于 UCB 提出了 LinUCB 算法，它和传统的 UCB 算法相比，最大的改进就是加入了特征信息，每次估算每个候选的置信区间，不再仅仅是根据实验，而是根据特征信息来估算，这一点就非常的“机器学习”了。</p>
<p>优点：</p>
<ol>
<li>由于加入了特征，所以收敛比 UCB 更快，也就是比 UCB 更快见效；</li>
<li>各个候选臂之间参数是独立的，可以互相不影响地更新参数；</li>
<li>由于参与计算的是特征，所以可以处理动态的推荐候选池，编辑可以增删文章；</li>
</ol>
<p>LinUCB 只是一个推荐框架，可以将这个框架应用在很多地方，比如投放广告，为用户选择兴趣标签等。</p>
<h3 id="COFIBA-算法">COFIBA 算法</h3>
<p>概要：</p>
<ol>
<li>在时刻 t，有一个用户来访问推荐系统，推荐系统需要从已有的候选池子中挑一个最佳的物品推荐给他，然后观察他的反馈，用观察到的反馈来更新挑选策略。</li>
<li>这里的每个物品都有一个特征向量，所以这里的 Bandit 算法是 context 相关的，只不过这里虽然是给每个用户维护一套参数，但实际上是由用户所在的聚类类簇一起决定结果的。</li>
<li>这里依然是用岭回归去拟合用户的权重向量，用于预测用户对每个物品的可能反馈（payoff），这一点和我们上一次介绍的 LinUCB 算法是一样的。</li>
</ol>
<p>与linUCB算法的不同：</p>
<ol>
<li>基于用户聚类挑选最佳的物品，即相似用户集体动态决策；</li>
<li>基于用户的反馈情况调整用户和物品的聚类结果。</li>
</ol>
<p>算法流程：</p>
<ol>
<li>首先计算用户 i 的 Bandit 参数 W，做法和 LinUCB 算法相同，但是这个参数并不直接参与到选择决策中，注意这和 LinUCB 不同，只是用来更新用户聚类。</li>
<li>遍历候选物品，每一个物品已经表示成一个向量 x 了。</li>
<li>每一个物品都对应一个物品聚类类簇，每一个物品类簇对应一个全量用户聚类结果，所以遍历到每一个物品时，就可以判断出当前用户在当前物品面前，自己属于哪个用户聚类类簇，然后把对应类簇中每个用户的 M 矩阵 (对应 LinUCB 里面的 A 矩阵)，b 向量（表示收益向量，对应 LinUCB 里面的 b 向量）加起来，从而针对这个类簇求解一个岭回归参数（类似 LinUCB 里面单独针对每个用户所做），同时计算其收益预测值和置信区间上边界。</li>
<li>每个待推荐的物品都得到一个预测值及置信区间上界，挑出那个上边界最大的物品作为推荐结果。</li>
<li>观察用户的真实反馈，然后更新用户自己的 M 矩阵和 b 向量，只更新每个用户，对应类簇里其他的不更新。</li>
</ol>
<p>Bandit 算法系列，主要是解决推荐系统中的冷启动和 EE 问题。探索和利用这一对矛盾一直客观存在，而 Bandit 算法是公认的一种比较好的解决 EE 问题的方案。</p>
<h2 id="深度学习在推荐上的应用">深度学习在推荐上的应用</h2>
<h2 id="排行榜的构建">排行榜的构建</h2>
<p>热度计算</p>
<ol>
<li>Hacker News</li>
</ol>
<p>$$<br>
\frac{P-1}{(T+2)^G}<br>
$$</p>
<ol>
<li>P：得票数，去掉帖子作者自己投票。</li>
<li>T：帖子距离现在的小时数，加上帖子发布到被转帖至 Hacker News 的平均时长。</li>
<li>G：帖子热度的重力因子。</li>
</ol>
<p>公式中，分子是简单的帖子数统计，一个小技巧是去掉了作者自己的投票。分母就是将前面说到的时间因素考虑在内，随着帖子的发表时间增加，分母会逐渐增大，帖子的热门程度分数会逐渐降低。</p>
<ol start="2">
<li>牛顿冷却定律</li>
</ol>
<p>$$<br>
T(t) = H + C e^{-\alpha t}<br>
$$</p>
<ul>
<li>H：为环境维度，可以认为是平均票数，比如电商中的平均销量，由于不影响排序，可以不使用。</li>
<li>C：为净剩票数，即时刻 t 物品已经得到的票数，也就是那个最朴素的统计量，比如商品的销量。</li>
<li>t：为物品存在时间，一般以小时为单位。</li>
<li>\alpha：是冷却系数，反映物品自然冷却的快慢。</li>
</ul>
<h2 id="其他算法">其他算法</h2>
<h3 id="加权采样算法">加权采样算法</h3>
<p>有限数据集</p>
<p>$$<br>
S_{i} = R^{\frac{1}{w_{i}}}<br>
$$</p>
<ol>
<li>wi 是每个样本的权重，比如用户标签权重；</li>
<li>R 是遍历每个样本时产生的 0 到 1 之间的随机数；</li>
<li>Si 就是每个样本的采样分数</li>
</ol>
<p>你可以看到，每个样本采样概率和它的权重成正比。</p>
<p>指数分布采样</p>
<p>无限数据集：蓄水池采样</p>
<p>内容去重算法</p>
<ul>
<li>Simhash</li>
<li>布隆过滤器</li>
</ul>
<h1>工程实践</h1>
<p>信息流，feed流</p>
<p>信息流框架</p>
<p>Netflix架构</p>
<p>TODO</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
                    
                      <a class="hover-with-bg" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/zi-ran-yu-yan-chu-li/doublearraytrie/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">双数组Trie树(DoubleArrayTrie)</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/ji-qi-xue-xi/ner-survey/">
                        <span class="hidden-mobile">命名实体识别综述</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments">
                
                
  <script type="text/javascript">
    function loadUtterances() {
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'zekizz/commit-utterances');
      s.setAttribute('issue-term', 'title');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', 'github-light');
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    }
    waitElementVisible('comments', loadUtterances)
  </script>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '#post-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>





  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

  
















</body>
</html>
